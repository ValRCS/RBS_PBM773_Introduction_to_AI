{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjkWVY7_-isW"
      },
      "source": [
        "# Chapter 4 - Search in Complex Environments\n",
        "\n",
        "We have seen how to search in simple environments, where the agent addressed problems in fully observable, deterministic, static, known environments.\n",
        "\n",
        "Today we  will also see how to search in environments where the agent does not have complete information about the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dn-cAkH-isZ"
      },
      "source": [
        "## 4.1 - \"Local Search and Optimization Problems\"\n",
        "\n",
        "- **Focus on Final State, Not the Path**: Unlike Chapter 3, where the goal was to find paths (e.g., from Arad to Bucharest), Chapter 4.1 emphasizes finding a final state without concern for the specific path taken. This is exemplified in problems like the 8-queens problem, where the objective is to find a valid configuration.\n",
        "- **Applications in Various Fields**: This approach is applicable in many areas such as integrated-circuit design, factory floor layout, job scheduling, automatic programming, telecommunications, crop planning, and portfolio management.\n",
        "- **Local Search Algorithms**: These algorithms start from an initial state and move to neighboring states, without keeping track of the path or the states already explored. They are not systematic and might miss parts of the search space containing a solution.\n",
        "- **Advantages of Local Search**:\n",
        "   - **Low Memory Usage**: They require minimal memory.\n",
        "   - **Suitability for Large/Infinite State Spaces**: They can find reasonable solutions in cases where systematic algorithms are impractical.\n",
        "- **Optimization Problems**: Local search algorithms are also useful for optimization problems, where the goal is to find the best state according to an objective function.\n",
        "- **Objective Function and State-Space Landscape**:\n",
        "   - The states of a problem are considered in a landscape, with each state having an 'elevation' defined by the objective function's value.\n",
        "   - The process of finding the highest point (optimal state) varies depending on the nature of the objective function: it's called hill climbing if seeking a global maximum and gradient descent if seeking a global minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ROaRyvB-isa"
      },
      "source": [
        "### 4.1.1 \"Hill-Climbing Search,\"\n",
        "\n",
        "\n",
        "- **Basic Concept of Hill-Climbing Search**: Hill-climbing search is a local search algorithm. It continuously moves towards the state with the highest value (or lowest cost), akin to climbing up a hill. This process is repeated until a peak (optimal state) is reached.\n",
        "\n",
        "- normal exhaustive search would be to check every possible state, but hill-climbing only checks the states that are neighbors of the current state.\n",
        "- normal exhaustive is often not possible because of so called \"combinatorial explosion\" (e.g., 8-queens problem has 4.4 billion states)\n",
        "- if you have 10 parameters with 10 possible discrete values each, you have 10^10 possible states to check, not feasible most of the time\n",
        "\n",
        "- **Application to the 8-Queen Problem**:\n",
        "   - In the context of the 8-queen problem, hill-climbing involves placing queens on the board such that they move towards a configuration with fewer conflicts.\n",
        "   - The objective is to reach a state where no two queens threaten each other, which represents the peak or the solution.\n",
        "\n",
        "   ![8 Queens](https://upload.wikimedia.org/wikipedia/commons/1/1f/Eight-queens-animation.gif)\n",
        "\n",
        "   Note: animation actually shows backtracking algorithm...using depth-first search, we keep placing queens until we reach a state where we can't place any more queens. Then we backtrack to the previous state and try a different position for the last queen. We keep doing this until we find a solution.\n",
        "   \n",
        "- **Greedy Approach**: Hill-climbing is a greedy algorithm, meaning it chooses the best option at each step without considering the long-term consequences. This can lead to suboptimal solutions.\n",
        "- **Challenges and Limitations**:\n",
        "   - **Local Maxima**: The algorithm can get stuck at a local maximum (a state better than its neighbors but not the best overall solution).\n",
        "   - **Plateaus**: Flat areas of the search space where neighboring states have similar values can halt progress.\n",
        "   - **Ridges**: It can be difficult for the algorithm to navigate narrow pathways or ridges that lead to the solution.\n",
        "- **Variants of Hill-Climbing Search**:\n",
        "   - **Steepest-Ascent Hill Climbing**: Evaluates all neighbors and picks the best one to move forward.\n",
        "   - **Simple Hill Climbing**: Moves to a better neighbor state without evaluating all neighbors.\n",
        "- **Handling Limitations**: To address issues like local maxima, modifications such as random restarts (starting the search anew from a different random state) can be employed.\n",
        "- **Efficiency in the 8-Queen Problem**: Despite its limitations, hill-climbing search can be efficient for problems like the 8-queen puzzle, where a good heuristic can lead to quick and effective solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWOrZjxE-isa"
      },
      "source": [
        "<img src=\"https://github.com/ValRCS/RBS_PBM773_Introduction_to_AI/blob/main/img/ch4_search_in_complex_environments/DALL%C2%B7E%202024-01-18%2000.10.10%20-%20Create%20a%20comic-style%20illustration%20showing%20the%20concept%20of%20the%20hill-climbing%20algorithm.%20The%20image%20should%20depict%20multiple%20hills%20of%20different%20heights%20and%20.png?raw=true\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifq9aMyx-isa"
      },
      "source": [
        "### 4.1.2 \"Simulated Annealing\"\n",
        "\n",
        "- **Concept of Simulated Annealing**:\n",
        "   - Simulated annealing is an advanced form of local search that avoids getting stuck in local maxima by allowing 'bad' moves (moves that worsen the objective function) with a certain probability.\n",
        "   - This technique is inspired by the process of annealing in metallurgy, where controlled cooling of a material helps to reduce defects and achieve a more stable structure.\n",
        "- **Temperature Parameter**:\n",
        "   - The algorithm uses a \"temperature\" parameter that gradually decreases over time. High temperature allows the algorithm to accept worse solutions more readily, fostering exploration of the search space.\n",
        "   - As the temperature decreases, the algorithm becomes more conservative, focusing on exploitation and refining the current best solution.!\n",
        "   \n",
        "   ![Simulated Annealing](https://upload.wikimedia.org/wikipedia/commons/d/d5/Hill_Climbing_with_Simulated_Annealing.gif)\n",
        "\n",
        "\n",
        "\n",
        "- **Probability of Accepting Worse Solutions**:\n",
        "   - The probability of accepting worse solutions is higher at the start (high temperature) and decreases over time. This helps in escaping local maxima early on and then converging to a global maximum or minimum as the temperature lowers.\n",
        "- **Real-Life Examples of Applications**:\n",
        "   - **Optimization in Engineering**: Used in engineering design processes to optimize complex systems with many variables.\n",
        "   - **Financial Market Analysis**: Employed in portfolio optimization and risk management.\n",
        "   - **Route Planning**: Utilized in logistics and transportation for optimizing routes, minimizing travel time or cost.\n",
        "- **Comparison with Hill-Climbing Search**:\n",
        "   - Unlike hill-climbing search, which may get stuck in local maxima, simulated annealing has a better chance of finding a global maximum by allowing occasional moves to worse states.\n",
        "- **Cooling Schedule**:\n",
        "   - The cooling schedule, which dictates how temperature decreases over time, is crucial for the effectiveness of the algorithm. Different schedules can be experimented with for optimal performance in various problems.\n",
        "- **Versatility and Robustness**:\n",
        "   - Simulated annealing is versatile and robust, making it suitable for a wide range of optimization problems where the search space is rugged or contains many local maxima.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GyrW6J9-isb"
      },
      "source": [
        "<img src=\"https://github.com/ValRCS/RBS_PBM773_Introduction_to_AI/blob/main/img/ch4_search_in_complex_environments/DALL%C2%B7E%202024-01-18%2000.13.18%20-%20Draw%20a%20comic-style%20illustration%20showing%20a%20hill%20climber%20using%20a%20high%20beam%20car%20light%20to%20look%20over%20multiple%20hills.%20The%20climber%20should%20be%20depicted%20standin.png?raw=true\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbQ4oXSi-isb"
      },
      "source": [
        "### 4.1.3 \"Local Beam Search\"\n",
        "\n",
        "- **Concept of Local Beam Search**:\n",
        "   - Local beam search is a type of local search algorithm that operates with multiple \"beams\" or states simultaneously, unlike hill-climbing which uses only one.\n",
        "   - It starts with a set of randomly generated states and at each step, all the successors of these states are generated.\n",
        "- **Selection of States**:\n",
        "   - From the generated successors, the algorithm selects the\n",
        "   k best states (where\n",
        "   k is the beam width) to be the states for the next iteration. This ensures that the search is focused on the most promising areas of the search space.\n",
        "- **Advantages Over Simple Hill-Climbing**:\n",
        "   - It reduces the risk of getting stuck in local maxima since it explores multiple paths simultaneously.\n",
        "   - Offers a more comprehensive search of the problem space compared to hill-climbing.\n",
        "- **Stochastic Beam Search**:\n",
        "   - Stochastic beam search is a variation of local beam search where the selection of the k states is based on a probabilistic model rather than selecting just the top k states.\n",
        "   - This approach introduces randomness in the selection process, which can help in exploring more diverse parts of the search space and potentially finding better solutions.\n",
        "- **Application Examples**:\n",
        "   - Useful in problems where the search space is too large for exhaustive search but can be effectively sampled, like in natural language processing or genetic sequence analysis.\n",
        "   - Employed in machine learning for feature selection and model optimization.\n",
        "- **Limitations**:\n",
        "   - Beam search can still miss the optimal solution if it's not within the initially selected paths.\n",
        "   - The choice of beam width (k) is crucial; too narrow a beam may lead to missing good solutions, while too wide a beam may become computationally expensive.\n",
        "- **Efficiency and Scalability**:\n",
        "   - Offers a good balance between exploration and exploitation, making it efficient for certain types of problems.\n",
        "   - Scalability can be an issue for very large problems or when using a large beam width."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdrLAAaT-isb"
      },
      "source": [
        "<img src=\"https://github.com/ValRCS/RBS_PBM773_Introduction_to_AI/blob/main/img/ch4_search_in_complex_environments/DALL%C2%B7E%202024-01-18%2000.16.49%20-%20Create%20an%20illustration%20symbolizing%20the%20concept%20of%20a%20genetic%20algorithm.%20The%20image%20should%20depict%20the%20evolution%20of%20a%20tiger%20into%20a%20cat,%20representing%20the%20i.png?raw=true\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEMHeSuH-isc"
      },
      "source": [
        "### 4.1.4 \"Evolutionary Algorithms\"\n",
        "\n",
        "- **Overview of Evolutionary Algorithms**:\n",
        "   - Evolutionary algorithms are a subset of search algorithms inspired by the biological process of natural selection and genetics. They evolve a population of candidate solutions to a problem over time.\n",
        "- **Recombination**:\n",
        "   - Recombination, or crossover, is a key process where two parent solutions are combined to produce offspring solutions. This mimics biological reproduction and genetic crossover.\n",
        "- **Genetic Algorithms**:\n",
        "   - Genetic algorithms are a type of evolutionary algorithm that uses techniques inspired by evolutionary biology such as mutation, crossover (recombination), and selection.\n",
        "   - They are particularly useful for optimization problems where the search space is large and poorly understood.\n",
        "- **Evolution Strategies**:\n",
        "   - Evolution strategies focus more on mutation (small random changes in the candidate solutions) rather than crossover. They are used for continuous optimization problems.\n",
        "- **Selection Process**:\n",
        "   - The selection process decides which individuals (solutions) are chosen to create the next generation. It is typically based on fitness, where more fit individuals (better solutions) have a higher chance of being selected.\n",
        "\n",
        "- **Fitness Function**:\n",
        "   - The fitness function is a key component of evolutionary algorithms. It evaluates the quality of a solution based on the problem's objective function.\n",
        "   - The fitness function is used in the selection process to determine which individuals are selected to create the next generation.\n",
        "- **Crossover Point**:\n",
        "   - In genetic algorithms, the crossover point is where the genetic material (solution attributes) is exchanged between parents. The choice of crossover points can significantly impact the performance of the algorithm.\n",
        "- **Mutation Rate**:\n",
        "   - The mutation rate determines how often random changes are introduced into the offspring. A balance is needed; too high a rate can lead to random search, while too low a rate can cause premature convergence on suboptimal solutions.\n",
        "- **Elitism**:\n",
        "   - Elitism involves preserving a portion of the best individuals (solutions) unchanged for the next generation. This ensures that the quality of solutions does not degrade over time.\n",
        "- **Culling**:\n",
        "   - Culling is the process of removing the least fit individuals from the population. This makes room for new and potentially more fit individuals in the population.\n",
        "- **Applications**:\n",
        "   - Evolutionary algorithms are applied in various fields such as engineering design, artificial life, game playing, and machine learning, where they help to find optimal or near-optimal solutions in complex search spaces.\n",
        "\n",
        "   Norvig, Rusell are a bit sceptical on genetic algorithms saying: \"not clear whether they are better than other search algorithms, or are they popular because of the metaphor of evolution, or because they are easy to implement.\" pg. 132 of Norvig, Rusell 4th edition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxAhvWfx-isc"
      },
      "source": [
        "### Evolutionary Games\n",
        "\n",
        "* https://keiwan.itch.io/evolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dlo1h11-isc"
      },
      "source": [
        "## 4.2 Local Search in Continuous Spaces\n",
        "\n",
        "- **Example of Continuous Search Space**\n",
        "   - It could be a function of one or more variables, e.g., f(x) = x^2 + 2x + 1, where x is a real number. The goal is to find the value of x that minimizes the function.\n",
        "   - By contrast discrete search space is a set of discrete parameter values, e.g., {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}.\n",
        "   - Real life example given in a big is placement of 3 new airports in Romania defined by their coordinates (x, y) on a map. The goal is to minimize the total distance between the airports and the cities.\n",
        "   So the function would be f(x1, y1, x2, y2, x3, y3) = distance between airport 1 and city 1 + distance between airport 2 and city 2 + distance between airport 3 and city 3.\n",
        "   \n",
        "\n",
        "- **Variables in Continuous Spaces**:\n",
        "   - In continuous spaces, variables can take on any value within a given range, unlike in discrete spaces where variables have a distinct set of values.\n",
        "- **Discretization**:\n",
        "   - Discretization is the process of converting continuous variables into discrete ones. This simplifies the problem but can lead to loss of precision.\n",
        "- **Empirical Gradient**:\n",
        "   - Empirical gradients estimate the gradient (rate of change) of the objective function using finite differences. They are used when the actual gradient is difficult to compute.\n",
        "- **Gradient**:\n",
        "   - The gradient of a function gives the direction of the steepest ascent. In optimization, following the gradient can lead to finding local maxima.\n",
        "- **Step Size**:\n",
        "   - Step size determines how far to move in the direction of the gradient. Choosing the right step size is crucial for the efficiency and accuracy of gradient-based search methods.\n",
        "- **Line Search**:\n",
        "   - Line search is a method to find an optimal step size that minimizes the objective function along the direction of the gradient.\n",
        "- **Newton-Raphson Method**:\n",
        "   - The Newton-Raphson method is an iterative method used for finding successively better approximations to the roots (or zeroes) of a real-valued function. It's used in optimization to find where the gradient is zero.\n",
        "\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Newton-Raphson_method.gif/1600px-Newton-Raphson_method.gif?20220129153027\" width=\"400\" alt=\"Newton Raphson method\">\n",
        "<br>\n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/6929060731e351c465426e37567abe5ee13d65d9\" width=\"400\">\n",
        "\n",
        "we repeat the above formula until we reach a point of sufficient accuracy.\n",
        "\n",
        "More: https://en.wikipedia.org/wiki/Newton%27s_method\n",
        "\n",
        "See also: https://en.wikipedia.org/wiki/Fast_inverse_square_root\n",
        "\n",
        "- **Constrained Optimization**:\n",
        "   - Constrained optimization deals with optimization problems that have constraints on the values that the solution variables can take.\n",
        "- **Linear Programming**:\n",
        "   - Linear programming is a method for optimizing a linear objective function, subject to linear equality and linear inequality constraints.\n",
        "- **Convex Set**:\n",
        "   - A convex set in a geometrical space is a set where, for every pair of points within the set, every point on the straight line segment that joins them is also within the set.\n",
        "- **Convex Optimization**:\n",
        "   - Convex optimization involves optimizing an objective function over a convex set. It has the advantage that any local optimum is also a global optimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORYlb8o9-isd"
      },
      "source": [
        "## 4.3 Search with Nondeterministic Actions\n",
        "\n",
        "- **Concept of Nondeterministic Actions**:\n",
        "   - Nondeterministic actions in search refer to actions whose outcomes are not predictable with certainty. This means that a single action can lead to multiple possible outcomes.\n",
        "- **Belief State**:\n",
        "   - A belief state (or belief) represents a set of all possible states that an agent might be in at any given time. It's a way of dealing with the uncertainty caused by nondeterministic actions.\n",
        "   - As the agent gains more information (through observations or actions), the belief state is updated to reflect the new information.\n",
        "- **Handling Nondeterministic Actions in Search**:\n",
        "   - When dealing with nondeterministic actions, the search process must consider all possible outcomes of actions and how they affect the belief state.\n",
        "- **Conditional Plan**:\n",
        "   - A conditional plan is a plan that specifies actions to take depending on the current belief state. It is a sequence of actions and conditionals (if-then rules) that determine the next action based on the current state of knowledge.\n",
        "   - In environments with nondeterministic actions, conditional plans are necessary to adapt to different outcomes that might arise.\n",
        "- **Planning with Belief States**:\n",
        "   - The process involves creating plans that are robust to different possible worlds that might exist within the belief state.\n",
        "   - The goal is to find a plan that leads to success in all possible states within the belief state.\n",
        "- **Uncertainty and Complexity**:\n",
        "   - Dealing with nondeterministic actions adds complexity to the search problem since the agent must plan for multiple contingencies.\n",
        "   - The complexity of the search can grow significantly with the increase in possible states and actions.\n",
        "- **Real-World Applications**:\n",
        "   - This approach is applicable in real-world scenarios where actions have uncertain outcomes, such as in robotics (navigating uncertain terrain), financial decision-making (dealing with market volatility), and automated planning in unpredictable environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_tygu06-isd"
      },
      "source": [
        "<img src=\"https://github.com/ValRCS/RBS_PBM773_Introduction_to_AI/blob/main/img/ch4_search_in_complex_environments/DALL%C2%B7E%202024-01-18%2000.18.25%20-%20Draw%20an%20illustration%20of%20an%20erratic%20vacuum%20cleaner.%20The%20vacuum%20should%20be%20shown%20both%20cleaning%20a%20carpet%20and%20simultaneously%20dumping%20dirt%20onto%20it.%20The%20vacu.png?raw=true\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yajIZlU0-isd"
      },
      "source": [
        "### 4.3.1 The erratic vacuum world\n",
        "\n",
        "- **Concept of the Erratic Vacuum World**:\n",
        "   - The Erratic Vacuum World is a hypothetical environment used to illustrate the challenges of nondeterministic actions in AI search problems.\n",
        "   - It involves a vacuum cleaner agent operating in a simple environment where actions do not always have the intended effects.\n",
        "- **Nondeterministic Nature of Actions**:\n",
        "   - In this world, the actions of the vacuum cleaner have uncertain outcomes. For example, when the vacuum attempts to clean a square, the outcome is not always predictable.\n",
        "- **Transition Model**:\n",
        "   - A transition model is used to define the outcomes of actions in this nondeterministic environment.\n",
        "   - It specifies the probability of transitioning to different states given a particular action and current state.\n",
        "   - For instance, the transition model will detail what happens when the vacuum tries to clean a square, including the possibility of the dirt disappearing, the dirt remaining, or the dirt spreading to adjacent squares.\n",
        "- **Handling Uncertainty**:\n",
        "   - The erratic behavior in the vacuum world requires the agent to consider multiple potential outcomes for each action and plan accordingly.\n",
        "   - The agent needs to form a belief state that represents all possible configurations of dirt and clean squares.\n",
        "- **Goal of the Vacuum Agent**:\n",
        "   - The main objective for the vacuum cleaner agent in this world is to find a strategy that will clean all the squares, considering the unpredictability of its actions.\n",
        "- **Planning and Conditional Actions**:\n",
        "   - Planning in the erratic vacuum world involves creating conditional actions to adapt to the different outcomes that might occur.\n",
        "   - The agent must be prepared to revise its plan based on the actual results of its actions.\n",
        "- **Illustration of Real-World Challenges**:\n",
        "   - The Erratic Vacuum World serves as a simplified model for real-world problems where actions have uncertain and varying outcomes, making it a useful scenario for studying nondeterministic search and planning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqOfPTxN-isd"
      },
      "source": [
        "### 4.3.2 AND–OR search trees.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Andortree.png/638px-Andortree.png\" width=\"400\">\n",
        "\n",
        "- **Concept of AND–OR Search Trees**:\n",
        "   - AND–OR search trees are used in AI to model decision-making processes in environments with nondeterministic actions.\n",
        "   - They represent the branching of possibilities that an agent faces, with AND nodes representing actions and OR nodes representing different outcomes of those actions.\n",
        "- **Structure of the Tree**:\n",
        "   - In an AND–OR search tree, an AND node indicates that the agent must achieve all of its children nodes (outcomes) to succeed, simulating sequential actions.\n",
        "   - An OR node, on the other hand, represents a choice point where any one of the children nodes can lead to success, reflecting different possible states or actions available to the agent.\n",
        "- **Solution in AND–OR Trees**:\n",
        "   - A solution in an AND–OR search tree is a subtree that includes a choice (OR node) for every contingency and ensures success for all sequences of nondeterministic outcomes (AND nodes).\n",
        "- **Application in Nondeterministic Environments**:\n",
        "   - These trees are particularly useful in planning and decision-making in environments where actions can have multiple possible outcomes, requiring a strategy that covers all bases.\n",
        "- **Goal-Driven Search**:\n",
        "   - The search in AND–OR trees is goal-driven, often using a backward search from the goal to the start state, considering all possible ways to reach the goal.\n",
        "- **Complexity and Computation**:\n",
        "   - The complexity of AND–OR search trees can be significant due to the branching factor of both AND and OR nodes, making efficient search strategies and pruning methods important.\n",
        "- **Strategies for Traversing the Tree**:\n",
        "   - Strategies like depth-first search, breadth-first search, or heuristic methods can be applied to effectively traverse and find solutions in AND–OR search trees.\n",
        "- **Use in Real-World Scenarios**:\n",
        "   - AND–OR trees are applicable in real-world scenarios such as automated planning, where an agent must consider a sequence of actions and their possible outcomes to achieve a goal.\n",
        "- **Graphical Representation of Decision Processes**:\n",
        "   - These trees offer a graphical representation of decision processes in AI, helping to visualize and analyze the decision-making strategy in complex environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFiDp5HE-ise"
      },
      "source": [
        "### 4.3.3 Try, try again\n",
        "\n",
        "- **Concept of 'Try, Try Again'**:\n",
        "   - This approach is based on the principle of repeatedly attempting an action until a desired outcome is achieved, particularly in environments where actions do not always yield consistent results.\n",
        "- **The Slippery Vacuum World**:\n",
        "   - The slippery vacuum world is a hypothetical scenario used to illustrate the 'try, try again' approach. In this environment, the vacuum cleaner's actions are nondeterministic and may not always lead to the intended effect.\n",
        "   - For example, when the vacuum attempts to suck up dirt, it might not always succeed on the first try due to the 'slippery' nature of the environment.\n",
        "- **Handling Nondeterministic Actions**:\n",
        "   - In such a world, an agent must be prepared to repeat actions, as a single attempt may not guarantee success.\n",
        "   - This requires the agent to continually assess the state of the environment and adapt its actions accordingly.\n",
        "- **Cyclic Solution**:\n",
        "   - A cyclic solution refers to a strategy where the agent repeats a set of actions in a cycle until the goal is achieved.\n",
        "   - In the slippery vacuum world, this might mean repeatedly trying to clean a square until it is confirmed to be clean.\n",
        "- **Importance of Feedback**:\n",
        "   - For the 'try, try again' method to work, the agent must have a way to receive feedback about the current state of the environment to know whether to continue or stop an action.\n",
        "- **Efficiency Considerations**:\n",
        "   - While effective in certain scenarios, this approach can be inefficient if the probability of action success is very low or if the environment changes in ways that make repeated actions futile.\n",
        "- **Adaptation and Learning**:\n",
        "   - In more advanced applications, agents can adapt their strategies based on past experiences, learning the most effective ways to deal with the nondeterministic nature of the environment.\n",
        "- **Real-World Applications**:\n",
        "   - Similar strategies can be observed in real-world scenarios where outcomes are uncertain, such as in network communication (retransmitting packets until an acknowledgment is received) or robotic manipulation (re-attempting to grasp an object)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXdtWkS_-ise"
      },
      "source": [
        "## 4.4 Search in Partially Observable Environments.\n",
        "\n",
        "- **Characteristics of Partially Observable Environments**:\n",
        "   - In partially observable environments, an agent does not have access to the complete state of the environment at all times.\n",
        "   - This lack of complete information introduces significant challenges for decision-making and search strategies.\n",
        "- **Belief States in Partial Observability**:\n",
        "   - A belief state is a representation of all possible actual states the environment might be in, based on the agent's observations and knowledge.\n",
        "   - Agents in partially observable environments must make decisions based on belief states rather than actual states.\n",
        "- **Importance of Sensory Information**:\n",
        "   - In such environments, sensory information becomes crucial as it helps update the belief state, allowing the agent to better understand the environment.\n",
        "- **Strategies for Dealing with Uncertainty**:\n",
        "   - Agents must employ strategies that account for uncertainty, often involving probabilities and estimations to update and maintain their belief states.\n",
        "- **Planning and Decision Making**:\n",
        "   - Planning in partially observable environments often involves considering sequences of actions that are contingent on possible future observations.\n",
        "   - Decision-making becomes more complex as it must account for various possible scenarios due to the incomplete information.\n",
        "- **Use of Probabilistic Models**:\n",
        "   - Probabilistic models like Bayesian networks are often used to manage and reason about the uncertainty in partially observable environments.\n",
        "- **Sensor Models**:\n",
        "   - Sensor models are used to interpret sensory information. They define how observations are related to the actual state of the world.\n",
        "- **Challenges in Navigation and Mapping**:\n",
        "   - Tasks such as navigation and mapping are significantly more challenging in these environments because the agent must infer unobserved aspects of the world from limited observations.\n",
        "- **Real-World Applications**:\n",
        "   - Applications include robotic navigation in unknown or dynamic environments, medical diagnosis systems, and decision-making systems in uncertain business or economic conditions.\n",
        "- **Adaptive Behaviors**:\n",
        "   - Agents often need to exhibit adaptive behaviors, learning and updating their strategies as more information becomes available through interaction with the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYffenHm-ise"
      },
      "source": [
        "<img src=\"https://github.com/ValRCS/RBS_PBM773_Introduction_to_AI/blob/main/img/ch4_search_in_complex_environments/DALL%C2%B7E%202024-01-18%2000.33.32%20-%20Draw%20an%20illustration%20of%20a%20Mars%20rover%20exploring%20the%20red%20planet.%20The%20rover%20should%20be%20depicted%20on%20the%20Martian%20surface,%20equipped%20with%20various%20scientific%20i.png?raw=true\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pocLoeGq-ise"
      },
      "source": [
        "### 4.4.1 Searching with no observations\n",
        "\n",
        "- **Concept of Searching with No Observation**:\n",
        "   - This type of search occurs in environments where the agent receives no observations about the current state of the world after the initial state.\n",
        "- **Sensorless (Conformant) Problems**:\n",
        "   - In sensorless or conformant problems, the agent must make decisions without any sensory feedback. The agent knows the initial state and the effects of its actions but receives no updates during execution.\n",
        "   - The challenge is to find a sequence of actions that guarantees the goal is reached, regardless of the initial uncertainty.\n",
        "- **Real-Life Examples of Sensorless Problems**:\n",
        "   - **Industrial Processes**: Situations where direct monitoring is impossible due to constraints like high temperatures or hazardous materials.\n",
        "   - **Space Exploration**: Operations on distant planets where real-time sensory feedback is impractical.\n",
        "   - **Network Routing**: Situations where feedback on the state of the network is delayed or unavailable.\n",
        "- **Coercion**:\n",
        "   - Coercion refers to the process of narrowing down the possible states of the world by performing actions that force the world into fewer states, even without direct observation.\n",
        "- **Incremental Belief-State Search**:\n",
        "   - This approach involves incrementally building a belief state (a set of possible states the world could be in) and searching through this space for a path to the goal.\n",
        "   - It's a systematic way to handle the uncertainty by considering all possible outcomes of actions.\n",
        "- **Challenges in Sensorless Search**:\n",
        "   - The main challenge is the potentially vast number of states that might need to be considered, making the search space large.\n",
        "   - Designing a sequence of actions that is guaranteed to work in all possible initial states can be complex.\n",
        "- **Applications in Automated Planning**:\n",
        "   - Sensorless search is relevant in automated planning where actions must be devised to handle every contingency in environments without sensory feedback.\n",
        "- **Strategies for Sensorless Search**:\n",
        "   - Strategies include planning for the worst-case scenario, reducing the belief state size by coercion, and using heuristics to guide the search.\n",
        "- **Importance of Action Effects**:\n",
        "   - In sensorless environments, understanding the effects of actions is crucial since it's the only way to infer changes in the world.\n",
        "- **Use in Risk Management and Safety-Critical Systems**:\n",
        "   - Such search strategies are particularly important in risk management and safety-critical systems, where decisions must be made without complete information to ensure safety and reliability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRj5UW8e-ise"
      },
      "source": [
        "### 4.4.2 Searching in partially observable environments.\n",
        "\n",
        "- **Complexity of Partially Observable Environments**:\n",
        "   - Searching in partially observable environments is complex due to the uncertainty about the current state. The agent does not have complete information about the world and must make decisions based on partial observations.\n",
        "- **Belief State Representation**:\n",
        "   - In these environments, the agent maintains a belief state—a representation of all possible actual states based on its knowledge and observations.\n",
        "- **Three Stages of Belief State Management**:\n",
        "   - **Prediction Stage**: In this stage, the agent predicts the outcome of its actions based on its current belief state. It considers all possible states it might be in and predicts the outcomes of its actions in each of these states.\n",
        "   - **Possible Percepts Stage**: After executing an action, the agent considers all possible percepts (observations) it might receive as a result of the action. This stage helps in anticipating the range of possible scenarios that could arise.\n",
        "   - **Update Stage**: Once an actual percept is received, the agent updates its belief state. This update involves incorporating the new information provided by the percept and revising the belief state to reflect this new knowledge.\n",
        "- **Search Strategy**:\n",
        "   - The search strategy in such environments often involves a contingency plan that specifies actions for different possible percepts at each stage.\n",
        "- **Challenges of Uncertainty and Incomplete Information**:\n",
        "   - Dealing with the uncertainty inherent in partially observable environments requires sophisticated strategies to ensure that actions are effective even when based on incomplete information.\n",
        "- **Use of Probabilistic Reasoning**:\n",
        "   - Probabilistic reasoning plays a key role in managing belief states and making decisions under uncertainty.\n",
        "- **Real-World Applications**:\n",
        "   - Applications include robotic navigation where sensors cannot fully observe the environment, decision-making in financial markets, and medical diagnosis where all symptoms and their causes are not directly observable.\n",
        "- **Planning and Execution**:\n",
        "   - The planning process must account for various possible outcomes and observations, making execution adaptive to the actual observations received.\n",
        "- **Computational Complexity**:\n",
        "   - Searching in partially observable environments can be computationally demanding due to the need to consider multiple possible states and outcomes.\n",
        "- **Importance of Accurate Models**:\n",
        "   - Accurate models of the environment and the effects of actions are crucial for effective prediction and updating of belief states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0lVAc4R-isf"
      },
      "source": [
        "### 4.4.3 Solving partially observable problems.\n",
        "\n",
        "- **Complexity of Partially Observable Problems**:\n",
        "   - Solving problems in partially observable environments is challenging due to the uncertainty about the state of the world. The agent must make decisions with incomplete information.\n",
        "- **Belief State Approach**:\n",
        "   - The agent maintains a belief state, a set of all possible states the world might be in, based on available information and observations.\n",
        "- **Plan-Based Approach**:\n",
        "   - One approach involves creating plans that specify actions for all possible contingencies. These plans must be robust to all possible sequences of observations.\n",
        "- **Sensor Models and State Estimation**:\n",
        "   - Accurate sensor models are crucial for interpreting observations and updating belief states. State estimation methods are used to deduce the most likely current state of the world.\n",
        "- **Decision-Theoretic Planning**:\n",
        "   - Decision-theoretic planning involves calculating the expected utility of actions, considering all possible outcomes and their probabilities. This approach is used to select the best action under uncertainty.\n",
        "- **POMDPs (Partially Observable Markov Decision Processes)**:\n",
        "   - POMDPs provide a formal framework for decision-making in partially observable environments, combining probabilistic state transitions, rewards, and observations.\n",
        "- **Heuristic Search in Belief Space**:\n",
        "   - Heuristic search methods can be applied in belief space to find good plans efficiently. Heuristics guide the search by estimating the desirability of belief states.\n",
        "- **Challenges in Large-Scale Problems**:\n",
        "   - For large-scale problems, the size of the belief space can be prohibitively large, making search and planning computationally expensive.\n",
        "- **Approximation and Simplification Techniques**:\n",
        "   - Techniques such as sampling, state aggregation, and abstraction are used to simplify the belief space and make the problem more tractable.\n",
        "- **Real-World Applications**:\n",
        "   - Such approaches are applicable in various domains like robotics (navigating under uncertainty), automated control systems (in unpredictable environments), and strategic planning in uncertain market conditions.\n",
        "- **Importance of Adaptive Behavior**:\n",
        "   - In partially observable environments, adaptive behavior is key. The agent must continuously update its belief state and revise its plans based on new information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-86c30l-isf"
      },
      "source": [
        "### 4.4.4 An agent for partially observable environments\n",
        "\n",
        "- **Designing Agents for Partial Observability**:\n",
        "   - Agents operating in partially observable environments need specialized strategies to handle the lack of complete information about the state of the world.\n",
        "- **Monitoring**:\n",
        "   - Monitoring involves keeping track of the environment through sensors. The agent must interpret sensor data to understand changes in the environment and its own state.\n",
        "- **Filtering**:\n",
        "   - Filtering is the process of continuously updating the agent's belief state based on incoming observations. It includes techniques like the Kalman filter and particle filter.\n",
        "   - The goal of filtering is to refine the agent's understanding of the environment over time, incorporating new sensor data as it becomes available.\n",
        "- **State Estimation**:\n",
        "   - State estimation involves deducing the most probable current state of the world based on the history of actions and observations.\n",
        "   - This process is crucial for decision-making in partially observable environments, as the agent often has to act based on these estimates.\n",
        "- **Localization**:\n",
        "   - Localization is a specific form of state estimation where the agent determines its own location within the environment.\n",
        "   - This is particularly important in robotics and navigation, where an agent must orient itself in an environment with incomplete information.\n",
        "- **Dynamic Environments**:\n",
        "   - The agent must be capable of handling dynamic changes in the environment, adjusting its belief state and strategies as new data comes in.\n",
        "- **Probabilistic Reasoning**:\n",
        "   - Probabilistic reasoning underpins the agent's decision-making process, enabling it to make informed decisions despite uncertainty.\n",
        "- **Balancing Exploration and Exploitation**:\n",
        "   - The agent needs to balance exploration (gathering more information about the environment) with exploitation (making decisions based on current knowledge).\n",
        "- **Computational Challenges**:\n",
        "   - Creating efficient algorithms for monitoring, filtering, state estimation, and localization in real-time is computationally challenging, especially in complex environments.\n",
        "- **Real-World Implementation**:\n",
        "   - Implementing these concepts in real-world agents involves a combination of software (for decision-making, data processing) and hardware (sensors, actuators).\n",
        "- **Adaptive and Learning Capabilities**:\n",
        "   - Ideally, agents in partially observable environments should have adaptive and learning capabilities to improve their performance over time based on experience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxTwmiSy-isf"
      },
      "source": [
        "<img src=\"https://github.com/ValRCS/RBS_PBM773_Introduction_to_AI/blob/main/img/ch4_search_in_complex_environments/DALL%C2%B7E%202024-01-18%2000.22.16%20-%20Draw%20an%20illustration%20of%20a%20robot%20trying%20to%20map%20an%20unknown%20building.%20The%20robot%20should%20be%20equipped%20with%20various%20sensors%20and%20mapping%20tools,%20actively%20scann.png?raw=true\" width=\"400\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R5yL8Mq-isf"
      },
      "source": [
        "## 4.5 Online Search Agents and Unknown Environments.\n",
        "\n",
        "- **Online vs. Offline Search**:\n",
        "   - **Offline Search**: In offline search, the entire search process is completed before any action is taken. The agent first devises a complete plan based on its initial knowledge of the environment, and then executes the plan. This approach assumes that the agent has complete and accurate knowledge about the environment and the effects of its actions.\n",
        "   - **Online Search**: Online search agents, on the other hand, plan and execute actions simultaneously. They make decisions at each step based on current knowledge and then update their knowledge based on the outcome of their actions. This approach is necessary in environments where the agent cannot predict all aspects of the environment in advance or when the environment is dynamic and changes over time.\n",
        "- **Characteristics of Online Search Agents**:\n",
        "   - Online search agents operate in environments where they may not have complete information about the state of the world or the outcomes of their actions.\n",
        "   - These agents must be able to react to new information and adapt their plans accordingly, often making decisions with limited or incomplete knowledge.\n",
        "- **Mapping Problem**:\n",
        "   - The mapping problem refers to the challenge of constructing a representation (or map) of the environment based on partial or incremental observations.\n",
        "   - In online search, agents often need to map their environment while navigating it, which adds a layer of complexity to their task. This is especially common in robotic exploration, where the robot must build a map of an unknown area while simultaneously navigating through it.\n",
        "- **Decision Making in Online Search**:\n",
        "   - Online search agents must make decisions that balance exploration (gathering more information about the environment) and exploitation (using current knowledge to achieve goals).\n",
        "   - They need to be robust to uncertainties and changes in the environment and must be able to update their strategies based on real-time feedback.\n",
        "- **Applications of Online Search**:\n",
        "   - Online search is particularly relevant in dynamic or unpredictable environments, such as robotic navigation in unknown territories, real-time gaming, or decision-making in rapidly changing market conditions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F10s3iEq-isf"
      },
      "source": [
        "### 4.5.1 Online search problems.\n",
        "\n",
        "- **Nature of Online Search Problems**:\n",
        "   - Online search problems involve making decisions and acting without complete information about the environment, requiring the agent to learn and adapt as it goes.\n",
        "- **Competitive Ratio**:\n",
        "   - The competitive ratio is a measure used to evaluate the performance of an online algorithm relative to an optimal offline algorithm. It compares the cost incurred by the online algorithm to the cost of an optimal solution known only in hindsight.\n",
        "   - It provides a way to quantify how well an online search strategy performs in the worst-case scenario compared to an ideal strategy with full information.\n",
        "- **Dead Ends**:\n",
        "   - Dead ends are situations in an online search where the agent reaches a state from which it cannot reach the goal. These are particularly problematic because the agent might not recognize a dead end until it's too late.\n",
        "   - Avoiding dead ends or having strategies to handle them when encountered is crucial in online search.\n",
        "- **Adversary Argument**:\n",
        "   - The adversary argument is a method of analysis in online algorithms where the worst-case scenario is considered. It assumes an 'adversary' that makes the environment as challenging as possible within the rules of the problem.\n",
        "   - This approach helps in designing algorithms that are robust against the most challenging conditions.\n",
        "- **Irreversible Actions**:\n",
        "   - Irreversible actions are actions that, once taken, cannot be undone. These actions add significant complexity to online search problems since the agent must consider the long-term consequences of its actions.\n",
        "   - Strategies for dealing with irreversible actions often involve careful planning and risk assessment.\n",
        "- **Safely Explorable**:\n",
        "   - A safely explorable environment is one where the agent can explore without getting into states that are harmful or impossible to recover from (i.e., without encountering dead ends or irreversible actions).\n",
        "   - In such environments, the agent has more freedom to explore and gather information, which can be crucial for successful online search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o5NkDCg-isf"
      },
      "source": [
        "### 4.5.2 Online search agents\n",
        "\n",
        "- **Role of Online Search Agents**:\n",
        "   - Online search agents operate in environments where they must make decisions with incomplete information, adapting their strategy based on real-time feedback and discoveries.\n",
        "- **Action Selection in Real-Time**:\n",
        "   - These agents select actions based on current knowledge, without a complete plan of the entire search space. They must continually assess and react to new information as they navigate through the environment.\n",
        "- **Depth-First Search in Online Context**:\n",
        "   - Depth-first search can be adapted for online search, where the agent explores as far as possible along a path before backtracking. This method is useful in environments where the space is too large to search completely and where the agent needs to make quick decisions.\n",
        "   - However, in an online setting, depth-first search must be carefully managed to avoid getting stuck in deep paths that don't lead to the goal.\n",
        "\n",
        "   <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/TnsiyngUC4o?si=TWiHVDdLrSy3KudW\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
        "   \n",
        "- **Handling Unknown Environments**:\n",
        "   - Online search agents are particularly suited for unknown or dynamic environments where it is not feasible to have a complete, predefined plan.\n",
        "   - They must be capable of exploring the environment while avoiding hazards and dead ends.\n",
        "- **Learning from Experience**:\n",
        "   - Ideally, online search agents learn from their experiences, improving their performance over time by adapting their search strategy based on past outcomes.\n",
        "- **Balancing Exploration and Exploitation**:\n",
        "   - A key challenge for these agents is balancing exploration (gathering new information about the environment) with exploitation (using known information to reach goals).\n",
        "- **Computational Efficiency**:\n",
        "   - Online search agents must be computationally efficient, as they often need to operate in real-time and make quick decisions.\n",
        "- **Application Areas**:\n",
        "   - These agents are widely used in robotics, autonomous vehicles, and real-time strategy games, where environments are complex and continually changing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DROLiLhK-isf"
      },
      "source": [
        "### 4.5.3 Online local search\n",
        "\n",
        "- **Concept of Online Local Search**:\n",
        "   - Online local search involves making sequential decisions in real-time, with the agent improving its understanding of the environment through continual exploration and feedback.\n",
        "- **Random Walk**:\n",
        "   - A random walk is a simple online local search strategy where the agent chooses its next action randomly.\n",
        "   - While not efficient for all problems, random walks can sometimes be surprisingly effective, especially in environments where structured search strategies might get stuck in local optima.\n",
        "- *\n",
        "Learning Real-Time A* (LRTA\n",
        ")**:\n",
        "   - LRTA* is a significant algorithm in online local search. It combines the ideas of A* search and learning to improve performance over time.\n",
        "   - As the agent navigates the environment, LRTA* updates its heuristic estimates based on the actual costs experienced, leading to more informed decisions in the future.\n",
        "   - LRTA* is particularly effective in scenarios where the agent must operate repeatedly in the same environment, allowing it to learn and adapt from past experiences.\n",
        "- **Optimism Under Uncertainty**:\n",
        "   - Optimism under uncertainty is a principle where the agent prefers actions that lead to less-known states, under the assumption that these states might lead to better outcomes.\n",
        "   - This approach encourages exploration, helping the agent discover new paths and solutions that it might otherwise overlook due to lack of information.\n",
        "- **Balancing Exploration and Exploitation**:\n",
        "   - In online local search, the agent must balance exploration (discovering new information about the environment) with exploitation (using existing knowledge to achieve goals).\n",
        "   - Finding the right balance is crucial for the efficiency and effectiveness of the search process.\n",
        "- **Adaptive Heuristics**:\n",
        "   - Online local search often involves adaptive heuristics that evolve as the agent learns more about the environment. These heuristics guide the search process, improving over time as the agent gathers more data.\n",
        "- **Application in Complex Environments**:\n",
        "   - Online local search is particularly relevant in complex, dynamic environments where precomputed plans are impractical or impossible due to the unpredictability and vastness of the search space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBZTMUUy-isg"
      },
      "source": [
        "### 4.5.4 Learning in online search\n",
        "\n",
        "- **Role of Learning in Online Search**:\n",
        "   - Learning is a crucial aspect of online search, as it allows agents to improve their performance over time by adapting based on experience and new information.\n",
        "- **Incremental Search**:\n",
        "   - Incremental search refers to the process where an agent refines its search strategy incrementally as it learns from each step or episode.\n",
        "   - This approach contrasts with traditional search methods that attempt to find a complete solution in one go. Instead, incremental search builds upon the knowledge gained from each action to improve future decisions.\n",
        "- **Adapting to Changes and Uncertainty**:\n",
        "   - Online search agents often operate in dynamic environments where changes can occur, or in situations with inherent uncertainty. Learning allows these agents to adapt their strategies to better handle such conditions.\n",
        "- **Use of Feedback in Learning**:\n",
        "   - Feedback from the environment, whether through success, failure, or the discovery of new information, is a key driver for learning in online search.\n",
        "   - The agent uses this feedback to adjust its understanding of the environment and to refine its decision-making process.\n",
        "- **Enhancing Heuristics**:\n",
        "   - One way agents learn in online search is by enhancing their heuristic functions. Over time, the agent develops more accurate estimations for the cost or value of different actions and states.\n",
        "- **Efficiency Improvements**:\n",
        "   - As the agent learns, it typically becomes more efficient in its search, reducing the time and resources needed to find solutions or achieve goals.\n",
        "- **Memory and Experience Utilization**:\n",
        "   - Online search agents may store past experiences and use this memory to inform future decisions. This can involve recognizing previously encountered states and knowing which actions were effective.\n",
        "- **Applications of Learning in Online Search**:\n",
        "   - Learning in online search is applicable in areas like robotic navigation, where the robot learns the most efficient paths over time, and in web search algorithms that adapt to user preferences and behaviors.\n",
        "- **Challenges in Learning**:\n",
        "   - One of the challenges in learning for online search is ensuring that the agent's learning process leads to continual improvement and does not get stuck in suboptimal strategies.\n",
        "- **Generalization and Pattern Recognition**:\n",
        "   - Agents can generalize from past experiences to new, but similar situations, enhancing their ability to handle a wide range of scenarios effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx_r8PhE-isg"
      },
      "source": [
        "## Extra bibliographical and historical notes\n",
        "\n",
        "- **Tabu Search**:\n",
        "   - Tabu search is a metaheuristic search method designed to avoid the trap of local optima. Introduced by Fred Glover in the 1980s, it enhances local search by keeping track of recently visited solutions (tabu list) and forbidding or penalizing moves that revisit them.\n",
        "   - The innovation of tabu search lies in its use of memory structures that describe the visited solutions, allowing the search to explore solution space more freely without getting stuck in cycles.\n",
        "- **Heavy-Tailed Distributions**:\n",
        "   - The concept of heavy-tailed distributions in the context of search problems relates to the observation that certain search procedures have a non-negligible probability of taking a very long time to complete.\n",
        "   - This understanding has influenced the design of search algorithms, particularly in the field of combinatorial optimization, where it's been observed that tweaking the algorithm parameters can significantly affect the tail behavior of the runtime distribution, sometimes leading to more efficient overall performance.\n",
        "- **Eulerian Graphs**:\n",
        "   - The concept of Eulerian graphs originates from the work of Leonhard Euler, a pioneering mathematician in graph theory. An Eulerian graph is one in which there exists a path that uses every edge exactly once (Eulerian path) or a cycle that uses every edge exactly once (Eulerian circuit).\n",
        "   - The relevance of Euler's work to search problems lies in the formulation and solution of routing and pathfinding problems, where the goal can be to traverse each part of a graph in an efficient manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtrkIh9J-isi"
      },
      "source": [
        "## Summary and conclusions\n",
        "\n",
        "- **Search in Complex Environments**:\n",
        "   - Search is a fundamental problem-solving technique in AI. It involves finding a sequence of actions that leads to a goal state or satisfies a given set of constraints.\n",
        "   - Complex environments often require sophisticated search strategies that can handle uncertainty, partial observability, and dynamic changes.\n",
        "- **Search Algorithms**:\n",
        "    - Search algorithms can be classified based on the type of search space (discrete or continuous), the nature of actions (deterministic or nondeterministic), and the availability of information (fully observable or partially observable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo2GI6Jw-isi"
      },
      "source": [
        "## Implementing a simple hill climbing algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "giNMAYA_-isi"
      },
      "outputs": [],
      "source": [
        "# let's implement a simple hill climbing algorithm\n",
        "# we will have 3 functions\n",
        "# one function the objective function with one parameter f(x)=-x^2+3x+5\n",
        "# one function to find the best neighbor\n",
        "# one function to find the best solution with the hill climbing algorithm (provide the number of iterations as a parameter)\n",
        "\n",
        "import random\n",
        "import math\n",
        "\n",
        "def objective_function(x):\n",
        "    return -x**2+3*x+5\n",
        "\n",
        "def find_best_neighbor(x, objective_function, step_size=0.1):\n",
        "    best_neighbor = x\n",
        "    best_neighbor_value = objective_function(x)\n",
        "    for i in [-step_size, step_size ]: # we check in both directions since we only have single x variable\n",
        "        neighbor = x + i\n",
        "        neighbor_value = objective_function(neighbor)\n",
        "        if neighbor_value > best_neighbor_value:\n",
        "            best_neighbor_value = neighbor_value\n",
        "            best_neighbor = neighbor\n",
        "    return best_neighbor, best_neighbor_value\n",
        "\n",
        "def hill_climbing(x, obj_fun = objective_function, iterations=25, step_size=0.1, debug=False, min_improvement=0.0001):\n",
        "    results = []\n",
        "    for i in range(iterations):\n",
        "        old_value = obj_fun(x)\n",
        "        x, value = find_best_neighbor(x, objective_function, step_size)\n",
        "        results.append((x, value))\n",
        "        if value - old_value < min_improvement:\n",
        "            print(f\"Converged after {i} iterations\")\n",
        "            break\n",
        "        if debug:\n",
        "            print(\"x: {}, value: {}\".format(x, value))\n",
        "\n",
        "    return x, value, results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wSBVZo0G-isk",
        "outputId": "4fff3406-25e7-4614-99f9-86f15e919e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: 1.1, value: 7.09\n",
            "x: 1.2000000000000002, value: 7.16\n",
            "x: 1.3000000000000003, value: 7.21\n",
            "x: 1.4000000000000004, value: 7.24\n",
            "Converged after 4 iterations\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.5000000000000004,\n",
              " 7.25,\n",
              " [(1.1, 7.09),\n",
              "  (1.2000000000000002, 7.16),\n",
              "  (1.3000000000000003, 7.21),\n",
              "  (1.4000000000000004, 7.24),\n",
              "  (1.5000000000000004, 7.25)])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "hill_climbing(1, iterations=25, step_size=0.1, debug=True, min_improvement=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's start hill_climbing with 0\n",
        "x, y, results = hill_climbing(0, iterations=25, step_size=0.1, debug=True, min_improvement=0.01)\n",
        "results"
      ],
      "metadata": {
        "id": "DW1ab3DoKYwn",
        "outputId": "2b23d059-666b-49b2-8f6c-6e707b8ad86e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: 0.1, value: 5.29\n",
            "x: 0.2, value: 5.5600000000000005\n",
            "x: 0.30000000000000004, value: 5.8100000000000005\n",
            "x: 0.4, value: 6.04\n",
            "x: 0.5, value: 6.25\n",
            "x: 0.6, value: 6.4399999999999995\n",
            "x: 0.7, value: 6.609999999999999\n",
            "x: 0.7999999999999999, value: 6.76\n",
            "x: 0.8999999999999999, value: 6.89\n",
            "x: 0.9999999999999999, value: 7.0\n",
            "x: 1.0999999999999999, value: 7.09\n",
            "x: 1.2, value: 7.16\n",
            "x: 1.3, value: 7.21\n",
            "x: 1.4000000000000001, value: 7.24\n",
            "Converged after 14 iterations\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.1, 5.29),\n",
              " (0.2, 5.5600000000000005),\n",
              " (0.30000000000000004, 5.8100000000000005),\n",
              " (0.4, 6.04),\n",
              " (0.5, 6.25),\n",
              " (0.6, 6.4399999999999995),\n",
              " (0.7, 6.609999999999999),\n",
              " (0.7999999999999999, 6.76),\n",
              " (0.8999999999999999, 6.89),\n",
              " (0.9999999999999999, 7.0),\n",
              " (1.0999999999999999, 7.09),\n",
              " (1.2, 7.16),\n",
              " (1.3, 7.21),\n",
              " (1.4000000000000001, 7.24),\n",
              " (1.5000000000000002, 7.25)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's visualize our objective function for x from -5 to 15\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.linspace(-2, 3,50)\n",
        "y = objective_function(x)\n",
        "plt.plot(x, y)\n",
        "# show grid\n",
        "plt.grid(True)\n",
        "# set labels\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "# let's plot red dots for our results\n",
        "plt.scatter([result[0] for result in results], [result[1] for result in results], color='red')\n",
        "\n",
        "# let's plot dots from our\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aqQWwJ5RKpZu",
        "outputId": "ea20342b-95c2-4738-b34a-053ff0a954cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ho11uTP7LKY9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "s4zU2PNZ-isk",
        "outputId": "901d403a-695f-47e2-dd33-b25b70d7bfcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: 9.9, value: -63.31\n",
            "x: 9.8, value: -61.640000000000015\n",
            "x: 9.700000000000001, value: -59.99000000000001\n",
            "x: 9.600000000000001, value: -58.36000000000002\n",
            "x: 9.500000000000002, value: -56.75000000000002\n",
            "x: 9.400000000000002, value: -55.16000000000004\n",
            "x: 9.300000000000002, value: -53.590000000000046\n",
            "x: 9.200000000000003, value: -52.04000000000005\n",
            "x: 9.100000000000003, value: -50.51000000000005\n",
            "x: 9.000000000000004, value: -49.00000000000006\n",
            "x: 8.900000000000004, value: -47.510000000000055\n",
            "x: 8.800000000000004, value: -46.040000000000056\n",
            "x: 8.700000000000005, value: -44.59000000000007\n",
            "x: 8.600000000000005, value: -43.16000000000007\n",
            "x: 8.500000000000005, value: -41.75000000000007\n",
            "x: 8.400000000000006, value: -40.360000000000085\n",
            "x: 8.300000000000006, value: -38.99000000000008\n",
            "x: 8.200000000000006, value: -37.640000000000086\n",
            "x: 8.100000000000007, value: -36.310000000000095\n",
            "x: 8.000000000000007, value: -35.00000000000009\n",
            "x: 7.9000000000000075, value: -33.71000000000009\n",
            "x: 7.800000000000008, value: -32.4400000000001\n",
            "x: 7.700000000000008, value: -31.190000000000104\n",
            "x: 7.6000000000000085, value: -29.960000000000107\n",
            "x: 7.500000000000009, value: -28.750000000000107\n",
            "x: 7.400000000000009, value: -27.560000000000116\n",
            "x: 7.30000000000001, value: -26.390000000000114\n",
            "x: 7.20000000000001, value: -25.240000000000116\n",
            "x: 7.10000000000001, value: -24.110000000000113\n",
            "x: 7.000000000000011, value: -23.000000000000117\n",
            "x: 6.900000000000011, value: -21.910000000000117\n",
            "x: 6.800000000000011, value: -20.840000000000117\n",
            "x: 6.700000000000012, value: -19.79000000000012\n",
            "x: 6.600000000000012, value: -18.760000000000122\n",
            "x: 6.500000000000012, value: -17.750000000000128\n",
            "x: 6.400000000000013, value: -16.760000000000126\n",
            "x: 6.300000000000013, value: -15.790000000000127\n",
            "x: 6.2000000000000135, value: -14.840000000000128\n",
            "x: 6.100000000000014, value: -13.910000000000132\n",
            "x: 6.000000000000014, value: -13.000000000000128\n",
            "x: 5.900000000000015, value: -12.110000000000127\n",
            "x: 5.800000000000015, value: -11.240000000000126\n",
            "x: 5.700000000000015, value: -10.390000000000128\n",
            "x: 5.600000000000016, value: -9.560000000000127\n",
            "x: 5.500000000000016, value: -8.750000000000124\n",
            "x: 5.400000000000016, value: -7.960000000000129\n",
            "x: 5.300000000000017, value: -7.190000000000127\n",
            "x: 5.200000000000017, value: -6.440000000000126\n",
            "x: 5.100000000000017, value: -5.710000000000127\n",
            "x: 5.000000000000018, value: -5.000000000000124\n",
            "x: 4.900000000000018, value: -4.310000000000125\n",
            "x: 4.8000000000000185, value: -3.6400000000001214\n",
            "x: 4.700000000000019, value: -2.990000000000121\n",
            "x: 4.600000000000019, value: -2.3600000000001202\n",
            "x: 4.5000000000000195, value: -1.750000000000119\n",
            "x: 4.40000000000002, value: -1.1600000000001138\n",
            "Converged after 56 iterations\n"
          ]
        }
      ],
      "source": [
        "# now let's pass in a different function let's say f(x) = -x^3 + 3x^2 + 5x + 5\n",
        "def objective_function_2(x):\n",
        "    return -x**3 + 3*x**2 + 5*x + 5\n",
        "\n",
        "x, y, results = hill_climbing(x=10, obj_fun = objective_function_2, iterations=125, step_size=0.1, debug=True, min_improvement=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrWlgL9f-isl"
      },
      "source": [
        "### Take away from hill climbing implementation\n",
        "\n",
        "Where you end up depends on where you start!\n",
        "\n",
        "You will find SOME local minimum or maximum"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ideas for improving our primitive hill climbing approach\n",
        "\n",
        "* adjustable step! start with big steps and then lower the steps (Newton method uses that)\n",
        "* maybe some randomness (see simulated annealing from metallurgy approach)"
      ],
      "metadata": {
        "id": "9Dy9winNMW-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try at Home\n",
        "\n",
        "Create an adjustable hill climbing algorithm function that starts with big steps and then slows down.\n",
        "\n",
        "Try more complex functions such as x to the 4th, 5th and so on power, where you have multiple maxima or minima."
      ],
      "metadata": {
        "id": "zg4Fxy5yM-tA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOgK5FnV-isl"
      },
      "source": [
        "### Notes on hill climbing implemention\n",
        "\n",
        "In our case our function had a solution that can be found analytically and we can compare our results with analytical solution.\n",
        "\n",
        "In this case max is when gradient(first order derivative) is zero.\n",
        "Derivation of the function f(x)=-(x^2)+3x+5 is: f'(x)=-2x+3\n",
        "Thus our maximum will be at x=3/2=1.5 which matches our hill climbing result.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}