{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model\n",
    "\n",
    "Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (i.e. hidden) states. The hidden Markov model can be represented as the simplest dynamic Bayesian network. The mathematics behind the HMM was developed by L. E. Baum and coworkers.\n",
    "\n",
    "## Example\n",
    "\n",
    "We have two states in Markov Model - Rainy and Sunny. We have two observations - Walk and Shop. We have the following probabilities:\n",
    "\n",
    "- Initial probabilities: \n",
    "    - P(Rainy) = 0.6\n",
    "    - P(Sunny) = 0.4\n",
    "- Transition probabilities:\n",
    "    - P(Rainy|Rainy) = 0.7\n",
    "    - P(Sunny|Rainy) = 0.3\n",
    "    - P(Sunny|Sunny) = 0.6\n",
    "    - P(Rainy|Sunny) = 0.4\n",
    "- Emission probabilities:\n",
    "    - P(Walk|Rainy) = 0.1\n",
    "    - P(Shop|Rainy) = 0.9\n",
    "    - P(Walk|Sunny) = 0.6\n",
    "    - P(Shop|Sunny) = 0.4\n",
    "\n",
    "We have the following observations: Walk, Shop, Walk, Shop, Shop. We need to find the most probable sequence of states.\n",
    "\n",
    "## Solution\n",
    "\n",
    "We will use the Viterbi algorithm to solve this problem. The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states – called the Viterbi path – that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models.    \n",
    "\n",
    "The Viterbi algorithm is used to find the most likely sequence of hidden states. It is based on dynamic programming and the observation that the probability of the most likely path can be calculated recursively. The algorithm is used in many fields, including speech recognition, speech synthesis, part-of-speech tagging, gene prediction, and bioinformatics.\n",
    "\n",
    "### Pseudocode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]\n",
      "Numpy: 1.23.5\n",
      "Pandas: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "# so let's implement Hidden Markov Model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# prints python and library versions\n",
    "import sys\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('Numpy: {}'.format(np.__version__))\n",
    "print('Pandas: {}'.format(pd.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.0000000e-02 2.8000000e-01]\n",
      " [6.1600000e-02 3.7200000e-02]\n",
      " [2.9000000e-02 4.0800000e-03]\n",
      " [1.0966000e-02 1.1148000e-03]\n",
      " [3.2488480e-03 7.9173600e-04]\n",
      " [2.5908880e-04 1.0147872e-03]]\n",
      "[[0.0047374 0.0035344]\n",
      " [0.014851  0.009652 ]\n",
      " [0.0401    0.0272   ]\n",
      " [0.106     0.1      ]\n",
      " [0.28      0.46     ]\n",
      " [1.        1.       ]]\n",
      "[0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# now let's define the model\n",
    "class HiddenMarkovModel:\n",
    "    def __init__(self, A, B, pi):\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.pi = pi\n",
    "        self.N = A.shape[0]\n",
    "        self.M = B.shape[1]\n",
    "        \n",
    "    def forward(self, O):\n",
    "        T = len(O)\n",
    "        alpha = np.zeros((T, self.N))\n",
    "        alpha[0] = self.pi * self.B[:, O[0]]\n",
    "        for t in range(1, T):\n",
    "            alpha[t] = self.B[:, O[t]] * np.dot(alpha[t-1], self.A)\n",
    "        return alpha\n",
    "    \n",
    "    def backward(self, O):\n",
    "        T = len(O)\n",
    "        beta = np.zeros((T, self.N))\n",
    "        beta[T-1] = 1\n",
    "        for t in range(T-2, -1, -1):\n",
    "            beta[t] = np.dot(self.A, self.B[:, O[t+1]] * beta[t+1])\n",
    "        return beta\n",
    "    \n",
    "    def fit(self, O):\n",
    "        T = len(O)\n",
    "        alpha = self.forward(O)\n",
    "        beta = self.backward(O)\n",
    "        xi = np.zeros((T-1, self.N, self.N))\n",
    "        for t in range(T-1):\n",
    "            xi[t] = (alpha[t].reshape(-1, 1) * self.A * self.B[:, O[t+1]].reshape(1, -1) * beta[t+1].reshape(1, -1)) / np.dot(alpha[t], np.dot(self.A, self.B[:, O[t+1]] * beta[t+1]))\n",
    "        gamma = alpha * beta / np.sum(alpha * beta, axis=1).reshape(-1, 1)\n",
    "        return xi, gamma\n",
    "    \n",
    "    def viterbi(self, O):\n",
    "        T = len(O)\n",
    "        delta = np.zeros((T, self.N))\n",
    "        psi = np.zeros((T, self.N))\n",
    "        delta[0] = self.pi * self.B[:, O[0]]\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.N):\n",
    "                delta[t, j] = np.max(delta[t-1] * self.A[:, j]) * self.B[j, O[t]]\n",
    "                psi[t, j] = np.argmax(delta[t-1] * self.A[:, j])\n",
    "        q = np.zeros(T, dtype=np.int32)\n",
    "        q[T-1] = np.argmax(delta[T-1])\n",
    "        return q\n",
    "\n",
    "# let's define the model parameters\n",
    "A = np.array([[0.7, 0.3], [0.4, 0.6]]) # our transition matrix for the hidden states\n",
    "B = np.array([[0.1, 0.4, 0.5], [0.7, 0.2, 0.1]]) # our emission matrix for the observations\n",
    "pi = np.array([0.6, 0.4]) # initial state distribution\n",
    "\n",
    "# let's define the observations\n",
    "O = np.array([0, 1, 2, 2, 1, 0]) # so imagine them being 0=red, 1=green, 2=blue\n",
    "# let's create the model\n",
    "hmm = HiddenMarkovModel(A, B, pi)\n",
    "# let's do the forward algorithm\n",
    "alpha = hmm.forward(O)\n",
    "print(alpha)\n",
    "# let's do the backward algorithm\n",
    "beta = hmm.backward(O)\n",
    "print(beta)\n",
    "# let's do the viterbi algorithm\n",
    "q = hmm.viterbi(O)\n",
    "print(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so it is saying that the most likely sequence of hidden states is 0, 0, 0, 0, 0, 1\n",
    "\n",
    "# TODO add the Baum-Welch algorithm for training the model\n",
    "# TODO add mapping of numerical values to the actual states and observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
