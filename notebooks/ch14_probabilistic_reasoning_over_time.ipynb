{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14 - Probabilistical Reasoning Over Time\n",
    "\n",
    "In this chapter, we will learn about probabilistic reasoning over time. We will learn about the Markov assumption, the hidden Markov model, and the Kalman filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction - Contents\n",
    "\n",
    "- **Probabilistic Models:**  Introduction to models that handle uncertainty over time. \n",
    "- **Temporal Evolution:**  How the state of the world changes over time and how to model these changes. \n",
    "- **Key Challenges:**  Dealing with the dynamic nature of the environment and the accumulation of uncertainty.\n",
    "\n",
    "**Time and Uncertainty**  \n",
    "- **Temporal Probability Distributions:**  Use of probability distributions to represent the uncertainty of events over time. \n",
    "- **Markov Processes:**  The assumption that the future state depends only on the current state, not on the sequence of events that preceded it. \n",
    "- **Bayesian Networks:**  Extending the concept of Markov processes to model the probabilistic relationships between multiple variables over time.\n",
    "\n",
    "**Hidden Markov Models (HMMs)**  \n",
    "- **Definition and Components:**  Explanation of states, observations, transition model, sensor model, and the initial state distribution. \n",
    "- **Inference in HMMs:**  Techniques for computing the probability of a sequence of observations and determining the most likely sequence of states (decoding). \n",
    "- **Applications:**  Examples of HMMs in speech recognition, bioinformatics, and other areas.\n",
    "\n",
    "**Kalman Filters**  \n",
    "- **Continuous States:**  Introduction to dealing with continuous state spaces using Kalman filters. \n",
    "- **Linear Gaussian Models:**  The assumption that the world evolves in a linear way with Gaussian noise. \n",
    "- **Prediction and Update:**  The two-step process of predicting the future state and updating the prediction with new observations.\n",
    "\n",
    "**Dynamic Bayesian Networks (DBNs)**  \n",
    "- **Generalization of HMMs and Kalman Filters:**  How DBNs extend HMMs and Kalman Filters to handle more complex situations with multiple interrelated variables. \n",
    "- **Structure and Inference:**  Explanation of the graphical structure of DBNs and methods for performing inference. \n",
    "- **Applications:**  Use cases of DBNs in complex temporal modeling tasks.\n",
    "\n",
    "**Particle Filtering**  \n",
    "- **Nonlinear and Non-Gaussian Processes:**  Addressing situations where Kalman filters and DBNs are inadequate due to nonlinearity or non-Gaussian noise. \n",
    "- **Sampling and Representation:**  How particle filters use a set of samples (particles) to approximate the posterior distribution of states. \n",
    "- **Importance Sampling:**  Technique used within particle filtering to focus computational resources on more probable states.\n",
    "\n",
    "**Conclusion**  \n",
    "- **Choosing the Right Model:**  Discussion on the criteria for selecting between HMMs, Kalman filters, DBNs, and particle filters based on the characteristics of the problem at hand. \n",
    "- **Combining Temporal Models with Decision Making:**  Overview of how these probabilistic temporal models can be integrated with decision-making processes in AI systems.---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Introduction**  \n",
    "- **Interpreting Present, Understanding Past, Predicting Future:**  The chapter opens with the challenge of making sense of uncertain and partially observable environments, emphasizing the goal of interpreting the present, understanding the past, and predicting the future despite the lack of clear information. \n",
    "- **Belief State and Partial Observability:**  It discusses how agents in such environments maintain a belief state to represent possible world states based on their sensory inputs. This concept is foundational for tracking the current state to the extent allowed by the agents' sensors. \n",
    "- **Transition and Sensor Models:**  The introduction outlines how agents use a transition model to predict the world's evolution and a sensor model to update their belief state based on observed percepts. This process is central to managing the uncertainty inherent in dynamic environments. \n",
    "- **Evolution of Belief State Representation:**  The text contrasts earlier chapters' representation of belief states (as explicitly enumerated sets of states or logical formulas) with the probabilistic approach introduced in this chapter. The probabilistic method quantifies the degree of belief in different possible states, offering a nuanced understanding of uncertainty. \n",
    "- **Temporal Modeling with Probability Theory:**  The chapter plans to extend the representation of time and uncertainty using probability theory. It explains how a changing world is modeled with variables for each aspect of the world state at different times, incorporating uncertainty into transition and sensor models. \n",
    "- **Inference Tasks and Algorithms:**  Introduction of the basic inference tasks and the structure of inference algorithms for temporal models. This sets the stage for a deeper exploration of how to perform reasoning over time in uncertain conditions. \n",
    "- **Specific Models – HMMs, Kalman Filters, and DBNs:**  The chapter aims to cover three specific models for temporal reasoning: Hidden Markov Models (HMMs), Kalman Filters, and Dynamic Bayesian Networks (DBNs), with the latter including HMMs and Kalman Filters as special cases. These models are pivotal for understanding and predicting the behavior of systems over time in the face of uncertainty.\n",
    "\n",
    "This introduction sets the stage for a detailed exploration of probabilistic reasoning over time, emphasizing the significance of probabilistic models in interpreting, understanding, and predicting events in uncertain and partially observable environments.---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ValRCS/RBS_PBM773_Introduction_to_AI/main/img/ch14_probilistic_reasoning_over_time/DALL%C2%B7E%202024-02-21%2015.39.00%20-%20An%20illustration%20depicting%20a%20doctor%20in%20modern%20attire%20advising%20a%20diabetes%20patient%20in%20a%20clinic%20setting.%20The%20room%20is%20filled%20with%20various%20contemporary%20medi.webp\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 Time and Uncertainty\n",
    "  \n",
    "- **Static vs. Dynamic Worlds:**  The section begins by contrasting probabilistic reasoning in static worlds, where variables have fixed values, with dynamic environments where variables change over time. It illustrates this difference through examples: diagnosing a car (static) versus treating a diabetic patient (dynamic). \n",
    "- **Dynamic Problem Example - Diabetic Patient Treatment:**  In treating a diabetic patient, the dynamic nature of the problem is highlighted. Variables such as blood sugar levels and insulin levels change rapidly over time, influenced by factors like food intake, insulin doses, metabolic activity, and time of day. This complexity necessitates a model that can account for these changes to assess the current state and predict the outcomes of treatment actions. \n",
    "- **Essentiality of Dynamic Aspects:**  The diabetic patient example underscores the importance of incorporating dynamic aspects into the problem-solving model. Unlike static problems, where the focus is on inferring a fixed state from given evidence, dynamic problems require modeling how states change over time based on a history of evidence and actions taken. \n",
    "- **Generalization to Other Contexts:**  The section generalizes the challenge of modeling dynamic situations to other contexts, such as tracking a robot's location, monitoring the economic activity of a nation, or understanding sequences of spoken or written words. This highlights the widespread applicability and necessity of dynamic modeling across various fields. \n",
    "- **Modeling Dynamic Changes:**  The key challenge in dynamic environments is to accurately model how variables change over time. This involves understanding the relationships between different variables and how actions and external factors influence these relationships and the system's state over time.\n",
    "\n",
    "In summary, this section introduces the complexity of dealing with time and uncertainty in dynamic environments, contrasting it with static world reasoning. It emphasizes the need for models that can account for the ever-changing nature of variables in real-world scenarios, using the treatment of a diabetic patient as a prime example to illustrate these concepts. The discussion sets the stage for exploring how probabilistic reasoning can be extended to effectively model and navigate dynamic situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.1 States and Observations\n",
    "  \n",
    "- **Discrete-Time Models:**  This section introduces the concept of modeling the world in discrete time slices or snapshots, where time is represented as a sequence of intervals (0, 1, 2, ...), without assigning specific times to these intervals. The time interval ∆ between slices is typically constant and chosen based on the application's needs. \n",
    "- **Time Interval Selection:**  The selection of the time interval ∆ is crucial and depends on factors such as sensor capabilities or the rates of change of relevant variables. Examples range from 1/30 of a second for video imagery to a million years for modeling continental drift, illustrating the adaptability of time intervals to different scales of observation. \n",
    "- **Random Variables in Time Slices:**  Each time slice contains a set of random variables, some of which are observable (evidence variables) and some are not (state variables). It simplifies the model to assume a consistent subset of variables is observable at each time slice, denoted as XtX_tXt​ for state variables and EtE_tEt​ for evidence variables at time ttt. \n",
    "- **Observable and Unobservable Variables:**  The distinction between observable evidence variables (EtE_tEt​) and unobservable state variables (XtX_tXt​) is highlighted. An example provided involves a security guard deducing whether it's raining based on the director carrying an umbrella (UtU_tUt​) as observable evidence and the actual rain state (RtR_tRt​) as unobservable. \n",
    "- **Evidence and State Sequences:**  The assumption is that state sequences start at t=0t=0t=0 and evidence starts arriving at t=1t=1t=1, facilitating the modeling of the progression of states and observations over time. The notation a:ba:ba:b is used to denote a sequence of integers and variables from aaa to bbb inclusive, differentiating from programming language notations. \n",
    "- **Modeling Noisy Measurements:**  The example of diabetes monitoring introduces the concept of modeling noisy measurements by distinguishing between the actual state variable (e.g., BloodSugart_tt​) and the measured evidence variable (e.g., MeasuredBloodSugart_tt​). This differentiation is crucial for accurately representing the uncertainty and inaccuracies inherent in real-world observations.\n",
    "\n",
    "In summary, section 14.1.1 lays the groundwork for modeling dynamic systems by introducing discrete-time models, the selection of time intervals, the representation of observable and unobservable variables, and the handling of noisy measurements. This foundation is essential for understanding and predicting changes in dynamic environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.2 Transition and Sensor Models\n",
    "  \n",
    "- **Transition Model:**  Specifies the probability distribution over the state variables at time ttt, given the previous states, formulated as P(Xt∣X0:t−1)P(X_t | X_{0:t-1})P(Xt​∣X0:t−1​). This model accounts for how the world evolves, but to manage the complexity of potentially infinite historical states, a Markov assumption is made. This assumption posits that the current state only depends on a finite number of previous states, simplifying the model to P(Xt∣Xt−1)P(X_t | X_{t-1})P(Xt​∣Xt−1​) for a first-order Markov process, where the future state depends only on the immediate past state. \n",
    "- **Markov Processes:**  Introduced as models satisfying the Markov assumption, these processes are named after statistician Andrei Markov. They include first-order Markov processes, where the future state depends only on the current state, and second-order processes, where the future state depends on the two most recent past states. \n",
    "- **Time-Homogeneous Process:**  Assumes the laws governing changes in the world do not vary over time, allowing the use of a single conditional probability table to represent transitions for all time steps, making the model time-efficient and simpler to manage. \n",
    "- **Sensor Model:**  Describes how evidence variables at time ttt obtain their values, based on the current state variables. A sensor Markov assumption is made, where the current sensor values depend only on the current state, formalized as P(Et∣Xt)P(E_t | X_t)P(Et​∣Xt​). This model focuses on how observations (evidence) are generated from the state of the world. \n",
    "- **Initial State:**  The model also includes a specification of the initial state distribution P(X0)P(X_0)P(X0​), setting the starting point for the system's evolution over time. \n",
    "- **Complete Joint Distribution:**  Combines the initial state model, the transition model, and the sensor model to define the complete dynamics of the system over time, allowing for the representation of the joint probability of all state and evidence variables across all time steps. \n",
    "- **Infinite Variable Handling:**  The challenge of representing an infinite set of variables is addressed by using integer indices and implicit universal quantification, enabling the model to cover every time step without explicitly defining each one. \n",
    "- **Markov Property Restoration:**  Discusses scenarios where the Markov property might be violated due to dependencies on historical states and suggests methods to restore it, such as increasing the order of the Markov process or enlarging the set of state variables to include relevant factors affecting the state transitions.\n",
    "\n",
    "In summary, this section elaborates on the construction of transition and sensor models essential for probabilistic reasoning over time in dynamic systems. It introduces the Markov assumption to simplify the modeling of state transitions and describes how evidence variables are linked to state variables, forming the basis for understanding and predicting changes in dynamic environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 Inference in Temporal Models \n",
    "\n",
    "This section outlines the core inference tasks involved in probabilistic reasoning over time within temporal models. These tasks are fundamental for interpreting and acting upon information in dynamic systems: \n",
    "- **Filtering (State Estimation):**  Involves computing the belief state P(Xt∣e1:t)P(X_t | e_{1:t})P(Xt​∣e1:t​), which is the posterior distribution over the most recent state given all evidence up to the current time. Filtering is crucial for a rational agent to maintain an updated understanding of the current state for making informed decisions. An example provided is computing the probability of rain today, given all prior umbrella observations. \n",
    "- **Prediction:**  Entails computing the posterior distribution over a future state, given all evidence up to the current point (P(Xt+k∣e1:t)P(X_{t+k} | e_{1:t})P(Xt+k​∣e1:t​) for some k>0k > 0k>0). Prediction helps in planning by allowing the assessment of potential future scenarios based on the evidence collected so far. \n",
    "- **Smoothing:**  Focuses on computing the posterior distribution over a past state, considering all evidence up to the present (P(Xk∣e1:t)P(X_k | e_{1:t})P(Xk​∣e1:t​) for 0≤k<t0 \\leq k < t0≤k<t). Smoothing offers a refined estimate of past states by incorporating a broader evidence base than was available at the time, improving historical state estimates. \n",
    "- **Most Likely Explanation (MLE):**  Aims to determine the sequence of states most likely to have resulted in the observed sequence of evidence (argmaxx1:tP(x1:t∣e1:t)\\text{argmax}_{x_{1:t}} P(x_{1:t} | e_{1:t})argmaxx1:t​​P(x1:t​∣e1:t​)). This task is vital in applications like speech recognition, where the goal is to identify the most likely sequence of words from sound inputs, or in reconstructing data transmitted over a noisy channel.\n",
    "\n",
    "Additionally, the section touches on: \n",
    "- **Learning:**  Discusses how the transition and sensor models, if not predefined, can be learned from observation data. Learning can occur as a by-product of inference, where estimates of state transitions and the origins of sensor readings inform the models. This process can be iterative, utilizing algorithms like expectation–maximization (EM) or Bayesian updating based on evidence to refine model parameters.\n",
    "\n",
    "The section sets the stage for presenting generic algorithms that address these inference tasks, applicable across different temporal model types. It also hints at subsequent discussions on model-specific improvements, underscoring the versatility and adaptability of temporal models in handling various dynamic scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.1 Filtering and Prediction**  \n",
    "- **Recursive Estimation for Filtering:**  Filtering is a process that updates the current state estimate based on new evidence without revisiting the entire history of percepts. This update, termed recursive estimation, uses a function fff to compute P(Xt+1∣e1:t+1)P(X_{t+1}|e_{1:t+1})P(Xt+1​∣e1:t+1​) from P(Xt∣e1:t)P(X_t|e_{1:t})P(Xt​∣e1:t​) and the new evidence et+1e_{t+1}et+1​. This process involves two steps: projecting the current state distribution forward and then updating it with new evidence. \n",
    "- **Two-Part Process in Filtering:**  The filtering calculation can be broken down into a prediction part (projecting the current state distribution forward) and an update part (adjusting this projection based on new evidence et+1e_{t+1}et+1​). The sensor model P(et+1∣Xt+1)P(e_{t+1}|X_{t+1})P(et+1​∣Xt+1​) is used for the update, and the prediction uses the transition model P(Xt+1∣Xt)P(X_{t+1}|X_t)P(Xt+1​∣Xt​) along with recursive application of previous state estimates. \n",
    "- **Recursive Filtering Implementation:**  The filtered estimate P(Xt∣e1:t)P(X_t|e_{1:t})P(Xt​∣e1:t​) acts as a \"message\" that is propagated forward, modified by each transition, and updated by each observation. This recursive formulation allows for constant time and space updates, essential for continuous tracking by finite agents. \n",
    "- **Example - Umbrella Scenario:**  The filtering process is demonstrated through the umbrella example, where the probability of rain (P(R2∣u1:2)P(R_2|u_{1:2})P(R2​∣u1:2​)) is computed across two days based on the appearance of an umbrella. This process shows how the probability of rain is updated as new evidence (umbrella sightings) is incorporated. \n",
    "- **Prediction Without New Evidence:**  Prediction is essentially filtering without the addition of new evidence, incorporating a one-step prediction within the filtering process itself. The state at t+k+1t+k+1t+k+1 can be predicted from a state at t+kt+kt+k using only the transition model, illustrating that prediction involves forward-projecting the state distribution without adjusting for new evidence. \n",
    "- **Convergence in Prediction:**  Over time, the predicted distribution may converge to a fixed point, representing the stationary distribution of the Markov process. This highlights the limitations in long-term predictions due to increasing uncertainty and the properties of the transition model. \n",
    "- **Computing Evidence Sequence Likelihood:**  The likelihood of the evidence sequence P(e1:t)P(e_{1:t})P(e1:t​) can also be computed using a forward recursion, useful for comparing different temporal models. This involves calculating a likelihood message and summing out the state variables to get the total likelihood of the evidence sequence. \n",
    "- **Numerical Challenges:**  The likelihood message tends to become numerically smaller over time, leading to potential underflow problems in practical implementations. Solutions to these numerical challenges are essential but not discussed in detail in this section.\n",
    "\n",
    "This summary captures the essence of filtering and prediction in temporal models, highlighting the mechanisms for updating state estimates based on new evidence and predicting future states, with practical examples and the consideration of computational challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.2 Smoothing**  \n",
    "- **Definition and Purpose:**  Smoothing refers to the process of determining the distribution over past states given all evidence up to the present, expressed as P(Xk∣e1:t)P(X_k | e_{1:t})P(Xk​∣e1:t​) for 0≤k<t0 \\leq k < t0≤k<t. This process provides a more accurate estimation of past states by utilizing all available evidence. \n",
    "- **Recursive Message-Passing Approach:**  The smoothing computation is divided into two parts: evidence up to time kkk and evidence from time k+1k+1k+1 to ttt. This division facilitates a recursive approach that combines a \"forward\" message (f1:kf_{1:k}f1:k​) from filtering up to time kkk and a \"backward\" message (bk+1:tb_{k+1:t}bk+1:t​) that incorporates evidence from time k+1k+1k+1 to ttt. \n",
    "- **Backward Message Calculation:**  The backward message, bk+1:t=P(ek+1:t∣Xk)b_{k+1:t} = P(e_{k+1:t} | X_k)bk+1:t​=P(ek+1:t​∣Xk​), is computed through a recursive process that runs backward from ttt to kkk. This message captures the influence of future evidence on the estimation of past states. \n",
    "- **Combining Forward and Backward Messages:**  The smoothed estimate for a state at time kkk is obtained by combining the forward message (resulting from filtering) with the backward message through pointwise multiplication. This approach integrates information from both past and future evidence relative to time kkk. \n",
    "- **Initialization for Backward Phase:**  The backward phase is initialized with bt+1:t=1b_{t+1:t} = 1bt+1:t​=1, representing the certainty of observing an empty sequence of evidence from time t+1t+1t+1 to ttt, effectively setting the stage for backward recursion. \n",
    "- **Umbrella Example Application:**  In an example involving umbrella observations, the smoothing process is applied to calculate the probability of rain on a specific day, given umbrella observations on consecutive days. This demonstrates how the smoothed estimate adjusts based on the accumulation of evidence. \n",
    "- **Time Complexity:**  The time complexity for smoothing at a particular time step kkk is O(t)O(t)O(t), making it efficient for calculating smoothed estimates over time. For smoothing the entire sequence, a straightforward approach would result in O(t2)O(t^2)O(t2) complexity, but dynamic programming techniques can reduce this to O(t)O(t)O(t), maintaining efficiency. \n",
    "- **Forward-Backward Algorithm:**  A linear-time algorithm called the forward–backward algorithm efficiently computes smoothed estimates for the entire sequence by combining results from forward filtering with a backward recursion, demonstrating its utility as a computational backbone for handling sequences of noisy observations. \n",
    "- **Practical Considerations:**  The forward–backward algorithm faces challenges related to space complexity, especially with large state spaces and long sequences. Additionally, modifications are required for online settings where new observations continuously update the need for smoothed estimates for earlier time slices, such as in fixed-lag smoothing scenarios.\n",
    "\n",
    "Smoothing effectively reconciles information from both past and future evidence to refine the estimation of past states in dynamic systems. Its computational framework, particularly the forward–backward algorithm, is crucial for a wide range of applications dealing with temporal data and noisy observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward-Backward Algorithm\n",
    "\n",
    "pseudo-code representation of the forward-backward algorithm based on the provided description. This algorithm computes posterior probabilities of states given a sequence of observations by utilizing forward and backward message passing:\n",
    "\n",
    "```pseudo\n",
    "function FORWARD-BACKWARD(ev, prior) returns a vector of probability distributions\n",
    "    inputs: ev, a vector of evidence values for steps 1,...,t\n",
    "            prior, the prior distribution on the initial state, P(X_0)\n",
    "    local variables: fv, a vector of forward messages for steps 0,...,t\n",
    "                     b, a representation of the backward message, initially all 1s\n",
    "                     sv, a vector of smoothed estimates for steps 1,...,t\n",
    "\n",
    "    // Initialize forward messages with the prior distribution\n",
    "    fv[0] ← prior\n",
    "\n",
    "    // Forward pass: Compute forward messages based on evidence\n",
    "    for i = 1 to t do\n",
    "        fv[i] ← FORWARD(fv[i-1], ev[i])\n",
    "\n",
    "    // Initialize backward message to all 1s\n",
    "    b ← all 1s vector\n",
    "\n",
    "    // Backward pass: Compute smoothed estimates and update backward messages\n",
    "    for i = t down to 1 do\n",
    "        sv[i] ← NORMALIZE(fv[i] × b)\n",
    "        b ← BACKWARD(b, ev[i])\n",
    "\n",
    "    return sv\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "In this pseudo-code: \n",
    "- `FORWARD` and `BACKWARD` are functions that implement the forward and backward update steps defined by Equations (14.5) and (14.9), respectively. \n",
    "- `NORMALIZE` is a function that normalizes a vector so that its elements sum up to 1, converting the raw scores into probabilities. \n",
    "- `fv` stores the forward messages, which represent the belief state about the system's state at each time step before seeing the evidence at that step. \n",
    "- `b` represents the backward message used to integrate information from future observations. \n",
    "- `sv` stores the smoothed estimates, which are the posterior probabilities of the states at each time step, computed by combining information from both forward messages and backward messages. \n",
    "- The loop from 1 to ttt iterates through the evidence vector, updating the forward messages based on the current evidence and prior messages. \n",
    "- The backward pass iterates from ttt down to 1, updating the smoothed estimates and backward messages at each step.\n",
    "- The algorithm returns the vector of smoothed estimates, providing a posterior distribution of states at each time step, given the entire sequence of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward-Backward Algorithm - Example\n",
    "\n",
    "For a more concrete example, let's consider a simplified scenario involving a binary state space where the system can be in one of two states (`state_0` or `state_1`), and the evidence can also be binary (`evidence_true` or `evidence_false`). We will make the following assumptions: \n",
    "1. **Transition Model:**  The probability of transitioning from any state to itself is 0.7, and to the other state is 0.3. \n",
    "2. **Sensor Model:**  The probability of observing the correct evidence given the state is 0.8 (e.g., if it's `state_0`, there's an 80% chance of `evidence_true`), and the probability of incorrect evidence is 0.2. \n",
    "3. **Initial Prior:**  The system has an equal chance of starting in either state (`state_0` or `state_1`).\n",
    "\n",
    "Based on these assumptions, we can implement the `FORWARD`, `BACKWARD`, and `NORMALIZE` functions in Python.\n",
    "\n",
    "```python\n",
    "def normalize(prob_dist):\n",
    "    \"\"\"Normalize a probability distribution.\"\"\"\n",
    "    total = sum(prob_dist)\n",
    "    return [p / total for p in prob_dist]\n",
    "\n",
    "def forward(prev_fv, ev_i):\n",
    "    \"\"\"Forward message computation based on transition and sensor models.\"\"\"\n",
    "    # Transition probabilities\n",
    "    transition_matrix = [[0.7, 0.3], [0.3, 0.7]]\n",
    "    # Sensor model probabilities\n",
    "    sensor_model = {\n",
    "        'evidence_true': [0.8, 0.2],  # Probability of observing true/false given the actual state\n",
    "        'evidence_false': [0.2, 0.8]\n",
    "    }\n",
    "    \n",
    "    # Apply transition model\n",
    "    predicted = [sum(prev_fv[j] * transition_matrix[j][i] for j in range(2)) for i in range(2)]\n",
    "    \n",
    "    # Apply sensor model\n",
    "    updated = [predicted[i] * sensor_model[ev_i][i] for i in range(2)]\n",
    "    \n",
    "    return normalize(updated)\n",
    "\n",
    "def backward(b, ev_i):\n",
    "    \"\"\"Backward message computation based on transition and sensor models.\"\"\"\n",
    "    # Transition probabilities (same as in forward because of time symmetry)\n",
    "    transition_matrix = [[0.7, 0.3], [0.3, 0.7]]\n",
    "    # Sensor model probabilities (assuming backward step has same model as forward)\n",
    "    sensor_model = {\n",
    "        'evidence_true': [0.8, 0.2],\n",
    "        'evidence_false': [0.2, 0.8]\n",
    "    }\n",
    "    \n",
    "    # Apply sensor model first (reverse of forward step)\n",
    "    sensor_applied = [b[i] * sensor_model[ev_i][i] for i in range(2)]\n",
    "    \n",
    "    # Apply transition model (reverse of forward, summing over future states)\n",
    "    backward_updated = [sum(sensor_applied[j] * transition_matrix[i][j] for j in range(2)) for i in range(2)]\n",
    "    \n",
    "    return normalize(backward_updated)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "To use these functions in the `forward_backward` function provided earlier, simply replace the placeholder calls to `FORWARD`, `BACKWARD`, and `NORMALIZE` with these implementations. This example assumes the evidence provided in the `ev` vector is either `'evidence_true'` or `'evidence_false'`, and it operates on a binary state space. The `normalize` function ensures that the probability distributions produced by the `FORWARD` and `BACKWARD` functions sum to 1.\n",
    "\n",
    "This setup is a basic illustration. For a real-world application, you would need to adapt these functions based on the specifics of your transition and sensor models, as well as the structure of your state space and evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.3 Finding the Most Likely Sequence** \n",
    "\n",
    "The task of finding the most likely sequence of states given a sequence of observations (e.g., whether it was raining on each day based on umbrella observations) can be challenging, especially when considering all possible state sequences directly. Here's how this problem is approached and solved efficiently: \n",
    "- **Linear-Time Procedure vs. Joint Probabilities:**  A naive approach might involve using smoothing to find the most likely state at each time step independently. However, this method doesn't account for joint probabilities over the entire sequence, which are crucial for identifying the most likely overall sequence. \n",
    "- **Markov Property and Graph Representation:**  The efficient solution leverages the Markov property, viewing the sequence of states as paths through a graph with nodes representing possible states at each time step. The goal is to find the most likely path through this graph, considering both transition probabilities between states and the probabilities of observations at each state. \n",
    "- **Recursive Relationship for Paths:**  The most likely path to a state at time t+1t+1t+1 depends on the most likely path to a state at time ttt and the transition to t+1t+1t+1. This recursive relationship allows for a systematic approach to identify the most likely path through the state graph. \n",
    "- **Viterbi Algorithm:**  This linear-time algorithm computes the most likely sequence of states given the evidence. It uses a recursively computed message, similar to the forward message in filtering, but instead of summing probabilities, it maximizes them at each step to focus on the most likely paths. \n",
    "- The algorithm initializes with the prior distribution P(X0)P(X_0)P(X0​) and iteratively updates a message vector m1:tm_{1:t}m1:t​ to keep track of the most likely paths. \n",
    "- For each state, the algorithm also records the predecessor state that leads to the maximum probability path, allowing for the reconstruction of the most likely sequence by tracing these pointers backward from the final state. \n",
    "- **Handling Numerical Underflow:**  Numerical underflow is a concern due to the multiplication of probabilities over long sequences. Solutions include normalizing probabilities at each step or using log probabilities and addition to avoid underflow while maintaining the monotonic properties necessary for maximizing probabilities. \n",
    "- **Time and Space Complexity:**  The Viterbi algorithm has a time complexity linear in the length of the sequence ttt and a space requirement linear in ttt to store pointers for reconstructing the most likely sequence.\n",
    "\n",
    "In summary, the Viterbi algorithm provides an efficient, linear-time method to compute the most likely sequence of states based on a series of observations. By leveraging the Markov property and a recursive approach to maximize the likelihood of paths through a state graph, the algorithm avoids the infeasibility of direct enumeration while accurately accounting for the joint probabilities of state sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm in Python\n",
    "\n",
    "The Viterbi algorithm is a dynamic programming algorithm used for finding the most likely sequence of hidden states—called the Viterbi path—that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMMs).\n",
    "\n",
    "Below is an example implementation of the Viterbi algorithm in Python. This example assumes a simple scenario with two hidden states and binary observations, similar to the umbrella example discussed earlier: \n",
    "- Hidden states: `Rainy` and `Sunny` \n",
    "- Observations: `Umbrella` and `No Umbrella`\n",
    "- Transition probabilities, emission probabilities, and initial state probabilities are hardcoded for simplicity.\n",
    "\n",
    "```python\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][obs[0]], \"prev\": None}\n",
    "    # Run Viterbi for t > 0\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = max(V[t-1][prev_st][\"prob\"]*trans_p[prev_st][st] for prev_st in states)\n",
    "            for prev_st in states:\n",
    "                if V[t-1][prev_st][\"prob\"] * trans_p[prev_st][st] == max_tr_prob:\n",
    "                    max_prob = max_tr_prob * emit_p[st][obs[t]]\n",
    "                    V[t][st] = {\"prob\": max_prob, \"prev\": prev_st}\n",
    "                    break\n",
    "    for line in V:\n",
    "        print(line)\n",
    "    opt = []\n",
    "    # The highest probability\n",
    "    max_prob = max(value[\"prob\"] for value in V[-1].values())\n",
    "    previous = None\n",
    "    # Get most probable state and its backtrack\n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] == max_prob:\n",
    "            opt.append(st)\n",
    "            previous = st\n",
    "            break\n",
    "    # Follow the backtrack till the first observation\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    "\n",
    "    print('The steps of states are ' + ' '.join(opt) + ' with highest probability of %s' % max_prob)\n",
    "\n",
    "# Example use case\n",
    "states = ('Rainy', 'Sunny')\n",
    "observations = ('Umbrella', 'Umbrella', 'No Umbrella', 'Umbrella', 'Umbrella')\n",
    "start_probability = {'Rainy': 0.6, 'Sunny': 0.4}\n",
    "transition_probability = {\n",
    "   'Rainy': {'Rainy': 0.7, 'Sunny': 0.3},\n",
    "   'Sunny': {'Rainy': 0.4, 'Sunny': 0.6},\n",
    "}\n",
    "emission_probability = {\n",
    "   'Rainy': {'Umbrella': 0.9, 'No Umbrella': 0.1},\n",
    "   'Sunny': {'Umbrella': 0.2, 'No Umbrella': 0.8},\n",
    "}\n",
    "\n",
    "viterbi(observations, states, start_probability, transition_probability, emission_probability)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "This Python example defines the Viterbi algorithm function and applies it to a simple scenario involving weather states and umbrella observations. The algorithm calculates the most likely sequence of weather states based on the observations, utilizing hardcoded probabilities for state transitions and observations.\n",
    "\n",
    "Remember, this is a simplified example. In real applications, especially in domains like speech recognition, bioinformatics, or natural language processing, the states, observations, and probabilities would be much more complex and typically derived from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 Hidden Markov Models (HMMs) \n",
    "\n",
    "Hidden Markov Models (HMMs) represent a specific category of temporal probabilistic models that play a crucial role in applications requiring the analysis of sequential data. This summary highlights the key characteristics and advantages of HMMs: \n",
    "- **Definition and Structure:**  An HMM is defined by a discrete random variable that captures the state of a process over time. The state evolves according to a Markov process, meaning the future state depends only on the current state, not the history of states. This simplicity allows HMMs to model complex temporal behaviors through a sequence of observable events generated by these hidden states. \n",
    "- **Single State Variable:**  The core of an HMM is a single state variable that represents different states of the world at each point in time. For example, in the umbrella scenario, the state variable `Rain_t` indicates whether it is raining at time `t`. The simplicity of having a single state variable is a defining feature of HMMs, making them distinct and manageable for various applications. \n",
    "- **Combining Multiple State Variables:**  For more complex models involving multiple state variables, HMMs accommodate these by combining them into a single \"megavariable.\" This megavariable encompasses all possible combinations of the individual state variables, effectively transforming a multi-variable model into a single-variable HMM. This approach maintains the Markov property and ensures the model's compatibility with the HMM framework. \n",
    "- **Evidence Variables:**  While HMMs restrict the state to be a single discrete variable, there are no such constraints on evidence variables. Evidence variables can be both discrete and continuous and are not limited in number. This flexibility allows HMMs to model a wide array of real-world phenomena where observations can come in various forms and from multiple sources. \n",
    "- **Matrix Implementation:**  The restricted structure of HMMs, with their single state variable and Markovian dynamics, lends itself to efficient implementations using matrices. This matrix formulation enables elegant and computationally efficient solutions for the basic algorithms, such as filtering, smoothing, and finding the most likely sequence of states. Matrix operations can leverage optimized linear algebra libraries, further enhancing performance. \n",
    "- **Applications:**  HMMs are widely used in fields such as speech recognition, where the sequence of spoken words (observable events) is generated by an underlying sequence of phonemes (hidden states); in bioinformatics for gene prediction and modeling protein sequences; and in natural language processing for tasks like part-of-speech tagging.\n",
    "\n",
    "In summary, HMMs are a powerful tool for modeling and analyzing sequences where the underlying state of the system is not directly observable. By efficiently capturing the dynamics of hidden states through observable events, HMMs facilitate a broad range of applications in temporal probabilistic reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
