{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16 - Making Simple Decisions\n",
    "\n",
    "*In which we see how an agent should make decisions so that it gets what it wants in an\n",
    "uncertain world—at least as much as possible and on average.* - Peter Norvig and Stuart Russell in Artificial Intelligence: A Modern Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "Chapter 16, \"Making Simple Decisions,\" in \"Artificial Intelligence: A Modern Approach\" by Stuart Russell and Peter Norvig, is a comprehensive examination of how agents can make rational decisions under conditions of uncertainty. This chapter integrates utility theory with probability theory, providing a foundational framework for decision-making processes in artificial intelligence. It starts with the premise that decisions involve choosing among actions with uncertain outcomes, emphasizing the importance of considering both the agent's beliefs (probabilities) and desires (utilities) to make rational choices.\n",
    "\n",
    "The chapter is organized into several key sections, beginning with an exploration of how beliefs and desires under uncertainty can be combined through expected utility theory, progressing through the intricacies of utility functions, multiattribute utility functions, and decision networks. It also discusses the value of information and how it can influence decision-making, and it concludes with an examination of how to handle unknown preferences.\n",
    "\n",
    "By the end of the chapter, readers are expected to understand the principles guiding rational decision-making in uncertain environments, how to construct and use decision networks, and the importance of information in shaping decisions. \n",
    "\n",
    "- The subchapters for Chapter 16 - MAKING SIMPLE DECISIONS in \"Artificial Intelligence: A Modern Approach\" by Stuart Russell and Peter Norvig are as follows:\n",
    "- 16.1 Combining Beliefs and Desires under Uncertainty\n",
    "- 16.2 The Basis of Utility Theory\n",
    "- 16.3 Utility Functions\n",
    "- 16.4 Multiattribute Utility Functions\n",
    "- 16.5 Decision Networks\n",
    "- 16.6 The Value of Information\n",
    "- 16.7 Unknown Preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1 \"Combining Beliefs and Desires under Uncertainty\"\n",
    "\n",
    "- **Introduction to Decision Theory** : It starts by introducing the concept of decision theory, which aims to combine utility theory with probability theory to guide how an agent can make rational decisions based on its beliefs and desires. \n",
    "- **Action and Outcome Uncertainty** : It acknowledges the uncertainties in both the agent's current state and the outcomes of its actions. The transition model, represented by the probability P(s′∣sa)P(s'|sa)P(s′∣sa), indicates the likelihood of reaching a new state s′s's′ after taking action aaa in state sss. \n",
    "- **Expected Utility Theory** : Central to decision theory is the principle of maximizing expected utility. The expected utility of an action, given the agent's evidence, is the sum of the utilities of possible outcomes weighted by their probabilities. \n",
    "- **Maximizing Expected Utility (MEU)** : The MEU principle dictates that a rational agent should choose the action that maximizes its expected utility. This principle is presented as a fundamental guide for intelligent behavior, highlighting the importance of calculating and maximizing utility over actions. \n",
    "- **Operational Challenges** : While the MEU principle provides a clear directive for rational action, operationalizing this principle in AI involves complex challenges such as estimating probabilities over possible world states, requiring capabilities in perception, learning, knowledge representation, and inference. \n",
    "- **Relation to Performance Measures** : The MEU principle is related to the idea of performance measures introduced in earlier chapters. It justifies the use of utility functions to guide step-by-step actions towards achieving the highest possible performance score across possible environments.\n",
    "\n",
    "These points illustrate the foundational concepts of decision theory as applied to AI, emphasizing the integration of beliefs (probabilities) and desires (utilities) to make rational decisions under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2 \"The Basis of Utility Theory\" \n",
    "\n",
    "Introducing  fundamental concepts underlying utility theory, which is essential for understanding rational decision-making. \n",
    "\n",
    "- **Rationality and Utility Maximization** : The chapter begins by questioning why maximizing expected utility (MEU) is considered the only rational way to make decisions, suggesting that while MEU seems intuitive, other methods could also appear rational. \n",
    "- **Constraints on Rational Preferences** : It introduces constraints that any rational preference ordering must satisfy, including orderability (the ability to order preferences), transitivity (if A is preferred to B and B is preferred to C, then A should be preferred to C), continuity (preferences between lotteries should be consistent), substitutability (indifferent preferences should be substitutable in complex lotteries without altering the preference order), and monotonicity (a preference for lotteries with a higher probability of a preferred outcome). \n",
    "- **Rational Preferences Lead to a Utility Function** : It argues that if an agent's preferences satisfy the axioms of utility theory, then a utility function exists that can represent these preferences. This utility function allows preferences to be expressed numerically, facilitating rational decision-making. \n",
    "- **Expected Utility of a Lottery** : It describes how the utility of a lottery (a probabilistic set of outcomes) can be calculated as the sum of the utilities of its outcomes, weighted by their probabilities. This concept is foundational for understanding how decisions under uncertainty can be evaluated in terms of expected utility. \n",
    "- **Utility Functions and Rational Decision-Making** : The chapter emphasizes that having a utility function enables an agent to make decisions that are consistent with its preferences, even in the face of uncertainty. This is because decisions can be evaluated based on the expected utility they generate, aligning with the principle of maximizing expected utility.\n",
    "\n",
    "These points illustrate the theoretical underpinnings of utility theory as it applies to rational decision-making, setting the stage for its application in various contexts, including AI and economics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.3 \"Utility Functions\" \n",
    "\n",
    "We dwelve into the concept of utility functions and their properties within the context of decision theory. \n",
    "\n",
    "- **Definition and Flexibility** : Utility functions translate lotteries or outcomes into real numbers, adhering to the axioms of orderability, transitivity, continuity, substitutability, monotonicity, and decomposability. This section underscores the flexibility in an agent's preferences, illustrating that practically any set of preferences, as long as it aligns with these axioms, can be rational. For instance, an agent's preference for having a prime number of dollars in its bank account, while peculiar, is not deemed irrational within the framework of utility theory. \n",
    "- **Preference Diversity** : The chapter highlights that while agents can have any preferences, including unusual ones like preferring a dented 1973 Ford Pinto over a brand-new Mercedes under certain conditions, real-world agents tend to have more systematic preferences that are easier to model and predict. \n",
    "- **Utility and Real-world Analogies** : It draws an analogy between utilities and temperature measurements, noting that converting temperatures between Fahrenheit and Celsius doesn't change the underlying physical temperature, similar to how different utility scales or transformations don't alter the preference orderings they represent​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.4 \"Multiattribute Utility Functions\"\n",
    "\n",
    "Here we explores the extension of utility theory to scenarios involving multiple attributes, which is crucial in fields like public policy where decisions impact both financial and human aspects.\n",
    "\n",
    "- **Introduction to Multiattribute Utility Theory** : This section introduces the concept of multiattribute utility theory, which is essential for making decisions where outcomes are characterized by multiple attributes. It emphasizes that this theory enables the comparison of options that differ across several dimensions, likening it to comparing apples to oranges. \n",
    "- **Attributes and Their Representation** : It discusses how to represent multiple attributes mathematically, focusing on ensuring that higher attribute values always correspond to higher utilities. This might involve transforming attributes so they align with the utility function's requirements. \n",
    "- **Example Attributes for an Airport Site Selection** : Provides concrete examples of attributes in the context of choosing a site for a new airport, such as throughput (number of flights), safety (measured by expected number of deaths), quietness (based on the number of people living under flight paths), and frugality (cost of construction), illustrating how complex real-world decisions can be broken down into quantifiable attributes. \n",
    "- **Dominance in Decision Making** : Introduces the concept of dominance, particularly strict dominance, where if one option is better than another across all attributes, the inferior option can be disregarded. It further explains how dominance helps narrow down choices but often does not lead to a single best option. \n",
    "- **Stochastic Dominance** : Expands the concept of dominance to include uncertain outcomes, showing how one option can be preferable to another under uncertainty by comparing their distributions across attributes. \n",
    "- **Preference Structure and Utility** : Discusses how preferences across multiple attributes can be structured and simplified, aiming to reduce the complexity of the utility function. It introduces concepts like preference independence and mutual preferential independence (MPI), which allow for the simplification of the utility function into an additive or multiplicative form based on the attributes. \n",
    "- **Additive and Multiplicative Utility Functions** : Details how preferences that exhibit certain structures, such as MPI, can be represented by simpler utility functions, either additive or multiplicative, reducing the need to assess complex multi-dimensional utility functions directly.\n",
    "\n",
    "These key ideas demonstrate the complexity of decision-making in scenarios with multiple attributes and outline how utility theory provides a framework for systematically comparing and evaluating different options based on their respective utilities across all relevant dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5 \"Decision Networks\" \n",
    "\n",
    "Our focus is now on the formalism of decision networks (also known as influence diagrams) as a means for making rational decisions in uncertain environments. Here are the key ideas: \n",
    "- **Introduction to Decision Networks** : The subchapter starts by introducing decision networks as a general mechanism for making rational decisions, combining Bayesian networks with additional nodes for actions and utilities, using the example of selecting a site for a new airport​[]() ​. \n",
    "- **Components of Decision Networks** : It breaks down the structure of decision networks into three types of nodes: \n",
    "- **Chance nodes**  represent random variables and uncertainties in the environment, such as construction costs or air traffic levels, which depend on the chosen site. \n",
    "- **Decision nodes**  symbolize the points where the decision maker has choices of actions, influencing the outcomes in terms of safety, quietness, and frugality. \n",
    "- **Utility nodes**  encapsulate the agent’s utility function, indicating the utility of outcomes as a function of their attributes. This includes both deterministic utilities and the expected utility associated with each action, simplifying decision-making under uncertainty​​. \n",
    "- **Evaluating Decision Networks** : Describes the process for selecting actions by evaluating the decision network for each possible decision, setting decision nodes similarly to evidence variables in Bayesian networks. This involves setting current state evidence, calculating posterior probabilities for outcomes, and then computing the utility for each action to select the one with the highest utility​​.\n",
    "\n",
    "These concepts illustrate how decision networks provide a structured and formal approach to decision-making under uncertainty, integrating probabilistic reasoning with utility theory to guide the selection of rational actions based on expected outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.6 \"The Value of Information\" \n",
    "\n",
    "Now we discuss the importance and valuation of information in decision-making processes. \n",
    "\n",
    "- **Importance of Acquiring Information** : Initially, it discusses the critical role of knowing which questions to ask or what information to gather before making decisions. This aspect is especially crucial in scenarios where not all relevant or available information is initially provided to the decision-maker, such as in medical diagnosis, where tests can be expensive or hazardous​[]() ​. \n",
    "- **Information Value Theory** : This section introduces information value theory, which helps an agent decide what information is worth acquiring. It emphasizes a simplified form of sequential decision-making, where observation actions only affect the agent's belief state and not the physical state, highlighting that the value of any observation derives from its potential to influence the agent's physical actions​[]() ​. \n",
    "- **Examples and General Formula for Perfect Information** : Through examples, such as an oil company considering buying drilling rights and the potential value of seismic survey information, it illustrates how to calculate the expected profit or loss from acquiring specific information. It also presents a general formula for the value of perfect information (VPI), which quantifies the difference in expected value between the best actions before and after obtaining information​[]() ​. \n",
    "- **Non-Negative Expected Value of Information** : The chapter asserts that the expected value of information is always non-negative, meaning that acquiring more information cannot harm decision-making. In worst-case scenarios, the decision-maker can choose to ignore the new information, underscoring that information either has positive value or no impact at all on the decision process​[]() ​. \n",
    "- **Myopic and Nonmyopic Information Gathering** : It distinguishes between myopic (shortsighted) information gathering, where the value of information is calculated assuming only a single piece of evidence will be acquired, and nonmyopic approaches that consider the possibility of obtaining multiple observations. Myopic information gathering is likened to greedy search, often effective but sometimes suboptimal compared to strategies that consider multiple evidence pieces​[]() ​.\n",
    "\n",
    "These concepts explain how decision-making can be significantly improved by strategically acquiring and utilizing information, emphasizing the calculation and implications of the value of information in uncertain environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.7 \"Unknown Preferences\"\n",
    "\n",
    "Now we discuss the complexities and methodologies for dealing with uncertainty in utility functions, whether it's an agent uncertain about its own preferences or a machine uncertain about human preferences. \n",
    "\n",
    "- **Uncertainty in Self-Preferences** : It illustrates situations where an agent (human or machine) is unsure about its own preferences. For example, a person trying to decide between two flavors of ice cream, vanilla and durian, with known preference for vanilla but uncertain about durian, due to its polarizing nature. \n",
    "- **Modeling Uncertainty with Decision Networks** : The text describes how to model this uncertainty using decision networks, incorporating uncertain preferences as random variables. This allows for a systematic approach to decision-making even when the agent's preferences aren't fully known. \n",
    "- **Decision Making with Uncertain Preferences** : It discusses how agents can make decisions when their preferences are uncertain, showing that it is possible to calculate expected utilities even when preferences over outcomes are not clear-cut. \n",
    "- **Deference to Humans** : The second part of the subchapter addresses situations where a machine aims to assist a human but is uncertain about the human's preferences. It explores scenarios where the machine might choose to defer to the human's judgment, effectively allowing the human to make the final decision. This deference is framed within the context of an \"off-switch\" game, illustrating how a machine might choose to switch itself off or defer to human decision-making to optimize outcomes based on uncertain preferences. \n",
    "- **Robbie and Harriet Case Study** : Provides a detailed example involving a robot assistant (Robbie) and its human owner (Harriet), showcasing how Robbie uses deference as a strategy to manage uncertainty about Harriet's preferences regarding hotel bookings. This example underscores the value of allowing for human input in decision-making processes when machines are uncertain about human desires or outcomes.\n",
    "\n",
    "These concepts highlight the importance of handling preference uncertainty, both for individual decision-making and in designing machines or systems that interact with humans. The discussions aim to provide frameworks for making rational decisions in the face of incomplete information about preferences, emphasizing the role of deference and information gathering in navigating these uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliographical and Historical Notes \n",
    "\n",
    "- **Arnauld (1662)** : In \"L’art de Penser\" or \"Port-Royal Logic,\" Arnauld discusses the necessity of considering both the probability of outcomes and their inherent good or evil to make judgments on actions. \n",
    "- **Daniel Bernoulli (1738)** : Investigated the St. Petersburg paradox, introducing the concept of utility to explain preferences for lotteries. \n",
    "- **Jeremy Bentham (1823)** : Proposed the hedonic calculus for weighing pleasures and pains, suggesting all decisions could be reduced to utility comparisons. \n",
    "- **Ramsey (1931)** : First carried out the derivation of numerical utilities from preferences, a foundational work for utility theory. \n",
    "- **Von Neumann and Morgenstern (1944)** : \"Theory of Games and Economic Behavior\" introduced axioms for preference that influenced the modern understanding of utility theory. \n",
    "- **Savage (1954)**  and **Jeffrey (1983)** : Contributed to constructing subjective probabilities and utilities from an agent's preferences. \n",
    "- **Howard and Matheson (1984)** : Introduced influence diagrams or decision networks, an essential tool for decision theory, based on earlier work at SRI (Miller et al., 1976). \n",
    "- **Shachter (1986)** : Developed a method for making decisions based directly on a decision network without creating an intermediate decision tree. \n",
    "- **Von Winterfeldt and Edwards (1986)** , **Smith (1988)** , **Fenton and Neil (2018)** : Provided significant insights into decision analysis, utility modeling, and solving real-world problems using decision networks. \n",
    "- **Jerry Feldman (1974, 1977)** : Applied decision theory to problems in vision and planning, highlighting early adoption of decision-theoretic tools in AI. \n",
    "- **Horvitz et al. (1988)** , **Cowell et al. (2002)** : Contributed to the acceptance and development of decision-theoretic expert systems. \n",
    "- **Harsanyi (1967)** : Explored the problem of incomplete information in game theory, showing that games with incomplete information are equivalent to games with imperfect information. \n",
    "- **Cyert and de Groot (1979)** : Developed a theory of adaptive utility, where an agent can be uncertain about its own utility function. \n",
    "- **Chajewska et al. (2000)** , **Boutilier (2002)** , **Fern et al. (2014)** : Worked on Bayesian preference elicitation and models of assistance, proposing frameworks for understanding and assisting with human goals under uncertainty. \n",
    "- **Hadfield-Menell et al. (2017b)** , **Russell (2019)** : Proposed models and frameworks for beneficial AI, including the off-switch game as a key example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
