{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 26: Robotics**\n",
    "\n",
    "*In which agents are endowed with sensors and physical effectors with which to move about\n",
    "and make mischief in the real world.* - Stuart Russell and Peter Norvig in Artificial Intelligence: A Modern Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **26.1 Robots** \n",
    "- Robots are physical agents that interact with the world using effectors like legs, wheels, and grippers to exert forces and manipulate their environment.\n",
    "- These actions can change the robot’s state, the environment's state, and even the state of people nearby.\n",
    "- Equipped with various sensors such as cameras, radars, and gyroscopes, robots can perceive their environment and their own state to make informed decisions.\n",
    "- The goal for robots is to maximize expected utility, selecting actions that yield the highest expected reward while accomplishing tasks in the physical world.\n",
    "- Robots operate in complex, partially observable, and stochastic environments where uncertainties like obscured views and unpredictable human behavior exist.\n",
    "- They model their environments with continuous state and action spaces, often involving high-dimensional spaces for more complex robots like autonomous vehicles or humanoid figures.\n",
    "- Robotic learning faces challenges due to the slow pace of real-world data acquisition compared to simulations, leading to issues in transferring simulated learning to real-world applications.\n",
    "- Robotics integrates numerous AI concepts such as probabilistic state estimation, planning, and reinforcement learning, providing practical applications and introducing new methodologies for continuous systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **26.2 Robot Hardware** \n",
    "- The success of real robots heavily depends on the design of their hardware, specifically sensors and effectors, which must be tailored to suit the specific tasks they are designed to perform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **26.2.1 Types of Robots from the Hardware Perspective**  \n",
    "- **Anthropomorphic Robots:**  These humanoid robots, often featured in media like movies and cartoons, resemble humans with heads, arms, and legs or wheels. \n",
    "- **Manipulators:**  Essentially robot arms, these do not need to be part of a larger robot body and can be mounted on stable surfaces like tables or floors. Used in various settings from heavy industrial applications (e.g., assembling cars) to assistive technologies for individuals with motor impairments. \n",
    "- **Mobile Robots:**  These robots move using wheels, legs, or rotors. Types include quadcopter drones (UAVs), autonomous underwater vehicles (AUVs), and ground-based robots like vacuum cleaners or autonomous cars. Mobile robots are versatile, operating in indoor environments or exploring harsh terrains like Mars. \n",
    "- **Legged Robots:**  Specifically designed to navigate rough terrain, these robots face more complex control challenges compared to their wheeled counterparts. \n",
    "- **Specialized Robots:**  This category includes robotic prostheses, exoskeletons, winged robots, robotic swarms, and intelligent environments where the room itself functions as a robot. These types demonstrate the diverse applications and forms of modern robotics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **26.2.2 Sensing the World**  \n",
    "- **Sensor Types:**  Robots utilize both passive sensors (like cameras, which observe without interaction) and active sensors (like sonar, which emit energy and detect its reflection) to interface with their environment. \n",
    "- **Active vs. Passive Sensors:**  Active sensors provide more detailed information than passive sensors but consume more power and risk interference if multiple are used simultaneously. \n",
    "- **Range Finders:**  These include sonar and optical range sensors that measure the distance to objects by emitting signals and analyzing the returned signal. Innovations like the Kinect and time-of-flight cameras offer sophisticated shape and distance detection. \n",
    "- **Advanced Range Sensing:**  Scanning lidars, used especially in autonomous vehicles, provide precise range measurements using laser beams, superior for long-range detection and effective in various lighting conditions. \n",
    "- **Radar:**  Preferred for air vehicles, radars can detect objects up to several kilometers away and perform well in conditions like fog. \n",
    "- **Tactile Sensors:**  These are used for close-range interaction and are based on physical contact, suitable for detecting immediate surroundings. \n",
    "- **Location Sensors:**  GPS is used outdoors for positioning by triangulating satellite signals, while indoors, localization might rely on wireless signals or fixed beacons. \n",
    "- **Proprioceptive Sensors:**  These inform the robot about its own movement, such as through shaft decoders in robot arms or wheels, crucial for tasks like odometry. \n",
    "- **Force and Torque Sensors:**  Essential in applications requiring delicate manipulation, these sensors help robots adjust the force and torque applied to objects, crucial for handling fragile items without causing damage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **26.2.3 Producing Motion**  \n",
    "- **Actuators:**  These are mechanisms that initiate movement in a robot's effectors, with common types including electric, hydraulic, and pneumatic actuators. Electric actuators are primarily used for rotational movements such as robot arm joints, while hydraulic and pneumatic actuators use fluids and compressed air, respectively, to create mechanical motion. \n",
    "- **Joints:**  Actuators often control joints, which connect different parts of the robot. Types of joints include: \n",
    "- **Revolute Joints:**  Allow rotation around one axis. \n",
    "- **Prismatic Joints:**  Enable sliding movements along an axis. \n",
    "- **Multi-axis Joints:**  Such as spherical, cylindrical, and planar joints, allow movements in multiple directions. \n",
    "- **Grippers:**  Robots interact with objects using various grippers: \n",
    "- **Parallel Jaw Gripper:**  Simple design with two fingers moved by a single actuator, widely used due to its simplicity but limited in versatility. \n",
    "- **Three-fingered Grippers:**  Provide more flexibility while still being simple. \n",
    "- **Humanoid Hands:**  Like the Shadow Dexterous Hand, which has 20 actuators allowing complex manipulations such as reorienting objects in-hand, though they are complex to control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **26.3 What Kind of Problem is Robotics Solving?**  \n",
    "- **Computational Frameworks and Conditions:**  Robotics tackles complex problems using various computational models depending on the scenario: \n",
    "- **MDPs (Markov Decision Processes):**  Used when environments are stochastic yet fully observable. \n",
    "- **POMDPs (Partially Observable Markov Decision Processes):**  Applied in situations where information is incomplete. \n",
    "- **Games:**  Relevant when multiple agents interact, either cooperatively or competitively. \n",
    "- **Robotics as Multiagent, Nondeterministic, Partially Observable:**  Robots often operate in environments where they must interact with humans and other agents, necessitating models that handle both cooperative and competitive dynamics. \n",
    "- **Reward Functions:**  Robots typically act in service of humans, thus the reward function is often aligned with human needs and desires, even though it may be challenging to perfectly capture this in a proxy reward function used by designers. \n",
    "- **Decoupling Perception from Action:**  Robotic systems often simplify the problem by separating perception from action. This division allows for handling complex data inputs and executing motor commands but can lead to challenges in integrating these systems for optimal performance. \n",
    "- **Hierarchical Planning in Robotics:**  \n",
    "- **Task Planning:**  High-level planning that involves defining subgoals or action primitives (e.g., navigating through a building). \n",
    "- **Motion Planning:**  Deals with the pathfinding necessary to achieve the task planning goals. \n",
    "- **Control:**  Focuses on the precise operation of the robot's actuators to execute the motion plan. \n",
    "- **Preference Learning and People Prediction:**  These are crucial for understanding and predicting human actions and preferences, which inform the robot's behavior in dynamic environments. \n",
    "- **Integration Challenges:**  While separating various functionalities (like perception, prediction, and action) simplifies the problem, it also limits the potential for these systems to inform and enhance each other. Ongoing research in robotics aims to better integrate these aspects to improve overall functionality and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **26.4 Robotic Perception**  \n",
    "- **Overview of Perception in Robotics:**  Perception in robotics involves translating sensor measurements into internal representations of the environment, integrating not only traditional computer vision techniques but also incorporating other sensors like lidar and tactile sensors. \n",
    "- **Challenges in Robotic Perception:**  Perception is complicated by sensor noise, partial observability, and the dynamic, unpredictable nature of environments. Robots must effectively estimate states under these conditions. \n",
    "- **Essential Properties of Good Internal Representations:**  \n",
    "1. **Sufficient Detail:**  Representations must provide enough information for decision-making. \n",
    "2. **Efficient Updatability:**  They should be structured to allow for efficient updates. \n",
    "3. **Natural Correspondence:**  Internal variables should naturally correspond to actual state variables in the physical world. \n",
    "- **Modeling and State Estimation:** \n",
    "- Techniques like Kalman filters, Hidden Markov Models (HMMs), and dynamic Bayes nets are employed to model transitions and sensor data of the environment.\n",
    "- These models consider both the robot’s past actions and observed variables, which are integral for updating belief states about the environment. \n",
    "- **Recursive Filtering for Continuous Variables:** \n",
    "- The update process for the belief state in robotics adapts traditional recursive filtering by integrating over continuous variables, reflecting the continuous nature of real-world environments.\n",
    "- The updated belief state calculation involves integrating previous actions and new sensor measurements to estimate the environment's state at the next time step. \n",
    "- **Practical Application:**  For instance, in developing a soccer-playing robot, the belief state would include continuously updated estimates of the soccer ball's location relative to the robot, incorporating both past movements and new visual data to refine the robot's understanding of its environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **26.4.1 Localization and Mapping**  \n",
    "- **Localization Overview:**  Localization determines the position of objects, including the robot itself, within an environment. Using a given map, the robot's position is defined by its Cartesian coordinates and heading (x, y, θ). \n",
    "- **Motion and Sensor Models:**  \n",
    "- **Motion Model:**  A probabilistic model capturing the effects of robot motion on location, typically represented by Gaussian distributions, which account for uncertainties in movement. \n",
    "- **Sensor Models:**  Two primary models are used: \n",
    "- **Landmark-Based:**  Detects specific features in the environment, calculating the range and bearing from the robot to these landmarks. \n",
    "- **Range-Scan:**  Utilizes an array of fixed-bearing sensors to measure distances to the nearest obstacles, advantageous in environments without distinct landmarks. \n",
    "- **Filtering Techniques for Localization:**  \n",
    "- **Kalman Filter:**  Represents the belief state as a Gaussian distribution, effective with linear motion and sensor models. Nonlinear models require linearization, typically handled by an extended Kalman filter (EKF). \n",
    "- **Particle Filter (Monte Carlo Localization - MCL):**  Represents the belief state through a collection of particles, adapting to complex and dynamic environments effectively. It starts with a broad distribution of particles that converge upon acquiring more measurements, refining the robot's estimated location. \n",
    "- **SLAM (Simultaneous Localization and Mapping):**  Addresses scenarios where no pre-existing map is available. Robots must simultaneously map the environment and localize themselves within it. Techniques include EKF and graph relaxation methods for managing and updating map data and robot location. \n",
    "- **Applications and Challenges:**  Localization and mapping are crucial for navigation in both familiar and novel environments, whether the robot is slowly moving through a two-dimensional space or navigating complex three-dimensional terrains. Challenges include handling noisy sensor data, dynamic environments, and integrating continuous updates from sensor inputs to maintain accurate location estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Localization (MCL) Algorithm in Python\n",
    "\n",
    "The Monte Carlo Localization (MCL) algorithm is a form of particle filter used in robotics for localization. It uses a set of particles (or samples) to represent the probability distribution of an estimate of the state (location) of a robot.\n",
    "\n",
    "Below is an implementation of the `MONTE_CARLO_LOCALIZATION` function in Python. This function simulates the next set of samples based on robot movements and sensory inputs, and updates the distribution of particles based on the weights calculated from the sensor data:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sample_motion_model(X, v, w, dt=1.0):\n",
    "    \"\"\"\n",
    "    Simulates the motion of the robot according to the velocity inputs.\n",
    "    This is a simple motion model.\n",
    "    \"\"\"\n",
    "    # Decompose the state\n",
    "    x, y, theta = X\n",
    "    \n",
    "    # Update theta\n",
    "    theta += w * dt\n",
    "    # Update x, y\n",
    "    x += v * dt * np.cos(theta)\n",
    "    y += v * dt * np.sin(theta)\n",
    "    \n",
    "    return np.array([x, y, theta])\n",
    "\n",
    "def ray_cast(j, X, map):\n",
    "    \"\"\"\n",
    "    Simulates a range measurement by performing a ray-casting in the map\n",
    "    from position X in the direction j.\n",
    "    \"\"\"\n",
    "    # This is a stub implementation\n",
    "    # Assume map is a function that takes position and direction and returns the measured range\n",
    "    return map.measure_range(X, j)\n",
    "\n",
    "def sensor_model(z, z_star):\n",
    "    \"\"\"\n",
    "    Compares the actual sensor reading z to the predicted measurement z_star\n",
    "    using a Gaussian noise model.\n",
    "    \"\"\"\n",
    "    sigma = 0.1  # Standard deviation for the sensor noise\n",
    "    return (1.0 / np.sqrt(2.0 * np.pi * sigma ** 2)) * np.exp(-0.5 * ((z - z_star) / sigma) ** 2)\n",
    "\n",
    "def weighted_sample_with_replacement(N, S_prime, W):\n",
    "    \"\"\"\n",
    "    Resamples N particles from S_prime according to the weights W.\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(range(N), size=N, p=W/W.sum())\n",
    "    return S_prime[indices]\n",
    "\n",
    "def monte_carlo_localization(a, z, N, motion_model, sensor_noise_model, map, S):\n",
    "    \"\"\"\n",
    "    Monte Carlo Localization algorithm implementation.\n",
    "    \n",
    "    :param a: Tuple of robot velocities (v, ω)\n",
    "    :param z: Vector of M range scan data points\n",
    "    :param N: Number of particles\n",
    "    :param motion_model: Function for the robot's motion model\n",
    "    :param sensor_noise_model: Function for the sensor noise model\n",
    "    :param map: 2D map of the environment\n",
    "    :param S: Vector of N samples (particles)\n",
    "    :return: Updated set of samples, S\n",
    "    \"\"\"\n",
    "    v, w = a\n",
    "    if S is None or len(S) == 0:\n",
    "        # Initialization phase if S is empty\n",
    "        S = np.random.rand(N, 3)  # Assuming random initialization\n",
    "    \n",
    "    S_prime = np.zeros_like(S)\n",
    "    W = np.ones(N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Sample from the motion model\n",
    "        S_prime[i] = motion_model(S[i], v, w)\n",
    "        \n",
    "        # Update weights based on the sensor model\n",
    "        for j in range(len(z)):\n",
    "            z_star = ray_cast(j, S_prime[i], map)\n",
    "            W[i] *= sensor_noise_model(z[j], z_star)\n",
    "    \n",
    "    # Resample based on the weights\n",
    "    S = weighted_sample_with_replacement(N, S_prime, W)\n",
    "    \n",
    "    return S\n",
    "```\n",
    "\n",
    "\n",
    "### Usage Notes: \n",
    "1. **sample_motion_model**  function simulates the robot's motion. It uses a simple kinematic model and may need adaptation to include more realistic motion physics. \n",
    "2. **ray_cast**  function simulates the sensor's behavior in detecting distances. This stub needs to be fleshed out with actual map interaction for practical use. \n",
    "3. **sensor_model**  computes the likelihood of a sensor reading given the predicted state. \n",
    "4. **weighted_sample_with_replacement**  performs resampling to focus the particle filter on high-probability areas.\n",
    "5. This example assumes a simplistic map and sensor model for demonstration. In a practical scenario, you would need a detailed implementation of these components.\n",
    "\n",
    "This function assumes all functions like `sample_motion_model`, `ray_cast`, and `sensor_model` are appropriately defined, with realistic implementations based on the robot's specific hardware and environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **26.4.2 Other Types of Perception**  \n",
    "- **Beyond Localization and Mapping:**  Robot perception extends beyond spatial awareness to detecting and interpreting various environmental stimuli such as temperature, odors, and sounds. These sensory data are essential for robots to interact effectively with their environment. \n",
    "- **Dynamic Bayes Networks for Perception:**  Many non-spatial sensory estimations in robots can be modeled using dynamic Bayes networks. These models rely on conditional probability distributions that define how state variables evolve over time and how these states relate to sensor measurements. \n",
    "- **Reactive Agents:**  Apart from probabilistic models, robots can also be programmed as reactive agents that operate without explicit probabilistic reasoning about states. This approach focuses on immediate reactions to sensory inputs rather than maintaining and updating beliefs about the world. \n",
    "- **Probabilistic vs. Simpler Techniques:**  While probabilistic methods are often superior for complex perceptual challenges like localization and mapping, they can be cumbersome and complex. In some cases, simpler methods might be equally effective, depending on the specific requirements and constraints of the robot's tasks. \n",
    "- **Practical Experience:**  Direct experience with physical robots is crucial in determining the most effective perception techniques. Working with robots in real-world settings provides insights that can lead to choosing the right balance between sophisticated probabilistic methods and simpler, more direct approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **26.4.3 Supervised and Unsupervised Learning in Robot Perception**  \n",
    "- **Role of Machine Learning:**  Machine learning is critical in robotic perception, especially when the optimal internal representations are unknown. It helps map complex, high-dimensional sensor data into more manageable, lower-dimensional embeddings. \n",
    "- **Low-Dimensional Embedding:**  This technique, a form of unsupervised learning, reduces the dimensionality of sensor data to simplify the model while preserving essential information, making it easier for robots to process and interpret data. \n",
    "- **Adaptive Perception Techniques:**  These methods allow robots to adapt to significant changes in sensor inputs, analogous to how humans adjust to varying lighting conditions. For instance, a robot can adapt its perception model to recognize 'drivable surfaces' under different environmental conditions using a mixture of Gaussians and the EM algorithm to adjust to new textures and colors detected by sensors. \n",
    "- **Self-Supervised Learning:**  Robots can also engage in self-supervised learning where they collect and label their own training data. An example includes using a short-range laser sensor to classify terrain directly in front of the robot, which then trains a model to predict larger areas based on initial classifications. This approach allows robots to extend the effective range of their sensors and adapt their movement strategies based on terrain changes detected from afar. \n",
    "- **Practical Applications:**  These learning techniques are particularly useful in dynamic environments, such as autonomous driving, where conditions can change rapidly and unpredictably. By continuously updating their models based on new data, robots can improve their functionality and decision-making in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **26.5 Planning and Control**  \n",
    "- **Overview of Robot Planning and Control:**  This section of the text discusses how robots decide on movement strategies from high-level planning down to the execution level, involving the direct control of motor functions. The process assumes a fully observable world with deterministic dynamics. \n",
    "- **Motion Planning:**  \n",
    "- **Definition:**  Motion planning involves determining a geometric path that the robot will follow. This path is defined as a sequence of spatial points that the robot, or a part of it such as an arm, needs to navigate through. \n",
    "- **Purpose:**  The primary goal is to find an optimal path through physical space that the robot can follow to achieve its task. \n",
    "- **Trajectory Tracking Control:**  \n",
    "- **Path vs. Trajectory:**  While a path consists of a series of points the robot will move through, a trajectory includes both these points and specific timing information—how long it takes to move from one point to the next. \n",
    "- **Control Task:**  Once a path is established, trajectory tracking control comes into play. This involves executing a sequence of actions that allows the robot to follow the planned trajectory accurately and efficiently. \n",
    "- **Integration of Planning and Control:**  The planning phase determines the 'where' and 'when' aspects of movement, while the control phase focuses on the 'how', ensuring that movements are carried out as planned through precise manipulations of the robot's mechanical systems.\n",
    "\n",
    "In summary, in robotic planning and control, planning determines the desired route and scheduling of movements (the trajectory), and control involves the real-time execution of these movements to adhere to the planned trajectory. This sequence from planning through control is critical for effective robotic operations in deterministic, observable environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **26.5.1 Configuration Space**  \n",
    "- **Workspace vs. Configuration Space (C-space):**  \n",
    "- **Workspace:**  The physical area where the robot operates, defined by dimensions like x, y (and z for 3D environments). \n",
    "- **Configuration Space (C-space):**  An abstract multidimensional space that represents all possible positions and orientations of the robot. Each point in C-space corresponds to a unique state of the robot in the workspace. \n",
    "- **Simplification of Complexity:** \n",
    "- By transforming the workspace into C-space, motion planning computations are simplified. Instead of considering every point on the robot and obstacles, calculations are done on a reduced set of points or configurations.\n",
    "- This transformation reduces the problem of motion planning to navigating through C-space without intersecting C-space obstacles. \n",
    "- **Examples and Dimensions in C-space:** \n",
    "- A non-rotating triangular robot might only require two dimensions (x, y) in C-space if rotation is not considered. Adding rotation would introduce a third dimension (θ).\n",
    "- For robots with scaling abilities or more complex movements, additional dimensions such as scale (s) could be added. \n",
    "- **Complexity in C-space with Articulated Robots:** \n",
    "- For robots with multiple moving parts, like a two-link arm, the C-space becomes defined by the angles of the joints (degrees of freedom - DOF), e.g., (θ_shoulder, θ_elbow).\n",
    "- The configuration of the robot determines the exact position of all its points, based on simple trigonometric calculations (forward kinematics).\n",
    "- Inverse kinematics is used when the desired location of a robot's part is known, and the required configuration to achieve that position needs to be determined. \n",
    "- **C-space Obstacles and Free Space:**  \n",
    "- **C-space Obstacles (C_obs):**  These are areas in C-space where the robot, in certain configurations, would intersect with physical obstacles in the workspace. \n",
    "- **Free Space (C_free):**  Represents the areas of C-space where the robot can exist without interference or collision. \n",
    "- **Practical Implications and Visualization:** \n",
    "- Visualizing C-space can be challenging due to its high dimensionality and abstract nature. Practical applications often involve probing C-space with potential configurations and testing them for collisions in the workspace.\n",
    "- C-space considerations are especially important in complex environments where robots interact with multiple objects or navigate through tight spaces.\n",
    "\n",
    "In summary, C-space is a foundational concept in robotics that facilitates the translation of real-world physical complexities into a more manageable mathematical framework, aiding in effective robot motion planning and control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **26.5.2 Motion Planning**  \n",
    "- **Overview and Definition:**  Motion planning is a fundamental task in robotics, involving finding a collision-free path for a robot to move from one configuration to another. It addresses the problem of navigating through a continuous state space, often referred to as the \"piano mover's problem,\" due to its similarity to the challenge of moving large objects through tight spaces without contact. \n",
    "- **Basic Components:**  \n",
    "- **Workspace (W):**  The physical environment where the robot operates, which can be two-dimensional (R2) or three-dimensional (R3). \n",
    "- **Obstacle Region (O):**  Specific areas within the workspace that are to be avoided. \n",
    "- **Configuration Space (C):**  An abstract space representing all possible positions of the robot, with each point in C corresponding to a specific arrangement of the robot's parts. \n",
    "- **Starting and Goal Configurations (q_s and q_g):**  The initial and target positions of the robot within C-space. \n",
    "- **Path Representation:** \n",
    "- The solution to the motion planning problem is a continuous path parameterized by a curve τ(t), where τ(0) = q_s (start) and τ(1) = q_g (goal). The curve must ensure that all points τ(t) for 0 ≤ t ≤ 1 lie within the collision-free space (C_free). \n",
    "- **Complexities in Motion Planning:**  \n",
    "- **Multiple Goals:**  The goal might be defined as a set of configurations rather than a single point. \n",
    "- **Workspace vs. C-space Goals:**  Goals might be specified in terms of workspace coordinates instead of C-space, adding a layer of complexity in translating these goals into feasible paths. \n",
    "- **Cost Functions:**  Adding criteria such as minimizing path length or energy consumption. \n",
    "- **Constraints:**  Incorporating specific requirements like maintaining the orientation of carried objects to prevent spills. \n",
    "- **Spaces of Motion Planning:**  \n",
    "- **Workspace:**  The real-world physical environment. \n",
    "- **Configuration Space (C):**  Defines the possible states of the robot, depending on its degrees of freedom. \n",
    "- **Path Space:**  Conceptual space where each point represents a complete path through C-space. This space is infinitely dimensional, reflecting the continuous nature of potential paths from start to goal. \n",
    "- **Challenges and Approaches:** \n",
    "- Motion planning involves navigating these complex spaces to devise a path that meets all specified criteria and constraints. The infinite dimensions of path space illustrate the theoretical and practical challenges in generating feasible motion plans.\n",
    "\n",
    "In summary, motion planning is a critical and complex activity in robotics that involves navigating through abstract spaces to find viable paths that avoid obstacles and meet other specified criteria. It requires a deep understanding of both the physical and abstract representations of the robot's environment and capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visibility Graphs**  \n",
    "- **Concept and Application:**  Visibility graphs are a method used specifically for motion planning in two-dimensional environments with polygonal obstacles. They provide a way to find the shortest path between a start point and a goal point, ensuring this path is free of collisions. \n",
    "- **Construction:**  \n",
    "- **Vertices:**  The graph's vertices (V) include the vertices of the polygons that make up the C-space obstacles (V_obs), along with the start (q_s) and goal (q_g) configurations. \n",
    "- **Edges:**  An edge (e_ij) connects two vertices (v_i to v_j) if a straight line between them does not intersect any part of the C-space obstacles (C_obs). This means the two vertices are within each other's line of sight, hence the term \"visibility\" graph. \n",
    "- **Pathfinding:** \n",
    "- To find the optimal path, a graph search algorithm such as best-first search is used, starting at q_s and aiming to reach q_g. The algorithm explores the graph by following edges from vertex to vertex, seeking the shortest path that remains within the collision-free space. \n",
    "- **Advantages:**  \n",
    "- **Optimality:**  Visibility graphs are particularly valued for their ability to provide the shortest possible path between the start and goal configurations, assuming such a path exists. \n",
    "- **Simplicity and Efficiency:**  In two-dimensional spaces with clearly defined polygonal obstacles, visibility graphs simplify the computation and are effective in producing optimal solutions. \n",
    "- **Usage Scenario:**  As illustrated in the use case shown in Figure 26.14, visibility graphs are demonstrated to produce an optimal three-step solution for navigating through a field of obstacles, highlighting their effectiveness in practical scenarios where obstacle boundaries are well-defined and the environment is not overly complex.\n",
    "\n",
    "In summary, visibility graphs are a powerful tool in robotics for solving motion planning problems efficiently in environments with polygonal obstacles, providing guaranteed shortest-path solutions. They are particularly useful in two-dimensional configuration spaces where obstacles and goals are clearly delineated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Voronoi Diagrams**  \n",
    "- **Concept and Purpose:**  Voronoi diagrams are used in motion planning to create paths that maximize the distance from obstacles, unlike visibility graphs which produce paths that closely skirt obstacles. This approach is particularly useful when dealing with uncertain motion or sensing, where maintaining a safe distance from obstacles reduces the risk of collisions. \n",
    "- **Construction and Principle:** \n",
    "- A Voronoi diagram divides a space into regions based on proximity to a set of points, which represent obstacles.\n",
    "- Each region in the diagram consists of all points that are closer to one particular obstacle point than to any others. These regions are defined such that every point within a region is the closest to the same obstacle point.\n",
    "- The edges of these regions form what is known as a Voronoi graph, consisting of lines that represent points equidistant from the nearest two or more obstacles. \n",
    "- **Application in Motion Planning:**  \n",
    "- **Path Initialization:**  Paths are initiated by connecting the start point (q_s) and the goal point (q_g) to the nearest points on the Voronoi graph, typically using straight lines. \n",
    "- **Path Optimization:**  A discrete graph search algorithm is then employed to determine the shortest path along the graph. This method tends to place the path centrally within corridors or open areas, avoiding close proximity to the boundaries and obstacles. \n",
    "- **Advantages and Limitations:**  \n",
    "- **Safety:**  By keeping the path as far from obstacles as possible, Voronoi diagrams enhance safety, making them ideal for applications where buffer zones are necessary. \n",
    "- **Cost of Calculation:**  However, computing Voronoi diagrams can be computationally expensive, especially in higher-dimensional spaces. \n",
    "- **Efficiency in Open Spaces:**  In large, open areas, this method might lead to less efficient paths due to its preference for central routes, potentially resulting in detours that increase travel distance unnecessarily. \n",
    "- **Practical Considerations:** \n",
    "- Voronoi diagrams are beneficial for indoor navigation, providing paths that safely navigate through the middle of passageways. In contrast, in expansive outdoor environments, the paths generated may not always be the most direct or efficient due to the diagram's centralizing tendency.\n",
    "\n",
    "In summary, Voronoi diagrams offer a strategic approach to motion planning that prioritizes safety by distancing paths from obstacles. This method is particularly advantageous in environments where maintaining a buffer zone is crucial, though it may sometimes lead to less direct routes in large open spaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell Decomposition**  \n",
    "- **Overview and Purpose:**  Cell decomposition is a motion planning technique that involves breaking down the configuration space (C-space) into discrete, manageable units called cells. This method simplifies the motion planning problem by allowing path planning within these cells to be straightforward, typically involving simple movements like straight lines. \n",
    "- **Methodology:** \n",
    "- The C-space is divided into contiguous regions or cells.\n",
    "- Path planning is then treated as a discrete graph search problem, where the task is to find a path that connects a sequence of these cells from the start point to the goal. \n",
    "- **Implementation and Examples:** \n",
    "- A common form of cell decomposition uses a regular grid, where each cell in the grid represents a potential step in the path planning process.\n",
    "- Path optimality and costs can be computed using algorithms like Value Iteration or A*, as depicted in the example where grayscale shading represents the cost from each cell to the goal. \n",
    "- **Advantages:** \n",
    "- Simplicity in implementation, particularly in environments where the dimensionality and complexity of the space are manageable. \n",
    "- **Challenges and Limitations:**  \n",
    "- **Dimensionality:**  The method scales poorly with increasing dimensions due to the exponential growth in the number of cells—this is known as the \"curse of dimensionality.\" \n",
    "- **Path Smoothness:**  Paths derived from grid-based decomposition can be jagged or angular, which may not be practically navigable by a robot requiring smoother trajectories. \n",
    "- **Mixed Cells:**  Handling cells that partially contain obstacles can lead to incomplete or unsound planning outcomes. Paths might either avoid potentially navigable areas or unrealistically plan through obstructed spaces. \n",
    "- **Refinements and Advanced Strategies:**  \n",
    "- **Subdivision:**  To address issues with mixed cells, further subdivision of cells can be pursued to refine the resolution of the grid and improve the accuracy of the free space representation. \n",
    "- **Collision Checking:**  Instead of explicitly defining the obstacle space, a collision checker function can be employed to dynamically assess whether a cell is free or obstructed. \n",
    "- * Algorithm:** This approach enhances grid-based planning by incorporating continuous state dynamics, allowing for the planning of more realistic, smoother paths that consider the robot’s physical capabilities and constraints.\n",
    "\n",
    "In summary, cell decomposition offers a structured approach to motion planning by simplifying the complex continuous space into a series of discrete cells. While effective in certain scenarios, particularly those involving simpler or lower-dimensional spaces, the method requires careful handling of mixed cells and path smoothness to be practical for real-world robotic applications. Advanced techniques like Hybrid A* help bridge the gap between theoretical planning and practical motion execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **Randomized Motion Planning**  \n",
    "- **Concept and Approach:**  Randomized motion planning introduces a non-deterministic approach by randomly sampling points in the configuration space (C-space) and connecting them based on the feasibility of direct paths (e.g., straight lines). This method contrasts with structured cell decomposition by not adhering to a regular grid or predefined pattern. \n",
    "- **Probabilistic Roadmap (PRM) Algorithm:**  \n",
    "- **Setup:**  The PRM begins by sampling a set number of milestones (random points in C-free), including the start point qsq_sqs​ and the goal point qgq_gqg​. \n",
    "- **Collision Checking:**  Each sampled point is checked for collisions using a function γ\\gammaγ, ensuring that it lies within the free space. \n",
    "- **Connection Strategy:**  A simple planner, B(q1,q2)B(q_1, q_2)B(q1​,q2​), attempts to connect pairs of milestones. If the planner can find a feasible path between two milestones without a collision, an edge is added between them in the graph. \n",
    "- **Expansion:**  The algorithm tries to connect each milestone to its nearest neighbors or all within a specified radius. If no path from qsq_sqs​ to qgq_gqg​ is initially found, more milestones are sampled and added, and the process repeats. \n",
    "- **Properties and Advantages:**  \n",
    "- **Probabilistic Completeness:**  PRMs are probabilistically complete, meaning that a path will eventually be found if one exists, due to the continuous sampling and expanding search space. \n",
    "- **High-dimensional Spaces:**  This method is particularly effective in high-dimensional spaces where structured methods like grids become computationally infeasible. \n",
    "- **Multi-query Planning:**  PRMs are advantageous for scenarios where multiple goals exist within the same C-space. A roadmap built for one query can be reused, saving computation time and effort across multiple planning tasks. \n",
    "- **Implementation and Use Case:**  \n",
    "- **Roadmap Construction:**  Initially involves an investment in constructing a detailed roadmap that includes potential paths between various points. \n",
    "- **Amortization Over Queries:**  Once constructed, the roadmap can be leveraged for multiple navigation tasks within the same environment, making PRMs efficient for dynamic or multi-goal scenarios.\n",
    "\n",
    "In summary, randomized motion planning via probabilistic roadmaps offers a flexible and efficient solution for navigating complex and high-dimensional spaces. It is particularly suited for environments where multiple paths are needed over time, allowing robots to navigate effectively based on a continuously improving understanding of the space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **Rapidly-Exploring Random Trees (RRTs)**  \n",
    "- **Overview and Concept:**  Rapidly-exploring random trees (RRTs) are an extension of probabilistic roadmaps (PRMs) specifically designed for single-query planning scenarios. They efficiently explore high-dimensional spaces by incrementally building trees from both the start and goal points (denoted as qsq_sqs​ and qgq_gqg​) toward each other. \n",
    "- **Operational Mechanism:**  \n",
    "- **Tree Growth:**  Trees start from qsq_sqs​ and qgq_gqg​. Random milestones are sampled, and attempts are made to connect these to the nearest points in the existing trees. \n",
    "- **Connection:**  When a milestone successfully connects both trees, a path between the start and goal is established. \n",
    "- **Expansion Strategy:**  If a direct connection isn't possible, the trees expand by adding new edges that extend from the closest tree point toward the new milestone by a defined distance, δ\\deltaδ, effectively pushing the exploration into new areas of the space. \n",
    "- **Characteristics and Challenges:**  \n",
    "- **Ease of Use:**  RRTs are favored for their simplicity and effectiveness in navigating complex spaces. \n",
    "- **Solution Quality:**  The paths generated by standard RRTs are usually non-optimal and may lack smoothness, often requiring post-processing to improve path quality. \n",
    "- **Post-Processing:**  Common techniques like \"short-cutting\" involve attempting to simplify the path by removing vertices and directly connecting their neighbors if feasible. \n",
    "- **RRT* Enhancement:**  \n",
    "- **Asymptotic Optimality:**  RRT* is a variant designed to improve upon the basic RRT by ensuring that the solution becomes asymptotically optimal as more samples are added. \n",
    "- **Cost-Based Connection:**  Unlike basic RRTs, RRT* selects neighbors based on a cost function (which includes path length and other metrics) rather than mere proximity. \n",
    "- **Tree Rewiring:**  RRT* continually adjusts its structure, or \"rewires,\" by changing parent nodes within the tree if a cheaper path to a node is found through a new milestone. \n",
    "- **Practical Implications:**  \n",
    "- **Robotic Applications:**  RRTs are particularly useful in robotics for tasks involving complex environments where obstacles and goals are dynamically defined. \n",
    "- **Path Planning:**  They are instrumental in generating feasible routes quickly, though the routes may require refinement to meet specific smoothness or optimality criteria.\n",
    "\n",
    "In summary, RRTs and their enhanced variant RRT* offer powerful tools for robotic path planning, providing rapid exploration and incremental tree expansion capabilities that adapt well to complex, high-dimensional spaces. While RRTs excel in fast pathfinding, RRT* addresses the need for paths that are not only feasible but also close to optimal as more data is incorporated over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Trajectory Optimization for Kinematic Planning**  \n",
    "- **Fundamental Approach:**  Trajectory optimization in kinematic planning begins with a simple but initially infeasible path, typically a straight line, and modifies it to avoid collisions while optimizing a specific cost function, J(τ)J(\\tau)J(τ). \n",
    "- **:**  The objective is to minimize J(τ)J(\\tau)J(τ), where τ\\tauτ is a trajectory mapping the interval [0,1] to configurations, starting at qsq_sqs​ and ending at qgq_gqg​. JJJ is composed of two main components: \n",
    "- **)** : This integrates a cost function over the path that penalizes proximity to obstacles, using a signed distance field to quantify the closeness to obstacles. \n",
    "- **)** : This measures the path's length and smoothness, favoring shorter and less erratic trajectories. It is typically modeled as the integral of the squared velocity, incentivizing shorter paths. \n",
    "- **Optimization Techniques:**  \n",
    "- **Gradient Descent:**  The primary method for finding a feasible path is gradient descent, which adjusts τ\\tauτ by moving in the direction that reduces JJJ. \n",
    "- **Calculus of Variations:**  Used to compute gradients for functionals like JJJ. The Euler-Lagrange equation helps determine how changes in τ\\tauτ affect JJJ. \n",
    "- **Practical Implementation:**  \n",
    "- **Path Integral:**  The optimization accounts for every point on the robot’s body, ensuring that the entire robot avoids obstacles, not just a single point. \n",
    "- **Gradient Challenges:**  The optimization process adjusts the initial straight-line path by calculating gradients that push the trajectory away from obstacles. \n",
    "- **Optimal Path Characteristics:**  \n",
    "- In an obstacle-free scenario, the optimal path τ\\tauτ with respect to JeffJ_{\\text{eff}}Jeff​ would be a straight line, which is the shortest and most efficient route between two points. \n",
    "- When obstacles are present, JobsJ_{\\text{obs}}Jobs​ modifies the trajectory to navigate around them, resulting in a path that balances efficiency with safety. \n",
    "- **Limitations and Advanced Methods:**  \n",
    "- **Local Minima:**  Gradient descent can get stuck in local minima, potentially failing to find the best possible path. \n",
    "- **Advanced Strategies:**  Techniques like simulated annealing can be employed to explore the solution space more thoroughly and escape local optima, increasing the likelihood of finding a better path.\n",
    "\n",
    "In summary, trajectory optimization for kinematic planning strategically modifies an initially simple path to develop a collision-free and cost-effective route. This approach contrasts with randomized methods by optimizing a predefined path rather than adjusting a complex path derived from sampling. The challenge lies in balancing the avoidance of obstacles with the maintenance of path efficiency, utilizing advanced mathematical tools and optimization techniques to iteratively refine the trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **26.5.3 Trajectory Tracking Control**  \n",
    "- **Overview of Trajectory Tracking Control:**  This area focuses on how a planned path (trajectory) is translated into actual motor commands and adjustments to keep a robot on the desired path. It involves both open-loop and closed-loop control mechanisms. \n",
    "- **From Configurations to Torques (Open-Loop Control):**  \n",
    "- The trajectory, denoted as τ(t)\\tau(t)τ(t), dictates the desired configurations over time from start qsq_sqs​ to goal qgq_gqg​. \n",
    "- A dynamics model calculates the necessary torques based on the robot's configuration, velocity, and acceleration to follow this trajectory. This model relates the applied torques to expected accelerations (similar to F=maF = maF=ma for linear systems). \n",
    "- **Closed-Loop Control:** \n",
    "- Addresses real-world deviations from the planned path by continuously adjusting the torques based on observed errors.\n",
    "- Proportional controllers (P controllers) correct deviations by applying forces proportional to the error between the current state and the desired trajectory. This approach can lead to overshooting due to the robot’s inertia. \n",
    "- **Improving Control with PD Controllers:** \n",
    "- PD controllers add a derivative term to the proportional control, enhancing stability by dampening oscillatory responses that occur with proportional-only control.\n",
    "- The derivative term mitigates rapid changes in error, leading to smoother adjustments and maintaining the robot closer to its intended path. \n",
    "- **PID Controllers for Comprehensive Correction:** \n",
    "- PID controllers incorporate an integral term along with proportional and derivative terms. This integration helps eliminate steady-state errors by adjusting the control forces based on the accumulated past errors, ensuring long-term accuracy.\n",
    "- These controllers are highly effective in diverse industrial applications where precision and adaptability are crucial. \n",
    "- **Challenges in Implementation:** \n",
    "- Implementing these controllers requires careful tuning of the parameters (gain factors) to balance responsiveness with stability.\n",
    "- The robot's physical characteristics (like mass and inertia) and external disturbances (like friction or external forces) can affect the efficacy of the control algorithms. \n",
    "- **Advanced Control Techniques:**  \n",
    "- **Computed Torque Control:**  Combines predictive (feedforward) control based on the dynamics model with corrective (feedback) control that adjusts for real-time errors. This method calculates the expected torques and supplements them with proportional-derivative corrections based on the current state deviations.\n",
    "- This hybrid approach adjusts the control gains dynamically depending on the robot's configuration, providing a nuanced response to the complex dynamics involved in robotic motion.\n",
    "\n",
    "In summary, trajectory tracking control in robotics encompasses a range of techniques from basic open-loop controls that translate planned paths directly into actuator commands, to sophisticated closed-loop controls like PID controllers that adjust actions based on the differences between the planned and actual paths. These control strategies are essential for executing precise and reliable movements in robotic systems, adapting to both the theoretical models and the unpredictable variables encountered in real-world environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plans versus Policies**  \n",
    "- **Context and Comparison:** \n",
    "- The chapter relates motion planning in robotics to concepts previously discussed in the contexts of search, Markov Decision Processes (MDPs), and reinforcement learning. In robotics, motion is viewed through the lens of an underlying MDP where states include dynamic aspects like configuration and velocity, and actions are typically control inputs such as torques. \n",
    "- **Definition of Plans and Policies:**  \n",
    "- **Plans:**  These are predefined sequences of actions designed to achieve a goal from a particular start state. Plans are static and do not adapt to changes in the environment or the system’s state once execution begins. \n",
    "- **Policies:**  In contrast, policies provide guidelines or rules that decide the action to take based on the current state, regardless of the initial state. Policies are dynamic and adaptive, offering a course of action for any state the system might encounter. \n",
    "- **Application in Robotics:**  \n",
    "- **Motion Planning as Plan Creation:**  Initially, motion planning in robotics simplifies the state and action space to kinematic states, disregarding the underlying dynamics. This approach yields a reference path or plan based on the assumption of perfect state transitions without considering dynamic factors. \n",
    "- **From Plans to Policies:**  Due to the imperfections and inaccuracies in the dynamics model, the static plan derived from simplified motion planning cannot be directly executed. Instead, it is converted into a policy. This policy aims to follow the planned path but adjusts actions based on deviations from this path, attempting to correct any drifts. \n",
    "- **Challenges and Suboptimality:**  \n",
    "- **Suboptimality in Plans:**  Ignoring the dynamics during the planning phase leads to plans that might not be feasible when dynamic states are considered, resulting in suboptimal paths. \n",
    "- **Policy Adaptation:**  The policy derived from the plan is inherently suboptimal as well. It assumes that the best action in any deviated state is to return to the previously planned path, which might not always be optimal given the continuous and high-dimensional nature of dynamic states and action spaces. \n",
    "- **Advanced Techniques:** \n",
    "- The discussion moves towards methods that develop policies directly considering dynamic states, eliminating the need to simplify or separate the problem into static kinematic planning and dynamic adjustments. This approach aims to compute policies that are inherently more aligned with the real-world dynamics of robotic systems.\n",
    "\n",
    "In essence, this segment underscores a fundamental shift from creating rigid, non-adaptive plans to developing flexible, responsive policies that better accommodate the complexities of dynamic environments in robotics. These policies are designed to optimize actions across all possible states, embracing the full scope of challenges presented by real-world robotics applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
