{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Solving Problems with Search\n",
    "\n",
    "- **Chapter 3 Overview**: Focuses on problem-solving through search, demonstrating how an agent plans a sequence of actions to achieve a goal.\n",
    "- **Problem-solving Agent**: An agent that needs to plan a series of actions leading to a goal state; involves a computational process known as search.\n",
    "- **Atomic Representations**: Used by problem-solving agents, where states of the world are considered as indivisible wholes without internal structure (referenced from Section 2.4.7).\n",
    "- **Contrast with Planning Agents**: Planning agents, using factored or structured state representations, are discussed in later chapters (7 and 11).\n",
    "- **Search Algorithms**: Introduction to several search algorithms within simple environments characterized by being episodic, single-agent, fully observable, deterministic, static, discrete, and known.\n",
    "- **Informed vs. Uninformed Algorithms**: Distinction between algorithms where agents can estimate distance to the goal (informed) and those without such estimates (uninformed).\n",
    "- **Expansion in Subsequent Chapters**: Chapter 4 expands on environmental constraints, and Chapter 6 delves into scenarios involving multiple agents.\n",
    "- **Asymptotic Complexity**: The chapter employs concepts of asymptotic complexity, specifically O(n) notation, to describe algorithm efficiency.\n",
    "- **Search Trees**: The chapter uses search trees to represent the search space, where nodes represent states and edges represent actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Problem-Solving Agents\n",
    "\n",
    "- **Scenario Description**: An agent on vacation in Romania, starting in Arad and needing to reach Bucharest with limited knowledge of the area.\n",
    "- **Problem Complexity**: Agent faces a complex decision problem due to unfamiliar geography and multiple choices (Sibiu, Timisoara, Zerind).\n",
    "- **Agent's Challenge in Unknown Environment**: Without additional information, the agent might choose actions randomly, a situation explored in Chapter 4.\n",
    "- **Four-Phase Problem-Solving Process**:\n",
    "   - **Goal Formulation**: Agent sets a clear objective (reaching Bucharest), simplifying decision-making by focusing on relevant actions.\n",
    "   - **Problem Formulation**: Agent develops an abstract model of the relevant world, here considering travel between adjacent cities, with the changing state being the current city.\n",
    "   - **Search**: Agent simulates action sequences in the model to find a solution (e.g., traveling through Sibiu, Fagaras to Bucharest) before acting in the real world.\n",
    "   - **Execution**: Once a solution is found, the agent executes the planned actions sequentially.\n",
    "\n",
    "- **Fixed Action Sequences in Certain Environments**: In fully observable, deterministic, known environments, a problem's solution is a fixed sequence of actions.\n",
    "- **Open-Loop System**: In such environments, the agent can ignore percepts during execution, as the solution is guaranteed if the model is correct.\n",
    "\n",
    "- **Adaptation in Less Predictable Environments**: In partially observable or nondeterministic environments, solutions involve branching strategies with contingency plans based on potential percepts (e.g., alternative routes in case of unexpected circumstances like road closures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Search Problems and Solutions\n",
    "\n",
    "- **Formal Definition of a Search Problem**:\n",
    "   - **State Space**: A set of possible states in which the environment can exist.\n",
    "   - **Initial State**: The starting point of the agent, e.g., Arad.\n",
    "   - **Goal States**: One or more desired states. The goal might be a single state (like Bucharest), a set of states, or defined by a certain property applicable to many states.\n",
    "   - **IS-GOAL Method**: A method to determine if a state is a goal, accommodating various types of goal states.\n",
    "   - **Actions**: Defined for each state; `ACTIONS(s)` returns a set of actions executable in state `s`.\n",
    "   - **Transition Model**: Describes the outcome of actions; `RESULT(s, a)` gives the state resulting from action `a` in state `s`.\n",
    "   - **Action Cost Function**: Denoted as `ACTION-COST(S,a, s')` or `c(s, a, s')`, it quantifies the cost of performing action `a` in state `s` to reach state `s'`. Reflects the agent's performance measure (e.g., distance, time).\n",
    "\n",
    "#### **Solution and Path**:\n",
    "\n",
    "\n",
    "- **Path**: A sequence of actions.\n",
    "- **Solution**: A path leading from the initial state to a goal state.\n",
    "- **Optimal Solution**: The solution with the lowest path cost among all solutions.\n",
    "- **Assumption of Additive Action Costs**: Total path cost is the sum of individual action costs.\n",
    "- **Positive Action Costs**: Assumes all action costs are positive to avoid complications.\n",
    "\n",
    "#### **Graphical Representation**:\n",
    "\n",
    "\n",
    "- **State Space as a Graph**: States as vertices and actions as directed edges.\n",
    "- **Example**: The map of Romania, where roads represent actions between cities (states).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
