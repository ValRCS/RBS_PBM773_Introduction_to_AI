{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 24 - DEEP LEARNING FOR NATURAL LANGUAGE PROCESSING\n",
    "\n",
    "*In which deep neural networks perform a variety of language tasks, capturing the structure\n",
    "of natural language as well as its fluidity.* - Stuart Russell and Peter Norvig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction** \n",
    "- Chapter 23 discussed the core components of natural language, such as grammar and semantics. It highlighted how systems utilizing parsing and semantic analysis have shown promising results in various tasks, but are hindered by the complex nature of language in real-world texts.\n",
    "- This chapter introduces the idea of leveraging the vast amounts of text available in a machine-readable format to explore data-driven machine learning approaches, specifically using deep learning tools (introduced in Chapter 21) for enhancing natural language processing (NLP). \n",
    "- The introduction outlines the structure of the chapter: \n",
    "- **Section 24.1** : Discusses the improvement of learning by representing words as points in a high-dimensional space, rather than as atomic entities. \n",
    "- **Section 24.2** : Explores the use of recurrent neural networks (RNNs) for capturing meaning and long-distance context in sequential text processing. \n",
    "- **Section 24.3** : Focuses on machine translation as a key area where deep learning has significantly advanced NLP. \n",
    "- **Sections 24.4 and 24.5** : Describe models that can be trained on large volumes of unlabeled text to achieve state-of-the-art results in specific NLP tasks. \n",
    "- **Section 24.6** : Provides an overview of the current state of the field and contemplates future directions for NLP research and application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **24.1 Word Embeddings** \n",
    "- Word embeddings provide a way to represent words as dense vectors in a high-dimensional space without manual feature engineering, enabling generalization between related words across various aspects like syntax, semantics, topic, and sentiment.\n",
    "- The motivation for word embeddings comes from the idea that a word's meaning is closely related to the words it frequently appears with, leading to the representation of words as vectors based on their context (n-gram counts) within text.\n",
    "- This approach moves away from one-hot vectors, which fail to capture the similarity between words, towards dense, lower-dimensional vectors that can efficiently represent words and their relationships.\n",
    "- Word embeddings are learned from data and have the unique property of clustering similar words together in the vector space. Moreover, these vectors can capture complex relationships between words, such as analogies, by their spatial relationships.\n",
    "- While word embeddings are effective for a broad range of NLP tasks and can be obtained pre-trained from various sources (e.g., WORD2VEC, GloVe, FASTTEXT), training task-specific embeddings can further enhance performance by focusing on relevant aspects of the words.\n",
    "- The process of learning word embeddings often occurs alongside training for a specific NLP task, using techniques such as POS tagging as an entry point to understand the application of deep learning to NLP. This involves training both the embeddings and the task-specific model simultaneously, allowing for the model to leverage contextual clues in predicting, for example, the correct part of speech for a word.\n",
    "- An alternative to word-level embeddings is character-level models, which learn from sequences of characters, although the majority of NLP work favors word-level representations for their effectiveness and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **24.2 Recurrent Neural Networks for NLP** \n",
    "- While word embeddings effectively represent individual words, language inherently involves sequences of words where the context significantly influences meaning. Therefore, a more complex approach is necessary for tasks beyond simple ones like part-of-speech tagging.\n",
    "- For tasks requiring deep understanding, such as question answering or reference resolution, a broader context is crucial. These tasks may need context spanning dozens of words to accurately interpret references and relationships within text.\n",
    "- Recurrent Neural Networks (RNNs) are introduced as a solution for handling the sequential nature of language. RNNs are designed to process sequences by maintaining a form of memory of what has been processed, allowing them to consider the context of preceding and succeeding words to make informed predictions.\n",
    "- An example provided illustrates the need for broad context in understanding language: in a sentence mentioning several individuals and actions, identifying the referent of a pronoun (\"him\" referring to \"Miguel\" and not \"Eduardo\") requires considering information across the entire sentence. RNNs, by design, can manage this by leveraging their ability to remember and integrate information across a sequence as it is processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **24.2.1 Language Models with Recurrent Neural Networks** \n",
    "- Language models are probabilistic models that predict the next word in a sequence based on all previous words, serving as foundational elements for more complex NLP tasks.\n",
    "- Traditional n-gram and fixed-window feedforward network approaches face challenges with context limitations and parameter explosion, and feedforward networks also struggle with asymmetry—learning about word positions independently.\n",
    "- Recurrent Neural Networks (RNNs) are introduced as a solution capable of processing sequences, such as language, one word at a time. This allows RNNs to overcome the issues associated with fixed-window and feedforward approaches.\n",
    "- In RNN language models, each word is represented as an embedding vector. The model updates its hidden state over time, carrying forward information that theoretically allows it to consider the entire context of the sentence to date.\n",
    "- RNNs manage to reduce the parameter count to a constant level regardless of sequence length, addressing the scalability problem of other models and ensuring uniformity across different word positions, thus solving the asymmetry issue.\n",
    "- Although RNNs theoretically can remember information indefinitely through their hidden states, in practice, there's a limit to how much they can store and recall, depending on the complexity and length of the sequence.\n",
    "- Training an RNN involves feeding it sequences from a text corpus, predicting each next word based on previous ones, and adjusting model parameters through backpropagation to minimize prediction errors.\n",
    "- Once trained, RNNs can generate text by taking an initial input word and producing a sequence of output words, using the model to predict each subsequent word. The approach to sampling next words from the model's output can vary, affecting the diversity and unpredictability of generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **24.2.2 Classification with Recurrent Neural Networks** \n",
    "- RNNs can be applied to various NLP classification tasks such as part of speech (POS) tagging and coreference resolution. The model architecture remains similar, but the output layer is tailored to the specific task—outputting a softmax distribution over POS tags for tagging or possible antecedents for coreference resolution.\n",
    "- Training RNNs for classification involves using labeled data, which presents a greater challenge in data collection compared to language models that can learn from unlabeled text.\n",
    "- Unlike language models that predict the next word based on previous context, classification tasks can benefit from considering both preceding and following context within a sentence. This is crucial for accurately resolving references and understanding sentence structure beyond a left-to-right sequence.\n",
    "- Bidirectional RNNs are introduced to capture context from both directions in a sentence, improving the model's ability to handle tasks like coreference resolution by considering all relevant information within the sentence.\n",
    "- RNNs can also be utilized for sentence- or document-level classification tasks, such as sentiment analysis, where the objective is to classify the entire text rather than individual words or phrases. This requires aggregating information across the entire sentence or document.\n",
    "- For sentence-level classification, a common approach is to use the hidden state from the last word as a summary of the sentence. However, this may bias the model towards the sentence's end. Alternative techniques like average pooling of hidden states are used to mitigate this by providing a more balanced representation of the entire sentence's content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **24.2.3 LSTMs for NLP Tasks** \n",
    "- While Recurrent Neural Networks (RNNs) theoretically can carry information across many time steps, in practice, they struggle with maintaining context over long sequences. This is akin to the \"telephone game\" effect, where information gets increasingly distorted or lost as it is passed along, a manifestation of the vanishing gradient problem but across temporal layers.\n",
    "- Long Short-Term Memory (LSTM) networks are introduced as an advanced variant of RNNs designed to address these issues. LSTMs incorporate gating mechanisms that allow them to selectively remember and forget information across long sequences, thereby preserving essential context without the dilution or distortion common in traditional RNNs.\n",
    "- The gating units within LSTMs enable the network to maintain a balance between retaining valuable information from the past and updating the hidden state based on new inputs. This makes LSTMs particularly effective for tasks involving complex, long-distance dependencies within text.\n",
    "- An illustrative example highlights the ability of LSTMs to handle long sentences with many intermediate elements, maintaining the necessary context to accurately predict word forms that agree in number with distant subjects. This demonstrates LSTMs' superior capability in managing long-term dependencies compared to regular RNNs and n-gram models, which often struggle with such linguistic structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **24.3 Sequence-to-Sequence Models** \n",
    "- Machine Translation (MT) is a critical task in NLP that involves translating sentences from a source language to a target language, necessitating a model trained on large corpora of source-target sentence pairs.\n",
    "- Traditional RNNs face challenges in MT due to the lack of a one-to-one correspondence between words in different languages, complexities in word order, and the need to consider the entire source sentence context and previously generated target words.\n",
    "- Sequence-to-sequence (Seq2Seq) models address these challenges by using two RNNs: one to encode the source sentence into a context vector (encoder) and another to generate the target sentence from this vector (decoder), effectively linking the target word generation to both the entire source sentence and the sequence of previously generated target words.\n",
    "- This architecture allows for dynamic translation, adapting to the sentence structure and content of the source language while generating coherent and contextually appropriate translations in the target language.\n",
    "- Seq2Seq models have significantly advanced the field of MT, achieving notable reductions in error rates compared to previous methods. They are also versatile, being applicable to tasks beyond MT, such as text caption generation from images and text summarization.\n",
    "- However, Seq2Seq models have limitations, including a bias towards recent context due to the necessity of fitting information into a finite hidden state vector, a fixed context size limit that constrains the amount of information that can be represented in the model's hidden state, and inefficiencies in sequential processing that hinder training speed and scalability.\n",
    "- These shortcomings highlight areas for further innovation in model design and training approaches to enhance the performance and applicability of Seq2Seq models in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **24.3.1 Attention** \n",
    "- The attention mechanism enhances sequence-to-sequence (Seq2Seq) models by enabling the target RNN to condition on all hidden vectors from the source RNN, not just the final one. This addresses the limitations of fixed context size and context bias by allowing the model to access any part of the source sequence as needed.\n",
    "- Attention allows the model to focus on different parts of the source sentence for generating each word in the target sentence, dynamically adjusting its \"focus\" based on the current translation task.\n",
    "- The process involves computing attention scores between the current target state and each source word, normalizing these scores to form a probability distribution, and then creating a context vector as a weighted average of source hidden states. This context vector, which encapsulates relevant information for the next target word, is used along with the target input word to generate the next word in the target sequence.\n",
    "- Attention models are differentiated from standard Seq2Seq models in that they concatenate the target input word with a context vector derived from the entire source sequence, thereby integrating more comprehensive information from the source.\n",
    "- Attention is differentiable, allowing it to be integrated with back-propagation training methods, and it provides a probabilistic method for the model to incorporate long-distance dependencies and represent uncertainty in source-target alignments.\n",
    "- The attention probabilities are often interpretable, making it possible to visually represent and understand how the model is aligning source and target words, akin to human translation patterns.\n",
    "- While particularly beneficial for machine translation, the attention mechanism is broadly applicable to various NLP tasks that can be framed as sequence-to-sequence problems, such as question-answering systems, by allowing flexible and dynamic consideration of source information for generating target sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **24.3.2 Decoding** \n",
    "- Decoding in sequence-to-sequence (Seq2Seq) models is the process of generating the target sentence from a given source sentence, post-training. The model does this by sequentially predicting each word of the target sentence, conditioned on the source sentence and all previously generated target words.\n",
    "- The simplest decoding strategy is greedy decoding, where the model selects the highest probability word at each timestep as the next word in the sequence. While fast, this approach can lead to suboptimal results because it commits to each choice without considering the overall sequence probability, potentially compounding errors.\n",
    "- Greedy decoding's limitations are illustrated through translation examples where correct word order and selection depend on understanding the entire sentence structure, something greedy decoding may not handle well due to its incremental, non-revisiting nature.\n",
    "- A more effective strategy involves search algorithms that explore multiple hypotheses for each word prediction. Beam search is highlighted as a commonly used method, maintaining a fixed number of the best hypotheses (the \"beam\") at each timestep and expanding each hypothesis in the beam by considering multiple next-word options.\n",
    "- Beam search balances the breadth of exploration with computational efficiency by only keeping the top-scoring hypotheses at each step, thereby avoiding the exhaustive search's computational cost while aiming to find a high-quality sequence.\n",
    "- The choice of beam width (the number of hypotheses kept at each step) is crucial for balancing between quality and speed of decoding. Current state-of-the-art models use relatively small beams (4 to 8) compared to older statistical MT models, benefiting from advances in model accuracy that reduce the need for broader search to find optimal sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **24.4 The Transformer Architecture** \n",
    "- The transformer architecture, introduced by Vaswani et al. in 2018, represents a significant departure from previous sequence modeling and machine translation approaches by eliminating the need for sequential data processing inherent to RNNs and LSTMs.\n",
    "- Central to the transformer model is the self-attention mechanism, which allows the model to weigh the importance of different words within a sentence, regardless of their positional distance from each other. This mechanism enables the model to capture long-distance dependencies more effectively than models constrained by sequential processing.\n",
    "- Unlike architectures that process data sequentially, the transformer can handle entire sequences simultaneously, a property known as parallelization. This significantly reduces training times and allows the model to consider the full context of a sentence or sequence in one step.\n",
    "- The transformer has set new standards for performance in a wide range of natural language processing tasks, including but not limited to machine translation, due to its ability to learn complex patterns and relationships in data without the limitations imposed by the sequential nature of RNNs and LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **24.4.1 Self-attention** \n",
    "- Self-attention, an extension of the attention mechanism within sequence-to-sequence models, enables each sequence (both source and target) to attend to itself, capturing both long-distance and nearby context more effectively.\n",
    "- Traditional dot product-based attention mechanisms inherently bias hidden states to attend to themselves due to the high dot product values when a vector is compared with itself. The transformer addresses this by projecting input vectors into three distinct spaces, forming query (q), key (k), and value (v) vectors with separate weight matrices. This allows for differentiated roles within the attention mechanism, where the query vector seeks information, the key vector offers information to be attended to, and the value vector provides the actual context.\n",
    "- The attention score between any two positions in the sequence is computed as the scaled dot product of their respective query and key vectors, normalized across all positions to form a probability distribution. The context vector for each position is then calculated as a weighted sum of all value vectors, weighted by this distribution, enabling each word to dynamically focus on the most relevant parts of the input.\n",
    "- Self-attention is asymmetric, meaning the influence between two different positions is not necessarily reciprocal. The scale factor of the square root of the dimensionality (d) of the key and query vectors improves numerical stability.\n",
    "- The entire process is highly parallelizable, as the computations for all positions in a sequence can be performed simultaneously using matrix operations, leveraging modern hardware for efficiency.\n",
    "- The selection of context in self-attention is learned from the data rather than being pre-defined, allowing the model to adaptively determine the most relevant information for each word in a sequence.\n",
    "- Multiheaded attention further enhances the model's ability to capture various aspects of context by dividing the input into multiple segments (heads), applying attention to each separately with distinct weights, and then concatenating the results. This approach enables the model to maintain and emphasize important contextual signals that might be diluted in a single, aggregated context representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **24.4.2 From Self-attention to Transformer** \n",
    "- The transformer model integrates self-attention as a foundational component but encompasses additional layers and mechanisms to effectively process sequential data.\n",
    "- Each transformer layer is structured to include self-attention sub-layers followed by feedforward neural networks. These feedforward networks apply the same set of weights across all positions in the sequence, ensuring that each word is independently processed in parallel.\n",
    "- To mitigate potential issues such as the vanishing gradient problem, the transformer incorporates residual connections around each of the sub-layers, facilitating deeper model architectures by allowing gradients to flow more freely during training.\n",
    "- Despite the strengths of self-attention in capturing contextual relationships between words, it inherently lacks sensitivity to the sequence order of words. To overcome this, transformers employ positional embeddings, which encode the order of words in the sequence, ensuring that this critical aspect of language structure is preserved.\n",
    "- The combined input for each transformer layer consists of word embeddings enhanced by these positional embeddings, allowing the model to maintain awareness of both word identity and position within the sequence.\n",
    "- Transformers are designed with multiple layers (commonly six or more), where the output of one layer serves as the input to the next, progressively refining the representation of the sequence at each step.\n",
    "- While the discussed architecture describes the transformer encoder, which is adept at tasks like text classification, the full transformer model for sequence-to-sequence tasks like machine translation also includes a transformer decoder. The decoder mirrors the encoder's structure but with modifications to ensure proper sequential generation of text: it employs masked self-attention to prevent future words from influencing the prediction of the current word and includes additional attention layers that focus on the encoder's output.\n",
    "- This encoder-decoder architecture, with its innovative use of self-attention, positional embeddings, and parallel processing capabilities, marks a significant advancement in the field of NLP, setting new standards for a wide range of language understanding and generation tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **24.5 Pretraining and Transfer Learning** \n",
    "- In the realm of natural language processing (NLP), obtaining a sufficient amount of labeled data for model training can be a significant challenge. Unlike in computer vision, where large datasets like ImageNet are hand-labeled, NLP often relies on unlabeled text due to the complexity and expertise required for annotating linguistic elements.\n",
    "- The abundance of text data available, particularly on the internet, presents an opportunity to leverage unlabeled data for model development. Projects like Common Crawl facilitate access to this vast repository, encompassing a wide range of text from digitized books to social media posts.\n",
    "- This text data, while largely unlabeled, can still offer valuable insights for NLP tasks. For example, structured formats like FAQ question-answer pairs or side-by-side translations on websites provide a foundation for training models in specific applications such as question answering and machine translation, respectively.\n",
    "- To circumvent the need for creating new datasets for every NLP task, the concept of pretraining is introduced. Pretraining is a form of transfer learning where a model is initially trained on a large corpus of general-domain language data. This foundational training equips the model with a broad understanding of language, including vocabulary and syntactic structures.\n",
    "- After pretraining, the model can be further refined with a smaller set of domain-specific data, which may include some labeled examples. This refinement process allows the model to adapt to the particular linguistic characteristics of the target domain, improving its performance on domain-specific NLP tasks.\n",
    "- The pretraining and transfer learning approach offers a practical solution to the data scarcity challenge in NLP, enabling the development of robust models without the need for extensive labeled datasets for every new task.\n",
    "- The success of pretraining and transfer learning in NLP is exemplified by models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which have achieved state-of-the-art results across various language understanding tasks. These models leverage large-scale pretraining on general text data to learn rich language representations, which can then be fine-tuned for specific applications with smaller labeled datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **24.5.1 Pretrained Word Embeddings** \n",
    "- Word embeddings represent words as vectors in a high-dimensional space, capturing linguistic relationships such as similarity and analogy through unsupervised learning from large text corpora, without the need for labeled data like POS tags.\n",
    "- The GloVe (Global Vectors) model is highlighted as a specific approach to generating word embeddings. It operates by analyzing word co-occurrence within a defined window across the text, computing probabilities that reflect the likelihood of each word appearing in the context of others.\n",
    "- A key insight of GloVe is that the essence of word relationships can be captured by comparing their co-occurrence probabilities with various other words. For example, the relationship between \"ice\" and \"steam\" can be discerned by examining their probability ratios in the context of words like \"solid\" and \"gas,\" leading to intuitive associations based on physical states.\n",
    "- GloVe mathematically formulates these relationships into constraints where the dot product of two word vectors approximates the logarithm of their co-occurrence probability, linking geometric closeness in the vector space to semantic closeness in language use.\n",
    "- To address overfitting, GloVe introduces two sets of embeddings for each word, which are combined in the final model, improving robustness and representation quality.\n",
    "- Training word embeddings with models like GloVe is computationally efficient, allowing for the processing of billions of words in a matter of hours on standard computing resources.\n",
    "- Beyond general language models, word embeddings can be tailored to specific domains to uncover domain-specific knowledge. For instance, embeddings trained on scientific abstracts can predict relationships and classifications within fields like material science, indicating that these models capture not just word co-occurrences but deeper conceptual knowledge.\n",
    "- Such domain-specific embeddings can even predict future discoveries, as demonstrated by a model trained on material science abstracts up to 2008, which accurately identified materials later confirmed as thermoelectric in subsequent research.\n",
    "- Pretrained word embeddings, therefore, serve as a powerful tool for both general and specialized NLP applications, enabling models to leverage vast amounts of unlabeled text for learning semantic and syntactic patterns inherent in language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **24.5.2 Pretrained Contextual Representations** \n",
    "- Traditional word embeddings, while capturing semantic similarities, fall short in handling polysemy—words with multiple meanings. For example, \"rose\" can denote a flower or the past action of rising, each context suggesting a different semantic cluster.\n",
    "- To address this limitation, the concept of contextual representations was developed. Unlike static word embeddings, contextual representations dynamically generate word embeddings based on the surrounding context, allowing the model to distinguish between different meanings of the same word based on its usage in a sentence.\n",
    "- Contextual representations are generated by models that consider not only the target word but also its context, producing embeddings that vary with the word's application. For instance, \"rose\" in a gardening context would generate an embedding similar to other flowers, while in a context involving elevation, its embedding would align with terms related to rising.\n",
    "- A recurrent neural network (RNN) is one approach to creating these contextual embeddings. The RNN processes text sequentially, updating its representation for each word based on both its static embedding and the accumulated context from preceding words.\n",
    "- This process involves feeding the RNN individual words along with their context, aiming to predict the subsequent word in the sequence. Through training, the model learns to generate rich contextual embeddings that reflect the nuanced meanings and syntactic roles of words in various linguistic environments.\n",
    "- Unlike models for tasks like POS tagging, which might use bidirectional processing to consider both preceding and following context, the described RNN operates unidirectionally, focusing on the context leading up to the current word to predict the next.\n",
    "- Once trained, such a model can serve as a foundation for generating contextual embeddings for a wide range of tasks, bypassing the need to continue next-word prediction once the model has been sufficiently trained.\n",
    "- Contextual representations significantly enhance NLP models' ability to understand and process language by providing a more nuanced, context-sensitive approach to handling word meaning and usage, addressing the inherent limitations of non-contextual word embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **24.5.3 Masked Language Models** \n",
    "- Masked Language Models (MLMs) represent an innovative approach to language modeling that overcomes the limitations of traditional models, which rely solely on previous words for context, failing to consider the potential contextual relevance of subsequent words in a sentence.\n",
    "- Standard bidirectional models attempt to address this by training separate models for each direction (left-to-right and right-to-left) and then combining their outputs. However, these models still fall short of fully integrating contextual information from both directions simultaneously.\n",
    "- MLMs address this challenge by randomly masking out words in the input sentences and training the model to predict these masked words based on the context provided by the remaining unmasked words. This allows the model to learn from context in all directions—both preceding and following the masked word.\n",
    "- In practice, a deep bidirectional architecture, such as a transformer, processes the sentence with masked words, and the model's task is to fill in the blanks accurately. For instance, from a partially masked sentence like \"The river ___ five feet,\" the model learns to predict the missing word \"rose.\"\n",
    "- The training process involves multiple iterations over the same sentence with different words masked each time, enhancing the model's ability to understand and predict based on diverse contextual cues.\n",
    "- Crucially, MLMs leverage unlabeled data effectively, as the learning signal is internally generated by masking words in natural sentences, eliminating the need for externally provided labels.\n",
    "- Once trained on a large text corpus, MLMs can produce rich, contextually informed representations that significantly improve performance across a broad range of NLP tasks, including machine translation, question answering, text summarization, and more, by offering a deeper understanding of language nuances and syntax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **24.6 State of the Art (as of 2020)** \n",
    "- The progress in NLP has been likened to the transformative impact deep learning had on computer vision around 2012, with 2018 marking a pivotal moment for NLP, largely due to the success of transfer learning. General language models now can be fine-tuned for specific tasks, significantly advancing the field.\n",
    "- The evolution from simple word embeddings (like WORD2VEC and GloVe) to more complex pretrained contextual representations has been supported by hardware advancements (GPUs and TPUs), making training on large datasets feasible.\n",
    "- The transformer model, with its efficient training capabilities and deep neural network architecture, has become a staple starting point for many NLP projects since 2018. These models, originally trained for next-word prediction, have shown remarkable versatility across various NLP tasks.\n",
    "- Models like ROBERTA and GPT-2 have demonstrated that fine-tuning pretrained transformers can lead to state-of-the-art results in areas such as question answering, reading comprehension, and even generating coherent text.\n",
    "- ARISTO, an ensemble NLP system, notably achieved high scores on standard science exams, illustrating the practical applications of these advancements. It combines different strategies, including information retrieval, reasoning, and transformer language models.\n",
    "- T5, or the Text-to-Text Transfer Transformer, exemplifies the adaptability of transformer models, trained on a broad corpus for general linguistic knowledge and then fine-tuned for specific tasks, showing proficiency in tasks like translation and answering complex questions.\n",
    "- Despite these advances, challenges remain. Transformer models are limited by the context window they can process, and there's ongoing research to extend this capability, as seen in the Reformer system, which can handle context up to a million words.\n",
    "- The field is exploring beyond textual data, considering how models could integrate structured databases, numerical data, images, and videos, pending advancements in hardware and AI methodologies.\n",
    "- The shift towards data-driven models doesn't negate the value of grammatical and semantic understanding from traditional NLP approaches. Future developments may see a resurgence of explicit linguistic modeling or the emergence of hybrid models that combine deep learning with traditional linguistic analysis.\n",
    "- The continuing improvements in NLP suggest a significant potential for new insights and breakthroughs, with room for contributions from across disciplines to bridge the gap between machine and human language understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The real state of the art in NLP as of 2024\n",
    "\n",
    "### Transformer based models dominate the NLP landscape\n",
    "\n",
    "* GPT-4 by OpenAI, GPT-4.5/5 this year?\n",
    "* Claude by Anthropic,\n",
    "* Mistral Large by Mistral\n",
    "* Gemini Ultra by Google\n",
    "* Update to BERT? RoBERTa? T5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter Summary** \n",
    "- Word embeddings represent a significant advancement in NLP by providing continuous, dense representations of words. Unlike discrete atomic representations, embeddings capture semantic similarities and can be effectively pretrained on vast amounts of unlabeled text, facilitating a deeper understanding of language nuances.\n",
    "- Recurrent Neural Networks (RNNs) are adept at processing sequential data, modeling both local and long-distance dependencies within text. Through their hidden states, RNNs retain essential information across sequences, enabling nuanced language modeling and analysis.\n",
    "- Sequence-to-sequence (Seq2Seq) models excel in tasks like machine translation and text generation by mapping sequences of input text to sequences of output text. These models typically consist of an encoder and decoder component, handling complex linguistic transformations.\n",
    "- Transformer models, highlighted by their use of self-attention mechanisms, have revolutionized NLP by modeling context across entire sequences directly. This architecture supports parallel processing of sequences, leveraging hardware acceleration for efficient training on large datasets.\n",
    "- Transfer learning, particularly with pretrained contextual embeddings, has enabled the development of versatile NLP models. By training on large corpora and then fine-tuning for specific tasks, these models achieve remarkable performance across a broad spectrum of NLP applications, including but not limited to, question answering, text summarization, and machine translation.\n",
    "- The chapter underscores the transition from traditional discrete representations and models to continuous, context-aware systems in NLP. This shift, propelled by advancements in deep learning and transfer learning, has led to significant improvements in the field's ability to understand and generate natural language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliographical and Historical Notes\n",
    "\n",
    "- **Zipf’s Law (Zipf, 1935, 1949):**  Zipf observed that the frequency of words in natural language is inversely proportional to their rank, highlighting the data sparsity problem in NLP. \n",
    "- **Deerwester et al. (1990):**  Introduced projecting words into low-dimensional vectors through decomposing word-document co-occurrence matrices, foundational to word embeddings. \n",
    "- **Brown et al. (1992):**  Grouped words into clusters based on bigram context, aiding tasks like named entity recognition. \n",
    "- **WORD2VEC (Mikolov et al., 2013):**  Demonstrated the effectiveness of word embeddings from neural networks, significantly advancing NLP modeling. \n",
    "- **GloVe (Pennington et al., 2014):**  Introduced word embeddings based on word co-occurrence matrices, refining the understanding of linguistic regularities. \n",
    "- **Neural Networks for Language Models (Bengio et al., 2003):**  Pioneered neural network use for NLP, combining distributed word representations with probabilistic functions for sequences. \n",
    "- **RNNs for Local Context (Mikolov et al., 2010; Jozefowicz et al., 2016):**  Showcased RNNs' ability to outperform n-gram models by modeling local context. \n",
    "- **ELMO (Peters et al., 2018):**  Highlighted the importance of contextual word representations for a more nuanced understanding of word meanings. \n",
    "- **ULMFiT (Howard and Ruder, 2018):**  Introduced a framework for fine-tuning pretrained language models, making it easier to adapt models to specific domains. \n",
    "- **Sequence to Sequence Learning (Sutskever et al., 2015):**  Introduced deep network-based sequence-to-sequence learning, revolutionizing tasks like machine translation. \n",
    "- **All you need is Attention (Vaswani et al., 2017):**  Introduced the transformer model, leveraging self-attention mechanisms for efficient sequence processing.\n",
    "- **BERT (Devlin et al., 2018):**  Demonstrated the effectiveness of transformer models pretrained with a masked language objective for multiple NLP tasks. \n",
    "- **XLNet and ERNIE 2.0 (Yang et al., 2019; Sun et al., 2019):**  Improved on BERT by addressing discrepancies between pretraining and fine-tuning phases. \n",
    "- **ROBERTA (Liu et al., 2019b):**  Optimized BERT with more data and different training procedures, matching state-of-the-art results. \n",
    "- **ALBERT (Lite BERT) and XLM:**  Focused on reducing parameters and incorporating multilingual training data for broader applicability. \n",
    "- **GLUE and SUPERGLUE Benchmarks (Wang et al., 2018a, 2019):**  Introduced to evaluate NLP systems, with transformers dominating leaderboards. \n",
    "- **Machine Translation History:**  From Petr Troyanskii’s 1933 \"translating machine\" idea to Warren Weaver’s 1947 insights, leading to the SYSTRAN system (Toma, 1977) and later, the advancement of end-to-end neural models for translation (Sutskever et al., 2015; Bahdanau et al., 2015; Vaswani et al., 2018). \n",
    "- **Question Answering and Language Inference:**  Development of datasets like SQuAD and large-scale data sets for natural language inference (Dagan et al., 2005; Rajpurkar et al., 2016) facilitated advances in deep learning models for these tasks. \n",
    "- **Overall,**  the field of NLP has seen substantial evolution, moving from hand-crafted features to data-driven models with deep learning and transfer learning, dramatically enhancing model performance across a wide range of linguistic tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources to learn more\n",
    "\n",
    "- **Books:** \n",
    "  - \"Speech and Language Processing\" by Jurafsky and Martin provides a comprehensive introduction to NLP concepts and techniques.\n",
    "  - \"Deep Learning\" by Goodfellow, Bengio, and Courville offers in-depth coverage of deep learning fundamentals and applications.\n",
    "  - \"Natural Language Processing in Action\" by Lane, Howard, and Hapke provides practical insights into NLP techniques and applications.\n",
    "\n",
    "- **Courses:**\n",
    "    - Stanford University's \"CS224N: Natural Language Processing with Deep Learning\" offers a deep dive into NLP concepts and applications.\n",
    "    - Coursera's \"Natural Language Processing\" course by National Research University Higher School of Economics provides a comprehensive overview of NLP techniques.\n",
    "\n",
    "- **Online Resources:**\n",
    "    - The Allen Institute for AI's \"The State of AI Report\" offers insights into the latest trends and advancements in AI, including NLP.\n",
    "    - The Hugging Face website provides access to pretrained transformer models and resources for NLP tasks. URL: https://huggingface.co/\n",
    "\n",
    "- **Research Papers:**\n",
    "    - \"Attention is All You Need\" by Vaswani et al. (2017) introduces the transformer model and self-attention mechanism.\n",
    "    - \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" by Devlin et al. (2018) presents the BERT model and its impact on NLP tasks.\n",
    "\n",
    "- **Conferences:**\n",
    "    - The Conference on Empirical Methods in Natural Language Processing (EMNLP) showcases the latest research in NLP and machine learning.\n",
    "    - The Association for Computational Linguistics (ACL) conference features presentations on cutting-edge NLP research and applications.\n",
    "\n",
    "- **Videos:**\n",
    "    - The YouTube channel \"Two Minute Papers\" offers concise summaries of recent research papers, including those on NLP and deep learning.\n",
    "    - The Stanford NLP Group's YouTube channel provides lectures and tutorials on NLP concepts and techniques.\n",
    "    - What is GPT? by 3Blue1Brown: https://youtu.be/wjZofJX0v4M?si=dtXEt_wXNKO9ZMDY\n",
    "    - Let's build GPT: from scratch,in code, spelled out by Andrej Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "\n",
    "- **Blog Posts:**\n",
    "    - Zero to Hero: https://karpathy.ai/zero-to-hero.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
