{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 22 - Reinforcement Learning\n",
    "\n",
    "*In which we see how experiencing rewards and punishments can teach an agent how to\n",
    "maximize rewards in the future.* - Stuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ValRCS/RBS_PBM773_Introduction_to_AI/main/img/ch22_reinforcement_learning/DALL%C2%B7E%202024-03-23%2022.35.53%20-%20Visualize%20a%20compelling%20scene%20where%20a%20human%20chess%20player%20is%20deeply%20engrossed%20in%20a%20game%20of%20chess%20against%20a%20neural%20network.%20The%20human%2C%20a%20figure%20of%20concen.webp\" width=\"500\">\n",
    "\n",
    "Rewarding learning agent for playing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - **22.1 Learning from Rewards**  \n",
    "- **Supervised learning challenges in complex environments:**  Applying supervised learning to complex tasks like chess is challenging due to the vast number of possible states and the difficulty in defining \"correct\" moves solely based on past grandmaster games. \n",
    "- **Introduction to reinforcement learning (RL):**  RL involves an agent learning from interactions with its environment through rewards, aiming to maximize the sum of these rewards. Unlike in supervised learning, the agent in RL may not know the environment's transition model or reward function ahead of time. \n",
    "- **Benefits of reinforcement learning:**  Providing reward signals is usually simpler and requires less expertise than supplying labeled examples for supervised learning. Even sparse rewards (where informative signals are rare) can be beneficial, and additional intermediate rewards can significantly aid learning. \n",
    "- **Versatility and applications of RL:**  Reinforcement learning is a flexible approach that has been applied successfully in various domains, including video games, robotics, and strategic games like poker. It can be enhanced with deep learning techniques for even broader applications. \n",
    "- **RL algorithms and categories:**  The chapter outlines two main types of reinforcement learning strategies: \n",
    "- **Model-based RL,**  where the agent uses or learns a model of the environment to interpret rewards and make decisions. This approach often involves learning a utility function based on the sum of rewards. \n",
    "- **Model-free RL,**  which does not rely on understanding the environment's model but directly learns how to act. This category includes action-utility learning (like Q-learning, where the agent learns a Q-function to evaluate the sum of rewards for state-action pairs) and policy search (learning a direct mapping from states to actions). \n",
    "- **Structure of the chapter:**  The chapter progresses from discussing passive reinforcement learning, where the agent's policy is predetermined, through active reinforcement learning that involves exploration and learning how to act within an environment. It explores the use of inductive and deep learning to enhance RL, the concept of providing intermediate rewards, and organizing behavior hierarchically. It concludes with a discussion on apprenticeship learning and real-world applications of RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22.2 Passive Reinforcement Learning**  \n",
    "- **Overview:**  Passive reinforcement learning involves an agent with a fixed policy π(s) learning the utility function Uπ(s) in a fully observable environment with a defined set of actions and states. This utility function represents the expected total discounted reward from following policy π starting in state s. \n",
    "- **Difference from policy evaluation:**  While passive reinforcement learning shares similarities with policy evaluation in policy iteration, the key difference is the passive learner's ignorance of the transition model P(s′|s,a) and the reward function R(s,a,s′), which define probabilities of state transitions and rewards for transitions, respectively. \n",
    "- **Learning without knowing transition and reward functions:**  The agent executes trials within the environment, following its fixed policy, and observes sequences of state transitions and rewards without prior knowledge of the transition or reward functions. The aim is to use these observations to learn the expected utility Uπ(s) for each nonterminal state. \n",
    "- **Example of learning process:**  Using the 4×3 world from Chapter 17 as an example, the agent conducts trials starting from an initial state and moving through the environment until reaching a terminal state. Each transition during the trials is annotated with the action taken and the reward received, which the agent uses to update its understanding of the utility of each state. \n",
    "- **Calculation of expected utility:**  The expected utility Uπ(s) is calculated as the expected sum of discounted rewards received by following the policy π from state s. A discount factor γ is included in the calculation to account for the time value of rewards, with γ = 1 indicating no discounting in the example 4×3 world.\n",
    "\n",
    "This section emphasizes the foundational aspects of passive reinforcement learning by illustrating how an agent can learn about an environment's dynamics and rewards through direct experience, even without initial knowledge of the environment's structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **22.2.1 Direct Utility Estimation**  \n",
    "- **Concept:**  Direct utility estimation defines the utility of a state as the expected total reward from that state onward, known as the expected reward-to-go. Each trial in the learning process provides a sample of this reward for each visited state. \n",
    "- **Method:**  The algorithm updates the estimated utility for each state by calculating the observed reward-to-go at the end of each sequence and maintaining a running average for each state. With an infinite number of trials, this method will converge to the true expected utility as defined by the reinforcement learning model. \n",
    "- **Reduction to supervised learning problem:**  This approach effectively reduces reinforcement learning to a supervised learning problem, where each data point is a pair consisting of a state and its corresponding reward-to-go. While this reduction allows the use of powerful supervised learning algorithms, it overlooks the dependencies between states and their successor states. \n",
    "- **Ignoring Bellman equations:**  The direct utility estimation method does not account for the Bellman equations, which articulate that the utility of a state is influenced by both the immediate reward and the expected utility of successor states. This oversight limits the method's efficiency by ignoring the inherent connections between state utilities. \n",
    "- **Drawbacks:**  By neglecting the relationships between states as described by the Bellman equations, direct utility estimation misses out on learning opportunities and may converge slowly. The approach treats the utility estimation problem as if searching within a hypothesis space larger than necessary, including many potential utility functions that violate the Bellman equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **22.2.2 Adaptive Dynamic Programming**  \n",
    "- **Definition and approach:**  Adaptive Dynamic Programming (ADP) integrates learning the transition model of the environment with solving the Markov decision process (MDP) via dynamic programming. This method leverages the interconnectedness of state utilities by learning the transition probabilities P(s′|s,π(s)) and observed rewards R(s,π(s),s′) to compute state utilities using Bellman equations. \n",
    "- **Use of linear algebra and modified policy iteration:**  Given that the Bellman equations form a linear system when the policy is fixed, they can be solved using linear algebra software. ADP can also use a simplified version of value iteration, called modified policy iteration, to update utility estimates efficiently after each incremental model adjustment. \n",
    "- **Learning the transition model:**  In fully observable environments, learning the transition model becomes a straightforward supervised learning task, using state–action pairs as inputs and resulting states as outputs. This model is often represented as a table, with transition probabilities estimated from observed transitions. \n",
    "- **Efficiency and limitations:**  The ADP agent's performance is primarily constrained by its ability to accurately learn the transition model. While ADP sets a benchmark for evaluating other reinforcement learning algorithms due to its direct approach to solving the MDP, it becomes impractical for very large state spaces, such as those in complex games like backgammon, due to the computational challenge of solving an enormous number of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Adaptive Dynamic Programming (ADP) Learner in Python\n",
    "\n",
    "Implementing a Passive Adaptive Dynamic Programming (ADP) Learner involves several key components: \n",
    "1. **Initialization** : Setting up the environment, including states, actions, policy, and initial estimates of the transition model and utilities. \n",
    "2. **Learning the Transition Model** : Updating the transition model based on observed transitions. \n",
    "3. **Estimating Utilities** : Using the learned transition model and observed rewards to update utilities, typically by solving the Bellman equations. \n",
    "4. **Utility Update Method** : Solving the Bellman equations can be done using linear algebra for the entire system or iteratively with a form of value iteration.\n",
    "\n",
    "Let's consider a simplified environment for clarity. We'll implement a passive ADP learner for a grid world, where the agent has a fixed policy π(s) and learns utilities of states by observing transitions and rewards.\n",
    "\n",
    "This example assumes a very basic environment setup for demonstration purposes. In more complex scenarios, you would need to expand this framework significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "{'A': 1.0, 'B': -1.0, 'C': 2.0, 'D': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# print numpy version\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "class PassiveADPLearner:\n",
    "    def __init__(self, states, actions, policy, gamma=0.9):\n",
    "        self.states = states  # List of states\n",
    "        self.actions = actions  # List of actions\n",
    "        self.policy = policy  # Fixed policy: state -> action\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.rewards = {}  # Reward function: (state, action, next_state) -> reward\n",
    "        self.transitions = {}  # Transition model: (state, action) -> {next_state: count}\n",
    "        self.returns = {state: 0 for state in states}  # State returns\n",
    "        self.counts = {state: 0 for state in states}  # State visit counts\n",
    "        self.utilities = {state: 0 for state in states}  # State utilities\n",
    "\n",
    "    def observe_transition(self, state, action, next_state, reward):\n",
    "        # Update the rewards and transition counts based on observed (s, a, s', r)\n",
    "        if (state, action, next_state) not in self.rewards:\n",
    "            self.rewards[(state, action, next_state)] = reward\n",
    "        self.transitions.setdefault((state, action), {}).setdefault(next_state, 0)\n",
    "        self.transitions[(state, action)][next_state] += 1\n",
    "\n",
    "    def update_utilities(self):\n",
    "        # Solve the Bellman equations using the observed transition model and rewards\n",
    "        for state in self.states:\n",
    "            action = self.policy[state]\n",
    "            total = 0\n",
    "            action_transitions = self.transitions.get((state, action), {})\n",
    "            total_transitions = sum(action_transitions.values())\n",
    "            for next_state, count in action_transitions.items():\n",
    "                transition_prob = count / total_transitions\n",
    "                reward = self.rewards[(state, action, next_state)]\n",
    "                total += transition_prob * (reward + self.gamma * self.utilities[next_state])\n",
    "            self.returns[state] += total\n",
    "            self.counts[state] += 1\n",
    "            self.utilities[state] = self.returns[state] / self.counts[state] if self.counts[state] else 0\n",
    "\n",
    "# Example usage\n",
    "states = ['A', 'B', 'C', 'D']  # Simplified states\n",
    "actions = ['left', 'right']  # Simplified actions\n",
    "policy = {'A': 'right', 'B': 'left', 'C': 'right', 'D': 'left'}  # Example policy\n",
    "\n",
    "learner = PassiveADPLearner(states, actions, policy)\n",
    "# Assume some transitions and rewards have been observed\n",
    "learner.observe_transition('A', 'right', 'B', 1)\n",
    "learner.observe_transition('B', 'left', 'C', -1)\n",
    "learner.observe_transition('C', 'right', 'D', 2)\n",
    "\n",
    "learner.update_utilities()\n",
    "print(learner.utilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22.3 Active Reinforcement Learning**  \n",
    "- **Transition from Passive to Active Learning:**  While a passive learning agent operates under a fixed policy, an active learning agent has the autonomy to choose its actions. This shift demands a broader learning scope, including a comprehensive transition model for all possible actions, not just those dictated by a predetermined policy. \n",
    "- **Learning a Complete Transition Model:**  Active reinforcement learning requires the agent to acquire a full transition model that encompasses outcome probabilities for every action across all states. This model facilitates the understanding of the environment's dynamics beyond the constraints of a fixed policy. \n",
    "- **Incorporating Choice of Actions:**  Active learning introduces the complexity of decision-making, where the agent must determine the most beneficial actions to take. The objective is to learn the utilities aligned with the optimal policy, adhering to the Bellman equations. These utilities reflect the highest expected returns from any given state, considering all available actions and their consequent states. \n",
    "- **Solving for Optimal Utilities:**  The utility function U, indicative of the optimal policy, can be derived through algorithms like value iteration or policy iteration, as outlined in previous chapters. These methods systematically solve the Bellman equations to identify the actions that maximize expected utility from each state. \n",
    "- **Determining Actions with Optimal Utility:**  With an established utility function, the agent can engage in one-step look-ahead to identify the action that maximizes expected utility, effectively deciding its next move based on the learned model's predictions. If the agent utilizes policy iteration, the optimal policy is explicitly defined, streamlining the action selection process. \n",
    "- **The Dilemma of Exploration vs. Exploitation:**  A critical question for active reinforcement learning agents is whether to follow the optimal action suggested by the current model or to explore alternative actions that might yield better long-term benefits. This dilemma highlights the importance of balancing immediate rewards with the potential discovery of more advantageous policies through exploration.\n",
    "\n",
    "Active reinforcement learning emphasizes an agent's ability to independently navigate its environment, making informed decisions based on a combination of learned models and strategic exploration. This approach not only requires comprehensive modeling of environmental dynamics but also necessitates a deliberate balance between exploiting known paths to rewards and exploring new possibilities to enhance the agent's understanding and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **22.3.1 Exploration**  \n",
    "- **Exploration vs. Exploitation:**  Active reinforcement learning faces the challenge of balancing exploration (discovering new information) and exploitation (utilizing current knowledge to maximize rewards). A greedy agent, which always chooses what seems best based on current information, may miss the optimal policy due to insufficient exploration. \n",
    "- **Greedy Agent Limitations:**  A greedy strategy can lead to suboptimal performance when the learned model of the environment is incomplete or inaccurate. Optimal actions within an incomplete model may not be truly optimal, leading the agent to settle prematurely on suboptimal policies. \n",
    "- **Informational Value of Actions:**  Actions provide value not just through immediate rewards but also by offering information that can lead to better future decisions. This highlights the necessity for a strategy that considers both immediate gains and the potential for discovering more rewarding options. \n",
    "- **GLIE (Greedy in the Limit with Infinite Exploration):**  A GLIE scheme ensures that every action is explored infinitely over time, guaranteeing that the agent eventually discovers the optimal policy. Such schemes balance exploration and exploitation over the long term, allowing for a comprehensive understanding of the environment. \n",
    "- **Implementation of Exploration Strategies:**  Practical exploration strategies may include random action selection with diminishing probability over time or prioritizing actions that have been less explored. Adjusting the utility calculation to favor less-explored actions encourages a more thorough exploration of the state space. \n",
    "- **Optimistic Initial Values:**  Starting with optimistic estimates of state utilities encourages exploration by initially biasing the agent towards unexplored states, under the assumption that unknown areas could offer high rewards. This optimism is gradually adjusted based on actual experiences, guiding the agent towards more beneficial areas of the state space. \n",
    "- **Exploration Functions:**  Exploration functions modulate the trade-off between the desire for high utility (exploitation) and the inclination to explore less-tried actions (exploration). By adjusting the utility estimates for state-action pairs based on the frequency of their selection, these functions ensure a balanced approach to learning the environment. \n",
    "- **Consequences of Effective Exploration:**  Properly implemented exploration strategies lead to quicker convergence to optimal or near-optimal policies, as the agent learns not only the immediate utility of actions but also acquires a broad understanding of the environment. This comprehensive learning approach ensures that less rewarding states are explored less frequently, optimizing the learning process towards the most valuable experiences.\n",
    "\n",
    "Exploration in active reinforcement learning is crucial for overcoming the limitations of greedy decision-making and for ensuring that an agent can discover the best possible policy within a complex environment. By intelligently balancing the need to explore unknown options with the use of current knowledge to gain rewards, an agent can achieve optimal performance over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**22.3.2 Safe Exploration**  \n",
    "- **Real-world Constraints on Exploration:**  Unlike simulations or games where mistakes can be easily corrected, real-world exploration by agents (such as robots or self-driving cars) must consider the risk of irreversible actions or entering absorbing states with severe negative consequences. \n",
    "- **Challenges of Safe Exploration:**  Safe exploration is crucial when negative outcomes can lead to significant harm, such as physical damage in the case of a self-driving car, or even the termination of the agent's ability to act, as in the case of entering an absorbing state from which no recovery is possible. \n",
    "- **Bayesian Reinforcement Learning:**  This approach utilizes a probabilistic model over possible hypotheses of the environment, updating beliefs based on observed evidence. The optimal policy maximizes expected utility across all hypotheses, weighted by their probabilities. However, this method may not always prevent risky exploratory actions that could lead to dangerous outcomes. \n",
    "- **Exploration Partially Observable Markov Decision Processes (POMDPs):**  In scenarios where future learning is anticipated, the problem of choosing an optimal policy becomes more complex. An exploration POMDP formulates the problem to include the impact of future observations on the agent's model of the environment, though solving this POMDP is often impractical. \n",
    "- **Robust Control Theory:**  This method does not assign probabilities to different models but instead considers a set of plausible models. It seeks an optimal policy that performs best in the worst-case scenario across all considered models. While providing a safety net by preparing for adverse outcomes, it may lead to overly cautious behavior that restricts the agent's effectiveness. \n",
    "- **Leveraging Human Expertise:**  Incorporating human knowledge and experience can enhance safety in reinforcement learning. This could involve using recorded actions from experienced operators as initial policies or defining explicit constraints on the agent's actions to prevent entering dangerous states. \n",
    "- **Trade-offs in Safe Exploration:**  Navigating the balance between exploration and safety involves making trade-offs between the potential for learning and the risk of negative outcomes. Strategies like Bayesian reinforcement learning and robust control theory provide frameworks for managing these risks but often require careful consideration of their assumptions and limitations.\n",
    "\n",
    "Safe exploration in reinforcement learning addresses the challenge of learning in environments where mistakes can have serious repercussions. By carefully balancing the need for exploration with the imperative of avoiding irreversible harm, reinforcement learning systems can be designed to navigate complex real-world environments more safely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
