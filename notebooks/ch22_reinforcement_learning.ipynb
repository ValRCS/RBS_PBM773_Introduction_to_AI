{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 22 - Reinforcement Learning\n",
    "\n",
    "*In which we see how experiencing rewards and punishments can teach an agent how to\n",
    "maximize rewards in the future.* - Stuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - **22.1 Learning from Rewards**  \n",
    "- **Supervised learning challenges in complex environments:**  Applying supervised learning to complex tasks like chess is challenging due to the vast number of possible states and the difficulty in defining \"correct\" moves solely based on past grandmaster games. \n",
    "- **Introduction to reinforcement learning (RL):**  RL involves an agent learning from interactions with its environment through rewards, aiming to maximize the sum of these rewards. Unlike in supervised learning, the agent in RL may not know the environment's transition model or reward function ahead of time. \n",
    "- **Benefits of reinforcement learning:**  Providing reward signals is usually simpler and requires less expertise than supplying labeled examples for supervised learning. Even sparse rewards (where informative signals are rare) can be beneficial, and additional intermediate rewards can significantly aid learning. \n",
    "- **Versatility and applications of RL:**  Reinforcement learning is a flexible approach that has been applied successfully in various domains, including video games, robotics, and strategic games like poker. It can be enhanced with deep learning techniques for even broader applications. \n",
    "- **RL algorithms and categories:**  The chapter outlines two main types of reinforcement learning strategies: \n",
    "- **Model-based RL,**  where the agent uses or learns a model of the environment to interpret rewards and make decisions. This approach often involves learning a utility function based on the sum of rewards. \n",
    "- **Model-free RL,**  which does not rely on understanding the environment's model but directly learns how to act. This category includes action-utility learning (like Q-learning, where the agent learns a Q-function to evaluate the sum of rewards for state-action pairs) and policy search (learning a direct mapping from states to actions). \n",
    "- **Structure of the chapter:**  The chapter progresses from discussing passive reinforcement learning, where the agent's policy is predetermined, through active reinforcement learning that involves exploration and learning how to act within an environment. It explores the use of inductive and deep learning to enhance RL, the concept of providing intermediate rewards, and organizing behavior hierarchically. It concludes with a discussion on apprenticeship learning and real-world applications of RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22.2 Passive Reinforcement Learning**  \n",
    "- **Overview:**  Passive reinforcement learning involves an agent with a fixed policy π(s) learning the utility function Uπ(s) in a fully observable environment with a defined set of actions and states. This utility function represents the expected total discounted reward from following policy π starting in state s. \n",
    "- **Difference from policy evaluation:**  While passive reinforcement learning shares similarities with policy evaluation in policy iteration, the key difference is the passive learner's ignorance of the transition model P(s′|s,a) and the reward function R(s,a,s′), which define probabilities of state transitions and rewards for transitions, respectively. \n",
    "- **Learning without knowing transition and reward functions:**  The agent executes trials within the environment, following its fixed policy, and observes sequences of state transitions and rewards without prior knowledge of the transition or reward functions. The aim is to use these observations to learn the expected utility Uπ(s) for each nonterminal state. \n",
    "- **Example of learning process:**  Using the 4×3 world from Chapter 17 as an example, the agent conducts trials starting from an initial state and moving through the environment until reaching a terminal state. Each transition during the trials is annotated with the action taken and the reward received, which the agent uses to update its understanding of the utility of each state. \n",
    "- **Calculation of expected utility:**  The expected utility Uπ(s) is calculated as the expected sum of discounted rewards received by following the policy π from state s. A discount factor γ is included in the calculation to account for the time value of rewards, with γ = 1 indicating no discounting in the example 4×3 world.\n",
    "\n",
    "This section emphasizes the foundational aspects of passive reinforcement learning by illustrating how an agent can learn about an environment's dynamics and rewards through direct experience, even without initial knowledge of the environment's structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **22.2.1 Direct Utility Estimation**  \n",
    "- **Concept:**  Direct utility estimation defines the utility of a state as the expected total reward from that state onward, known as the expected reward-to-go. Each trial in the learning process provides a sample of this reward for each visited state. \n",
    "- **Method:**  The algorithm updates the estimated utility for each state by calculating the observed reward-to-go at the end of each sequence and maintaining a running average for each state. With an infinite number of trials, this method will converge to the true expected utility as defined by the reinforcement learning model. \n",
    "- **Reduction to supervised learning problem:**  This approach effectively reduces reinforcement learning to a supervised learning problem, where each data point is a pair consisting of a state and its corresponding reward-to-go. While this reduction allows the use of powerful supervised learning algorithms, it overlooks the dependencies between states and their successor states. \n",
    "- **Ignoring Bellman equations:**  The direct utility estimation method does not account for the Bellman equations, which articulate that the utility of a state is influenced by both the immediate reward and the expected utility of successor states. This oversight limits the method's efficiency by ignoring the inherent connections between state utilities. \n",
    "- **Drawbacks:**  By neglecting the relationships between states as described by the Bellman equations, direct utility estimation misses out on learning opportunities and may converge slowly. The approach treats the utility estimation problem as if searching within a hypothesis space larger than necessary, including many potential utility functions that violate the Bellman equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **22.2.2 Adaptive Dynamic Programming**  \n",
    "- **Definition and approach:**  Adaptive Dynamic Programming (ADP) integrates learning the transition model of the environment with solving the Markov decision process (MDP) via dynamic programming. This method leverages the interconnectedness of state utilities by learning the transition probabilities P(s′|s,π(s)) and observed rewards R(s,π(s),s′) to compute state utilities using Bellman equations. \n",
    "- **Use of linear algebra and modified policy iteration:**  Given that the Bellman equations form a linear system when the policy is fixed, they can be solved using linear algebra software. ADP can also use a simplified version of value iteration, called modified policy iteration, to update utility estimates efficiently after each incremental model adjustment. \n",
    "- **Learning the transition model:**  In fully observable environments, learning the transition model becomes a straightforward supervised learning task, using state–action pairs as inputs and resulting states as outputs. This model is often represented as a table, with transition probabilities estimated from observed transitions. \n",
    "- **Efficiency and limitations:**  The ADP agent's performance is primarily constrained by its ability to accurately learn the transition model. While ADP sets a benchmark for evaluating other reinforcement learning algorithms due to its direct approach to solving the MDP, it becomes impractical for very large state spaces, such as those in complex games like backgammon, due to the computational challenge of solving an enormous number of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Adaptive Dynamic Programming (ADP) Learner in Python\n",
    "\n",
    "Implementing a Passive Adaptive Dynamic Programming (ADP) Learner involves several key components: \n",
    "1. **Initialization** : Setting up the environment, including states, actions, policy, and initial estimates of the transition model and utilities. \n",
    "2. **Learning the Transition Model** : Updating the transition model based on observed transitions. \n",
    "3. **Estimating Utilities** : Using the learned transition model and observed rewards to update utilities, typically by solving the Bellman equations. \n",
    "4. **Utility Update Method** : Solving the Bellman equations can be done using linear algebra for the entire system or iteratively with a form of value iteration.\n",
    "\n",
    "Let's consider a simplified environment for clarity. We'll implement a passive ADP learner for a grid world, where the agent has a fixed policy π(s) and learns utilities of states by observing transitions and rewards.\n",
    "\n",
    "This example assumes a very basic environment setup for demonstration purposes. In more complex scenarios, you would need to expand this framework significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPassiveADPLearner\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, states, actions, policy, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PassiveADPLearner:\n",
    "    def __init__(self, states, actions, policy, gamma=0.9):\n",
    "        self.states = states  # List of states\n",
    "        self.actions = actions  # List of actions\n",
    "        self.policy = policy  # Fixed policy: state -> action\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.rewards = {}  # Reward function: (state, action, next_state) -> reward\n",
    "        self.transitions = {}  # Transition model: (state, action) -> {next_state: count}\n",
    "        self.returns = {state: 0 for state in states}  # State returns\n",
    "        self.counts = {state: 0 for state in states}  # State visit counts\n",
    "        self.utilities = {state: 0 for state in states}  # State utilities\n",
    "\n",
    "    def observe_transition(self, state, action, next_state, reward):\n",
    "        # Update the rewards and transition counts based on observed (s, a, s', r)\n",
    "        if (state, action, next_state) not in self.rewards:\n",
    "            self.rewards[(state, action, next_state)] = reward\n",
    "        self.transitions.setdefault((state, action), {}).setdefault(next_state, 0)\n",
    "        self.transitions[(state, action)][next_state] += 1\n",
    "\n",
    "    def update_utilities(self):\n",
    "        # Solve the Bellman equations using the observed transition model and rewards\n",
    "        for state in self.states:\n",
    "            action = self.policy[state]\n",
    "            total = 0\n",
    "            action_transitions = self.transitions.get((state, action), {})\n",
    "            total_transitions = sum(action_transitions.values())\n",
    "            for next_state, count in action_transitions.items():\n",
    "                transition_prob = count / total_transitions\n",
    "                reward = self.rewards[(state, action, next_state)]\n",
    "                total += transition_prob * (reward + self.gamma * self.utilities[next_state])\n",
    "            self.returns[state] += total\n",
    "            self.counts[state] += 1\n",
    "            self.utilities[state] = self.returns[state] / self.counts[state] if self.counts[state] else 0\n",
    "\n",
    "# Example usage\n",
    "states = ['A', 'B', 'C', 'D']  # Simplified states\n",
    "actions = ['left', 'right']  # Simplified actions\n",
    "policy = {'A': 'right', 'B': 'left', 'C': 'right', 'D': 'left'}  # Example policy\n",
    "\n",
    "learner = PassiveADPLearner(states, actions, policy)\n",
    "# Assume some transitions and rewards have been observed\n",
    "learner.observe_transition('A', 'right', 'B', 1)\n",
    "learner.observe_transition('B', 'left', 'C', -1)\n",
    "learner.observe_transition('C', 'right', 'D', 2)\n",
    "\n",
    "learner.update_utilities()\n",
    "print(learner.utilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
