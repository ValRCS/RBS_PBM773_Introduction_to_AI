{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 22 - Reinforcement Learning\n",
    "\n",
    "*In which we see how experiencing rewards and punishments can teach an agent how to\n",
    "maximize rewards in the future.* - Stuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - **22.1 Learning from Rewards**  \n",
    "- **Supervised learning challenges in complex environments:**  Applying supervised learning to complex tasks like chess is challenging due to the vast number of possible states and the difficulty in defining \"correct\" moves solely based on past grandmaster games. \n",
    "- **Introduction to reinforcement learning (RL):**  RL involves an agent learning from interactions with its environment through rewards, aiming to maximize the sum of these rewards. Unlike in supervised learning, the agent in RL may not know the environment's transition model or reward function ahead of time. \n",
    "- **Benefits of reinforcement learning:**  Providing reward signals is usually simpler and requires less expertise than supplying labeled examples for supervised learning. Even sparse rewards (where informative signals are rare) can be beneficial, and additional intermediate rewards can significantly aid learning. \n",
    "- **Versatility and applications of RL:**  Reinforcement learning is a flexible approach that has been applied successfully in various domains, including video games, robotics, and strategic games like poker. It can be enhanced with deep learning techniques for even broader applications. \n",
    "- **RL algorithms and categories:**  The chapter outlines two main types of reinforcement learning strategies: \n",
    "- **Model-based RL,**  where the agent uses or learns a model of the environment to interpret rewards and make decisions. This approach often involves learning a utility function based on the sum of rewards. \n",
    "- **Model-free RL,**  which does not rely on understanding the environment's model but directly learns how to act. This category includes action-utility learning (like Q-learning, where the agent learns a Q-function to evaluate the sum of rewards for state-action pairs) and policy search (learning a direct mapping from states to actions). \n",
    "- **Structure of the chapter:**  The chapter progresses from discussing passive reinforcement learning, where the agent's policy is predetermined, through active reinforcement learning that involves exploration and learning how to act within an environment. It explores the use of inductive and deep learning to enhance RL, the concept of providing intermediate rewards, and organizing behavior hierarchically. It concludes with a discussion on apprenticeship learning and real-world applications of RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **22.2 Passive Reinforcement Learning**  \n",
    "- **Overview:**  Passive reinforcement learning involves an agent with a fixed policy π(s) learning the utility function Uπ(s) in a fully observable environment with a defined set of actions and states. This utility function represents the expected total discounted reward from following policy π starting in state s. \n",
    "- **Difference from policy evaluation:**  While passive reinforcement learning shares similarities with policy evaluation in policy iteration, the key difference is the passive learner's ignorance of the transition model P(s′|s,a) and the reward function R(s,a,s′), which define probabilities of state transitions and rewards for transitions, respectively. \n",
    "- **Learning without knowing transition and reward functions:**  The agent executes trials within the environment, following its fixed policy, and observes sequences of state transitions and rewards without prior knowledge of the transition or reward functions. The aim is to use these observations to learn the expected utility Uπ(s) for each nonterminal state. \n",
    "- **Example of learning process:**  Using the 4×3 world from Chapter 17 as an example, the agent conducts trials starting from an initial state and moving through the environment until reaching a terminal state. Each transition during the trials is annotated with the action taken and the reward received, which the agent uses to update its understanding of the utility of each state. \n",
    "- **Calculation of expected utility:**  The expected utility Uπ(s) is calculated as the expected sum of discounted rewards received by following the policy π from state s. A discount factor γ is included in the calculation to account for the time value of rewards, with γ = 1 indicating no discounting in the example 4×3 world.\n",
    "\n",
    "This section emphasizes the foundational aspects of passive reinforcement learning by illustrating how an agent can learn about an environment's dynamics and rewards through direct experience, even without initial knowledge of the environment's structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
